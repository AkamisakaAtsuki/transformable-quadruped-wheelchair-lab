from __future__ import annotations

import os
import time
import torch
from rsl_rl.runners import OnPolicyRunner
from isaaclab_tasks.utils import get_checkpoint_path
from typing import TYPE_CHECKING, Dict

from isaaclab.assets import Articulation
from isaaclab.managers import SceneEntityCfg
from isaaclab.sensors import ContactSensor
from isaaclab.sim import SimulationContext
from isaaclab.utils.math import sample_uniform
import isaaclab_tasks.manager_based.locomotion.velocity.mdp as mdp

import numpy as np
from collections import defaultdict

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedEnv

models_dir = "models"
walking_mode_model = "walking_mode.pt"
wheel_mode_model = "wheel_mode.pt"
change_mode_model = "change_mode.pt"
walking_mode_model_path = os.path.join(os.path.dirname(__file__), models_dir, walking_mode_model)
wheel_mode_model_path = os.path.join(os.path.dirname(__file__), models_dir, wheel_mode_model)
change_mode_model_path = os.path.join(os.path.dirname(__file__), models_dir, change_mode_model)

# CUDAãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèª
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

if not os.path.exists(walking_mode_model_path):
    print(f"[Error] Model file not found at {walking_mode_model_path}")
elif not os.path.exists(wheel_mode_model_path):
    print(f"[Error] Model file not found at {wheel_mode_model_path}")
elif not os.path.exists(change_mode_model_path):
    print(f"[Error] Model file not found at {change_mode_model_path}")
else:
    try:
        # JITãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        walking_mode_policy = torch.jit.load(walking_mode_model_path, map_location=device)
        walking_mode_policy.to(device)  # ãƒ¢ãƒ‡ãƒ«ã‚’CUDAãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
        walking_mode_policy.eval()
        print("[INFO] Walking Mode Policy loaded successfully as JIT model.")

        wheel_mode_policy = torch.jit.load(wheel_mode_model_path, map_location=device)
        wheel_mode_policy.to(device)  # ãƒ¢ãƒ‡ãƒ«ã‚’CUDAãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
        wheel_mode_policy.eval()
        print("[INFO] Wheel Mode Policy loaded successfully as JIT model.")

        change_mode_policy = torch.jit.load(change_mode_model_path, map_location=device)
        change_mode_policy.to(device)  # ãƒ¢ãƒ‡ãƒ«ã‚’CUDAãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
        change_mode_policy.eval()
        print("[INFO] Change Mode Policy loaded successfully as JIT model.")

    except Exception as e:
        print(f"[Error] Failed to load JIT model: {e}")   

# å¿…è¦ãªå¤‰æ•°ã®åˆæœŸåŒ–
len_env_ids = 64
walking_mode_policy_action_scale = 0.1
previous_actions_walking_mode = torch.zeros((len_env_ids, 12), dtype=torch.float32, device=device)
wheel_mode_policy_action_scale = 0.1
previous_actions_wheel_mode = torch.zeros((len_env_ids, 12), dtype=torch.float32, device=device)
actions_change_mode = torch.ones((len_env_ids, 1), dtype=torch.float32, device=device)

count = 0

def validate_env_and_joint_ids(env_ids: torch.Tensor, joint_ids: torch.Tensor):
    """env_ids ã¨ joint_ids ã®æ¤œè¨¼"""
    if env_ids.numel() == 0:
        return False
        # raise ValueError("[Error] env_ids is empty. Check your environment setup.")
    if joint_ids.numel() == 0:
        # raise ValueError("[Error] joint_ids is empty. Check your joint configuration.")
        return False
    
    return True

# ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
output_dir = "saved_data/output_dir"  # å‡ºåŠ›ãƒ‘ã‚¹ã‚’æŒ‡å®š
os.makedirs(output_dir, exist_ok=True)

# ãƒãƒƒãƒæ›¸ãè¾¼ã¿ç”¨ã®ãƒãƒƒãƒ•ã‚¡
write_buffer = defaultdict(list)
BATCH_SIZE = 100  # ä¸€åº¦ã«æ›¸ãè¾¼ã‚€ãƒ‡ãƒ¼ã‚¿æ•°

# æºã‚Œãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—CSVã«ä¿å­˜
def collect_vibration_data(
    env: ManagerBasedEnv,
    env_ids: torch.Tensor,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
):
    """
    TESLABOTã®æºã‚Œãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã€ISO 2631è©•ä¾¡ã«å‘ã‘ã¦æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã€‚
    """
    global write_buffer

    # IMUãƒ‡ãƒ¼ã‚¿å–å¾— (X, Y, Zè»¸ã®åŠ é€Ÿåº¦)
    observations = env.observation_manager.compute()
    base_acc = observations['policy'][:, :3].cpu().numpy()  # æœ€åˆã®3è¦ç´ ãŒåŠ é€Ÿåº¦ (X, Y, Z)

    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚åˆ»ã‚’å–å¾—
    sim_context = SimulationContext.instance()
    sim_time = sim_context.current_time

    for idx, env_id in enumerate(env_ids):
        env_id_int = int(env_id.item())
        acc_data = base_acc[idx]  # å„ç’°å¢ƒIDã®åŠ é€Ÿåº¦ãƒ‡ãƒ¼ã‚¿ (X, Y, Z)

        # åŠ é€Ÿåº¦ãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—åŒ–ã—ã¦ä¿å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
        acc_str = ";".join(f"{val:.4f}" for val in acc_data)
        write_buffer[env_id_int].append(f"{sim_time:.4f},{acc_str}\n")

        # ãƒãƒƒãƒ•ã‚¡ãŒä¸€å®šæ•°ã‚’è¶…ãˆãŸã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        if len(write_buffer[env_id_int]) >= BATCH_SIZE:
            file_path = os.path.join(output_dir, f"env_{env_id_int}_vibration.csv")
            if not os.path.exists(file_path):
                with open(file_path, "w") as f:
                    f.write("time,acc_x,acc_y,acc_z\n")
            with open(file_path, "a") as f:
                f.writelines(write_buffer[env_id_int])
            write_buffer[env_id_int] = []

    print(f"[INFO] Vibration data collected and saved for {len(env_ids)} environments.")

def illegal_contact_with_collect_data(env: ManagerBasedRLEnv, threshold: float, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
    """Terminate when the contact force on the sensor exceeds the force threshold."""
    global write_buffer

    # extract the used quantities (to enable type-hinting)
    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
    net_contact_forces = contact_sensor.data.net_forces_w_history
    # check if any contact force exceeds the threshold
    state = torch.any(
        torch.max(torch.norm(net_contact_forces[:, :, sensor_cfg.body_ids], dim=-1), dim=1)[0] > threshold, dim=1
    )
    # if state == True:
    sim_context = SimulationContext.instance()
    sim_time = sim_context.current_time  
    # print(f"{sim_time:.4f} + l:{len(state)}")

    for env_id_int in range(len(state)):
        state_ = state[env_id_int]
        
        if state_ == True:
            data = write_buffer[env_id_int]
            file_path = os.path.join(output_dir, f"env_{env_id_int}_vibration.csv")
            if not os.path.exists(file_path):
                with open(file_path, "w") as f:
                    f.write("time,acc_x,acc_y,acc_z\n")
            with open(file_path, "a") as f:
                f.writelines(data)
                f.write("\n")  # æœ€å¾Œã«ç©ºç™½è¡Œã‚’è¿½åŠ 
            write_buffer[env_id_int] = []

    return state

def time_out_with_collect_data(env: ManagerBasedRLEnv) -> torch.Tensor:
    """Terminate the episode when the episode length exceeds the maximum episode length."""
    global write_buffer

    state = env.episode_length_buf >= env.max_episode_length
    # if state == True:
    sim_context = SimulationContext.instance()
    sim_time = sim_context.current_time  
    # print(f"{sim_time:.4f} + t:{len(state)}")

    for env_id_int in range(len(state)):
        state_ = state[env_id_int]

        if state_ == True:
            data = write_buffer[env_id_int]
            file_path = os.path.join(output_dir, f"env_{env_id_int}_vibration.csv")
            if not os.path.exists(file_path):
                with open(file_path, "w") as f:
                    f.write("time,acc_x,acc_y,acc_z\n")
            with open(file_path, "a") as f:
                f.writelines(data)
                f.write("\n")  # æœ€å¾Œã«ç©ºç™½è¡Œã‚’è¿½åŠ 
            write_buffer[env_id_int] = []

    return state

def replace_observation_column(observation, info, column_name, new_data):
    """
    è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿å†…ã®æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ©ãƒ ã‚’æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ç½®ãæ›ãˆã‚‹é–¢æ•°ã€‚

    Args:
        observation (torch.Tensor): è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®ãƒ†ãƒ³ã‚½ãƒ«ã€‚
        info (dict): å„è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ åã¨ãã®ã‚µã‚¤ã‚ºã‚’ä¿æŒã™ã‚‹è¾æ›¸ã€‚
        column_name (str): ç½®ãæ›ãˆã‚‹ã‚«ãƒ©ãƒ ã®åå‰ã€‚
        new_data (torch.Tensor): ç½®ãæ›ãˆã‚‹æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã€‚

    Returns:
        torch.Tensor: ç½®ãæ›ãˆå¾Œã®è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã€‚
    """
    # ã‚«ãƒ©ãƒ ã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
    start_idx = 0
    for key, size in info.items():
        if key == column_name:
            break
        start_idx += size
    
    end_idx = start_idx + info[column_name]

    # print(f"start_idx: {start_idx}, end_idx: {end_idx}")

    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®ç½®ãæ›ãˆ
    updated_observation = torch.cat([
        observation[:, :start_idx],  # ç½®ãæ›ãˆå‰éƒ¨åˆ†
        new_data,                    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿
        observation[:, end_idx:]     # ç½®ãæ›ãˆå¾Œéƒ¨åˆ†
    ], dim=1)
    
    return updated_observation

def set_joint_angles(
    env: ManagerBasedEnv,
    env_ids: torch.Tensor,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    joint_angles: Dict[str, float] = None,
):
    """
    éšæ®µã‚’ã®ã¼ã‚‹ã¨ãã®æ¤…å­ã‚’ãƒ­ãƒœãƒƒãƒˆã®è§’åº¦ã«å¿œã˜ã¦å‹•çš„ã«å¤‰æ›´ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ 
    
    Parameters:
        env (ManagerBasedEnv): ç’°å¢ƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        env_ids (torch.Tensor): ç’°å¢ƒIDã®ãƒ†ãƒ³ã‚½ãƒ«ã€‚
        asset_cfg (SceneEntityCfg): ã‚¢ã‚»ãƒƒãƒˆè¨­å®šã€‚
        joint_pos_to_fix (Dict[str, float]): å›ºå®šã™ã‚‹ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆåã¨è§’åº¦ã®è¾æ›¸ã€‚
    """
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]

    joint_names = asset.joint_names

    # joint_angles ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿å®Ÿè¡Œ
    if joint_angles:
        for joint_name, angle in joint_angles.items():
            if joint_name in joint_names:  # joint_nameãŒjoint_namesã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                index_of_joint = joint_names.index(joint_name)
                asset.set_joint_position_target(target=angle, joint_ids=index_of_joint, env_ids=env_ids)
            else:
                print(f"Warning: Joint '{joint_name}' not found in joint_names.")

def four_wheel_independent_steering(
    env: ManagerBasedEnv,
    env_ids: torch.Tensor,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    joint_pos_to_fix: Dict[str, float] = None,
    front_left_steer: str = None,
    front_right_steer: str = None,
    rear_left_steer: str = None,
    rear_right_steer: str = None,
    front_left_wheel: str = None,
    front_right_wheel: str = None,
    rear_left_wheel: str = None,
    rear_right_wheel: str = None,
    debug_mode: bool = False,
):
    """
    å››è¼ªç‹¬ç«‹æ“èˆµã®åˆ¶å¾¡æƒ…å ±ã‚’å–å¾—ã—ã€å„ã‚¹ãƒ†ã‚¢ã¨ãƒ›ã‚¤ãƒ¼ãƒ«ã®è§’åº¦ã‚’ãƒãƒƒãƒå‡¦ç†ã§ç®¡ç†ã™ã‚‹é–¢æ•°ã€‚

    Args:
        env (ManagerBasedEnv): ç’°å¢ƒã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€‚
        env_ids (torch.Tensor): ç’°å¢ƒIDã®ãƒªã‚¹ãƒˆã€‚
        *_steer (str): å„ã‚¹ãƒ†ã‚¢ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆåã€‚
        *_wheel (str): å„ãƒ›ã‚¤ãƒ¼ãƒ«ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆåã€‚
    """

    global previous_actions_wheel_mode
    global actions_change_mode

    # ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹
    # start_time = time.time()

    # try:
    #     last_action_value = mdp.last_action(env)
    # except:
    #     pass

    # æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ç’°å¢ƒIDã‚’å–å¾—
    valid_env_mask = actions_change_mode.squeeze(-1) < 0.5  # å€¤ãŒ < 0 ã®ç’°å¢ƒã‚’æŠ½å‡º
    if debug_mode:
        valid_env_ids = env_ids # debug_modeãŒTrueã ã£ãŸã‚‰ã™ã¹ã¦ã®ç’°å¢ƒã«å¯¾ã—ã¦å®Ÿè¡Œ
    else:
        valid_env_ids = env_ids[valid_env_mask.nonzero(as_tuple=True)[0]]

    if valid_env_ids.numel() == 0:
        print("[INFO] No environments met the condition for four_wheel_independent_steering.")
        return

    # print(f"len valid_env_ids: {len(valid_env_ids)}")

    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    try:
        observations = env.observation_manager.compute()
        current_observations = torch.tensor(
            observations['policy'], dtype=torch.float32, device=device
        ).clone().detach()
        # print(f"[INFO] Observations shape: {current_observations.shape}")
        # è¦³æ¸¬å€¤ã®çµ¶å¯¾å€¤ãŒ100ã‚’è¶…ãˆã‚‹å ´åˆã®å‡¦ç†

    except Exception as e:
        print(f"[Error] Failed to get observations: {e}")
        return


    # print(f"[INFO] Observations: {current_observations}")
    # threshold = 100  # è¨±å®¹ç¯„å›²ã®çµ¶å¯¾å€¤ã®é–¾å€¤

    # # ç•°å¸¸å€¤ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    # if torch.any(torch.abs(current_observations) > threshold):
    #     print("[Error] current_observations contains values exceeding the threshold!")
    #     print("Abnormal observations detected. Stopping the simulation for debugging.")
    #     return  # å‡¦ç†ã‚’çµ‚äº†
 
    # num_nan = torch.sum(torch.isnan(current_observations))
    # if num_nan > 0:
    #     print(f"Detected NaN ! ({num_nan})")
    #     return

    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’èª¿æ•´
    try:
        if current_observations.shape[1] == 244:
            # actions (Index 6) ã‚’ç½®ãæ›ãˆ
            action_start_idx = sum([3, 3, 3, 3, 16, 28])  # actions ã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (ä¸Šè¨˜è¡¨ã‹ã‚‰è¨ˆç®—)
            action_end_idx = action_start_idx + 1  # ç¾åœ¨ã® actions ã¯ (1,)

            # rint(current_observations[:,action_start_idx:action_end_idx])

            # actions éƒ¨åˆ†ã‚’ä¸Šæ›¸ãã›ãšã€æ–°ã—ã„ã‚«ãƒ©ãƒ ã‚’æŒ¿å…¥
            current_observations = torch.cat([
                current_observations[:, :action_start_idx],  # actionsæ‰‹å‰ã¾ã§
                previous_actions_wheel_mode,  # (8,) ã® actions
                current_observations[:, action_end_idx:]  # actions ä»¥é™
            ], dim=1)

            # print(f"current_observations[b]: {current_observations[0]}")
            # print(f"[INFO] Actions replaced without affecting height_scan. New shape: {current_observations.shape}")
        else:
            print(f"[Error] Unexpected observation shape: {current_observations.shape}")
            return
    except Exception as e:
        print(f"[Error] Failed to adjust observations: {e}")
        return

    # æ–¹ç­–ã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—
    try:
        with torch.no_grad():
            actions = wheel_mode_policy(current_observations).to(device) 
            actions_steering = actions[:, :8]
            actions_wheel = actions[:, 8:]
            # print(f"[INFO] Actions from policy: {actions.shape}")
    except Exception as e:
        print(f"[Error] Failed to infer actions from policy: {e}")
        return

    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨˜éŒ²ï¼ˆæ¬¡å›ã®å…¥åŠ›ã«ä½¿ç”¨ï¼‰
    # previous_actions_wheel_mode = actions.clone().detach() # ã“ã“ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ãŸã‚‰ä¸Šæ‰‹ãå‹•ãã‚ˆã†ã«ãªã£ãŸã€‚æœ¬æ¥ã¯æ­£ã—ããªã„ã‘ã©ã€å®Ÿéš›ã¯åŸºæœ¬0ã«è¿‘ã„å€¤ã ã‹ã‚‰å•é¡Œãªã„ã ã‚ã†ã¨æ€ã‚ã‚Œã‚‹

    # å„ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’ç›´æ¥æŒ‡å®šï¼ˆè»Šè¼ªç§»å‹•ãƒ¢ãƒ¼ãƒ‰ã®æ™‚ã«ä½¿ç”¨ã—ãŸã‚‚ã®ï¼‰
    joint_offsets = {
        'FL_hip_joint': 0.1,
        'FR_hip_joint': -0.1,
        'RL_hip_joint': 0.1,
        'RR_hip_joint': -0.1, 
        'FL_thigh_joint': 0.0,
        'FR_thigh_joint': 0.0,
        'RL_thigh_joint': 0.0,
        'RR_thigh_joint': 0.0,
    }

    # ã‚¢ã‚»ãƒƒãƒˆå–å¾—
    asset: Articulation = env.scene[asset_cfg.name]
    joint_names = asset.joint_names
  
    # ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    target_joint_names = [
        "FL_hip_joint", "FR_hip_joint", "RL_hip_joint", "RR_hip_joint",
        "FL_thigh_joint", "FR_thigh_joint", "RL_thigh_joint", "RR_thigh_joint"
    ]

    # ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
    joint_indices = [joint_names.index(joint_name) for joint_name in target_joint_names]

    # å„ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’å–å¾—
    offsets = torch.tensor(
        [joint_offsets.get(joint_name, 0.0) for joint_name in target_joint_names],
        device=device
    )

    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®èª¿æ•´ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    adjusted_actions = actions_steering[valid_env_ids] * wheel_mode_policy_action_scale + offsets.unsqueeze(0)
    # adjusted_actions = actions[valid_env_ids] * wheel_mode_policy_action_scale + offsets.unsqueeze(0)

    # æœ‰åŠ¹ãªã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¸€æ‹¬ã§é©ç”¨
    # asset.set_joint_position_target(
    #     target=adjusted_actions,
    #     joint_ids=torch.tensor(joint_indices, device=device),
    #     env_ids=valid_env_ids
    # )

    # for i, joint_name in enumerate(target_joint_names):
    #     joint_index = joint_names.index(joint_name)
        
    #     # âœ… äº‹å‰å®šç¾©ã•ã‚ŒãŸã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’é©ç”¨
    #     offset = joint_offsets.get(joint_name, 0.0)

    #     if validate_env_and_joint_ids(valid_env_ids, torch.tensor([joint_index])):  # èµ·å‹•åˆæœŸã¯æ•°ãŒåˆã‚ãªã„ã“ã¨ãŒã‚ã‚‹ã®ã§ãƒã‚§ãƒƒã‚¯ï¼ˆã‚‚ã£ã¨ã¡ã‚ƒã‚“ã¨ã—ãŸã‚„ã‚Šæ–¹ãŒã‚ã‚‹ã¯ãšã ãŒ.tmpãªã‚„ã‚Šæ–¹ã§ã™ï¼‰
    #         # âœ… ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è€ƒæ…®ã—ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨
    #         adjusted_action = actions[valid_env_ids, i].unsqueeze(-1) * wheel_mode_policy_action_scale + offset
    #         asset.set_joint_position_target(
    #             target=adjusted_action,
    #             joint_ids=joint_index,
    #             env_ids=valid_env_ids
    #         )

    # å¿…è¦ãªã‚¸ãƒ§ã‚¤ãƒ³ãƒˆåã‚’ãƒªã‚¹ãƒˆåŒ–
    steer_joints = [front_left_steer, front_right_steer, rear_left_steer, rear_right_steer]
    wheel_joints = [front_left_wheel, front_right_wheel, rear_left_wheel, rear_right_wheel]

    # å„ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒãƒƒãƒã§å–å¾—
    steer_joint_indices = torch.tensor([joint_names.index(joint) for joint in steer_joints], device=env_ids.device)
    wheel_joint_indices = torch.tensor([joint_names.index(joint) for joint in wheel_joints], device=env_ids.device)

    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿å–å¾— (ãƒ­ãƒœãƒƒãƒˆåŸºæº–åº§æ¨™ç³»)
    velocity_commands = mdp.generated_commands(env=env, command_name="base_velocity")
    velocity_commands = velocity_commands[valid_env_ids]

    # ãƒ­ãƒœãƒƒãƒˆåŸºæº–åº§æ¨™ç³»
    linear_x = velocity_commands[:, 0]  # å‰å¾Œç§»å‹•é€Ÿåº¦
    linear_y = velocity_commands[:, 1]  # å·¦å³ç§»å‹•é€Ÿåº¦
    angular_z = velocity_commands[:, 2]  # æ—‹å›é€Ÿåº¦

    # è»Šä¸¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    wheel_base = 0.64  # å‰å¾Œè¼ªé–“ã®è·é›¢
    track_width = 0.6  # å·¦å³è¼ªé–“ã®è·é›¢

    # å„ãƒ›ã‚¤ãƒ¼ãƒ«ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆï¼ˆX: å‰å¾Œ, Y: å·¦å³ï¼‰
    offsets = torch.tensor([
        [wheel_base / 2, track_width / 2],  # front_left
        [wheel_base / 2, -track_width / 2],  # front_right
        [-wheel_base / 2, track_width / 2],  # rear_left
        [-wheel_base / 2, -track_width / 2]  # rear_right
    ], device=linear_x.device)

    # ğŸš— **åº§æ¨™å¤‰æ›**  
    # - å‰å¾Œç§»å‹• (xæ–¹å‘) â†’ æ­£ã—ã„ãƒ›ã‚¤ãƒ¼ãƒ«é€Ÿåº¦  
    # - å·¦å³ç§»å‹• (yæ–¹å‘) â†’ ã‚¹ãƒ†ã‚¢è§’åº¦  
    # - æ—‹å› (angular_z) â†’ å„ãƒ›ã‚¤ãƒ¼ãƒ«ã¸åæ˜   

    velocity = linear_x.unsqueeze(1) - angular_z.unsqueeze(1) * offsets[:, 1]
    lateral_velocity = linear_y.unsqueeze(1) + angular_z.unsqueeze(1) * offsets[:, 0]

    # åº§æ¨™è»¸ã®å›è»¢ä¿®æ­£ï¼ˆ90åº¦ãšã‚Œã‚’è£œæ­£ï¼‰
    angle = torch.atan2(
        lateral_velocity,
        velocity + 1e-6
    )

    wheel_speeds = torch.sqrt(
        velocity ** 2 + lateral_velocity ** 2
    )

    # æ–¹å‘ã®èª¿æ•´ï¼ˆé€†è»¢ã—ã¦ã„ã‚‹å¯èƒ½æ€§ã‚’è£œæ­£ï¼‰
    angle = -angle  # å¿…è¦ã«å¿œã˜ã¦é€†è»¢

    # ã‚¹ãƒ†ã‚¢ã¨ãƒ›ã‚¤ãƒ¼ãƒ«ã®ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’çµ±åˆ
    all_joint_indices = torch.cat([steer_joint_indices, torch.tensor(joint_indices, device=device)])

    # ã‚¹ãƒ†ã‚¢è§’åº¦ã¨ãƒ›ã‚¤ãƒ¼ãƒ«é€Ÿåº¦ã‚’çµ±åˆ
    all_targets = torch.cat([angle, adjusted_actions], dim=1)

    # å…¨ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã‚’ä¸€æ‹¬ã§è¨­å®š
    asset.set_joint_position_target(
        target=all_targets,
        joint_ids=all_joint_indices,
        env_ids=valid_env_ids
    )

    # ã‚¹ãƒ†ã‚¢è§’åº¦ã¨ãƒ›ã‚¤ãƒ¼ãƒ«é€Ÿåº¦ã‚’è¨­å®š
    # asset.set_joint_position_target(
    #     target=angle, 
    #     joint_ids=steer_joint_indices,
    #     env_ids=valid_env_ids
    # )

    wheel_speeds_adj = wheel_speeds + actions_wheel[valid_env_ids] * 0.25

    asset.set_joint_velocity_target(
        target=wheel_speeds * 55, 
        joint_ids=wheel_joint_indices,
        env_ids=valid_env_ids
    )

    # å›ºå®šã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆãŒã‚ã‚‹ãªã‚‰å›ºå®š
    if joint_pos_to_fix:
        set_joint_angles(env, valid_env_ids, asset_cfg, joint_pos_to_fix)

    # end_time = time.time()
    # print(f"[INFO] Processing time: {end_time - start_time:.6f} seconds")

def apply_learned_policy(
    env: ManagerBasedEnv,
    env_ids: torch.Tensor,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    joint_pos_to_fix: Dict[str, float] = None,
    observation_info: Dict[str, float] = None,
    debug_mode: bool = False,
):
    """
    å­¦ç¿’æ¸ˆã¿æ–¹ç­–ã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒœãƒƒãƒˆã‚’åˆ¶å¾¡ã™ã‚‹é–¢æ•°ã€‚

    Args:
        env (ManagerBasedEnv): ç’°å¢ƒã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€‚
        env_ids (torch.Tensor): ç’°å¢ƒIDã®ãƒªã‚¹ãƒˆã€‚
        asset_cfg (SceneEntityCfg): ã‚¢ã‚»ãƒƒãƒˆè¨­å®šã€‚
        policy_path (str): å­¦ç¿’æ¸ˆã¿æ–¹ç­–ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚

    Returns:
        None
    """
    global previous_actions_walking_mode
    global actions_change_mode

    # try:
    #     last_action_value = mdp.last_action(env)
    # except:
    #     pass

    # æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ç’°å¢ƒIDã‚’å–å¾—
    valid_env_mask = actions_change_mode.squeeze(-1) > 0.5  # å€¤ãŒ > 0 ã®ç’°å¢ƒã‚’æŠ½å‡º
    if debug_mode:
        valid_env_ids = env_ids # debug_modeãŒTrueã ã£ãŸã‚‰ã™ã¹ã¦ã®ç’°å¢ƒã«å¯¾ã—ã¦å®Ÿè¡Œ
    else:
        valid_env_ids = env_ids[valid_env_mask.nonzero(as_tuple=True)[0]]

    if valid_env_ids.numel() == 0:
        print("[INFO] No environments met the condition for apply_learned_policy.")
        return

    # âœ… è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    try:
        observations = env.observation_manager.compute()
        current_observations = torch.tensor(
            observations['policy'], dtype=torch.float32, device=device
        ).clone().detach()
        # print(f"[INFO] Observations shape: {current_observations.shape}")
    except Exception as e:
        print(f"[Error] Failed to get observations: {e}")
        return

    # info = {
    #     "base_lin_vel": 3,
    #     "base_ang_vel": 3,
    #     "projected_gravity": 3,
    #     "velocity_commands": 3,
    #     "joint_pos": 16,
    #     "joint_vel": 28,
    #     "actions": 1,
    #     "height_scan": 187,
    # }

    # âœ… è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’èª¿æ•´ (actions éƒ¨åˆ†ã‚’ç½®ãæ›ãˆ)
    # current_observations = replace_observation_column(
    #     current_observations, 
    #     observation_info, 
    #     'actions', 
    #     previous_actions_walking_mode
    # )

    try:
        if current_observations.shape[1] == 244:
            # actions (Index 6) ã‚’ç½®ãæ›ãˆ
            action_start_idx = sum([3, 3, 3, 3, 16, 28])  # actions ã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (ä¸Šè¨˜è¡¨ã‹ã‚‰è¨ˆç®—)
            action_end_idx = action_start_idx + 1  # ç¾åœ¨ã® actions ã¯ (1,)

            # actions éƒ¨åˆ†ã‚’ä¸Šæ›¸ãã›ãšã€æ–°ã—ã„ã‚«ãƒ©ãƒ ã‚’æŒ¿å…¥
            current_observations = torch.cat([
                current_observations[:, :action_start_idx],  # actionsæ‰‹å‰ã¾ã§
                previous_actions_walking_mode,  # (12,) ã® actions
                current_observations[:, action_end_idx:]  # actions ä»¥é™
            ], dim=1)
            # print(f"[INFO] Actions replaced without affecting height_scan. New shape: {current_observations.shape}")
        else:
            print(f"[Error] Unexpected observation shape: {current_observations.shape}")
            return
    except Exception as e:
        print(f"[Error] Failed to adjust observations: {e}")
        return

    # âœ… æ–¹ç­–ã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—
    try:
        with torch.no_grad():
            actions = walking_mode_policy(current_observations).to(device) 
            # print(f"[INFO] Actions from policy: {actions.shape}")
    except Exception as e:
        print(f"[Error] Failed to infer actions from policy: {e}")
        return

    # âœ… ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨˜éŒ²ï¼ˆæ¬¡å›ã®å…¥åŠ›ã«ä½¿ç”¨ï¼‰
    previous_actions_walking_mode = actions.clone().detach()
    
    # âœ… å„ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’ç›´æ¥æŒ‡å®šï¼ˆæ­©è¡Œã‚’å­¦ç¿’ã—ãŸã¨ãã«ä½¿ç”¨ã—ãŸã‚‚ã®ã‚’è¨­å®šã™ã‚‹ï¼‰
    joint_offsets = {
        "FL_hip_joint": 0.1,
        "FR_hip_joint": -0.1,
        "RL_hip_joint": 0.1,
        "RR_hip_joint": -0.1,
        "FL_thigh_joint": 0.8,
        "FR_thigh_joint": 0.8,
        "RL_thigh_joint": 1.0,
        "RR_thigh_joint": 1.0,
        "FL_calf_joint": -1.5,
        "FR_calf_joint": -1.5,
        "RL_calf_joint": -1.5,
        "RR_calf_joint": -1.5,
        "slider_joint": 0.325
    }

    asset: Articulation = env.scene[asset_cfg.name]
    joint_names = asset.joint_names

    # âœ… ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã¸ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ”ãƒ³ã‚°
    target_joint_names = [
        "FL_hip_joint", "FR_hip_joint", "RL_hip_joint", "RR_hip_joint",
        "FL_thigh_joint", "FR_thigh_joint", "RL_thigh_joint", "RR_thigh_joint",
        "FL_calf_joint", "FR_calf_joint", "RL_calf_joint", "RR_calf_joint"
    ]

    for i, joint_name in enumerate(target_joint_names):
        if joint_name in joint_names:
            joint_index = joint_names.index(joint_name)
            
            # âœ… äº‹å‰å®šç¾©ã•ã‚ŒãŸã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’é©ç”¨
            offset = joint_offsets.get(joint_name, 0.0)

            if validate_env_and_joint_ids(valid_env_ids, torch.tensor([joint_index])):  # èµ·å‹•åˆæœŸã¯æ•°ãŒåˆã‚ãªã„ã“ã¨ãŒã‚ã‚‹ã®ã§ãƒã‚§ãƒƒã‚¯ï¼ˆã‚‚ã£ã¨ã¡ã‚ƒã‚“ã¨ã—ãŸã‚„ã‚Šæ–¹ãŒã‚ã‚‹ã¯ãšã ãŒ.tmpãªã‚„ã‚Šæ–¹ã§ã™ï¼‰
                # âœ… ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è€ƒæ…®ã—ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨
                adjusted_action = actions[valid_env_ids, i].unsqueeze(-1) * walking_mode_policy_action_scale + offset
                asset.set_joint_position_target(
                    target=adjusted_action,
                    joint_ids=joint_index,
                    env_ids=valid_env_ids
                )
        
    # å›ºå®šã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆãŒã‚ã‚‹ãªã‚‰å›ºå®š
    if joint_pos_to_fix:
        set_joint_angles(env, valid_env_ids, asset_cfg, joint_pos_to_fix)

def change_mode_prediction(
    env: ManagerBasedEnv,
    env_ids: torch.Tensor,
    debug_mode: bool = False,
):
    global actions_change_mode

    # print(actions_change_mode)

    if len(env_ids) == 0:
        return

    # â‘  ActionManager ã‹ã‚‰ãƒã‚¤ãƒŠãƒªãƒªã‚¹ãƒˆç”¨ã® ActionTerm ã‚’å–å¾—ã™ã‚‹
    # ã“ã“ã§ã¯ "binary_action" ã¨ã„ã†ã‚­ãƒ¼ã§ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã¨ä»®å®šã—ã¾ã™ã€‚
    try:
        # print("[START]")
        continuous_action_term = env.action_manager.get_term("continuous_action")
        continuous_list = continuous_action_term.get_list()
        # actions_change_mode = (continuous_list > 0).float()
        actions_change_mode = torch.ones((len_env_ids, 1), dtype=torch.float32, device=device)

        # å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—
        mean_val = continuous_list.mean()
        std_val = continuous_list.std()
        # print("[END]")
        # print(continuous_list)
        print(f"mean = {mean_val:.4f}, std = {std_val:.4f}")

        return 
    except Exception as e:
        print(f"[Error] Failed to get binary_action term: {e}")
        return

    # print(actions_change_mode)

    # # âœ… è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    # try:
    #     observations = env.observation_manager.compute()
    #     current_observations = torch.tensor(
    #         observations['policy'], dtype=torch.float32, device=device
    #     ).clone().detach()[env_ids]
    #     # print(f"[INFO] Observations shape: {current_observations.shape}")
    # except Exception as e:
    #     print(f"[Error] Failed to get observations: {e}")
    #     return

    # try:
    #     if current_observations.shape[1] == 244:
    #         # actions (Index 6) ã‚’ç½®ãæ›ãˆ
    #         action_start_idx = sum([3, 3, 3, 3, 16, 28])  # actions ã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (ä¸Šè¨˜è¡¨ã‹ã‚‰è¨ˆç®—)
    #         action_end_idx = action_start_idx + 1  # ç¾åœ¨ã® actions ã¯ (1,)

    #         # actions éƒ¨åˆ†ã‚’å‰Šé™¤
    #         current_observations = torch.cat([
    #             current_observations[:, :action_start_idx],  # actionsæ‰‹å‰ã¾ã§
    #             current_observations[:, action_end_idx:]  # actions ä»¥é™
    #         ], dim=1)
    #         # print(f"[INFO] Actions replaced without affecting height_scan. New shape: {current_observations.shape}")
    #     else:
    #         print(f"[Error] Unexpected observation shape: {current_observations.shape}")
    #         return
    # except Exception as e:
    #     print(f"[Error] Failed to adjust observations: {e}")
    #     return

    # # âœ… æ–¹ç­–ã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—
    # try:
    #     with torch.no_grad():
    #         actions = change_mode_policy(current_observations).to(device) 
    #         # print(f"[INFO] Actions from policy: {actions.shape}")
    # except Exception as e:
    #     print(f"[Error] Failed to infer actions from policy: {e}")
    #     return

    # # âœ… ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨˜éŒ²ï¼ˆæ¬¡å›ã®å…¥åŠ›ã«ä½¿ç”¨ï¼‰
    # actions_change_mode[env_ids] = (actions.clone().detach() > 0.5).to(torch.float32)
   
    # # sim_context = SimulationContext.instance()
    # # sim_time = sim_context.current_time 
    # # print(sim_time)
    # # print(len(env_ids))

def change_mode_reward(
    env: ManagerBasedRLEnv,
    env_ids: torch.Tensor,
    terrain_y: Dict[str, float] = None,
    desirable_mode: Dict[str, float] = None,
    debug_mode: bool = False,
):
    """
    - env_idsã”ã¨ã«ãƒ­ãƒœãƒƒãƒˆã®Yåº§æ¨™ã‚’å–å¾—ã—ã€
    - äº‹å‰ã«å®šç¾©ã•ã‚ŒãŸ terrain_y ã‚’ä½¿ã£ã¦ã€Œæœ€ã‚‚è¿‘ã„ã‚µãƒ–ãƒ†ãƒ¬ã‚¤ãƒ³ã€ã‚’ç‰¹å®šã€
    - ã•ã‚‰ã« desirable_mode ã‹ã‚‰ã€ãã®ã‚µãƒ–ãƒ†ãƒ¬ã‚¤ãƒ³ã«æœ›ã¾ã—ã„ãƒ¢ãƒ¼ãƒ‰ (1 or -1) ã‚’å–å¾—ã™ã‚‹ã€‚
    - actions_change_mode ã¨æ¯”è¼ƒã—ã¦ç°¡æ˜“çš„ãªå ±é…¬ã‚’ä»˜ä¸ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã€‚
    """
    global actions_change_mode

    # ã‚‚ã—åœ°å½¢æƒ…å ±ã‚„ãƒ¢ãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ãªã‘ã‚Œã°ä½•ã‚‚ã—ãªã„
    if terrain_y is None or desirable_mode is None:
        return

    # env_ids ãŒç©ºãªã‚‰ä½•ã‚‚ã—ãªã„
    if len(env_ids) == 0:
        return

    # ----------------------------------------------------------------------------
    # 1. ãƒ­ãƒœãƒƒãƒˆã®Yåº§æ¨™ã‚’å–å¾—ï¼ˆIsaac Labã§ã¯ get_world_poses() ã§ã¯ãªã root_pos_w ã‚’å‚ç…§ï¼‰
    # ----------------------------------------------------------------------------
    asset: Articulation = env.scene["robot"]
    # shape: (num_envs, 3) â†’ (pos_x, pos_y, pos_z)
    pos_w = asset.data.root_pos_w[env_ids]  # env_idsã§æŠ½å‡º
    y_positions = pos_w[:, 1]               # Yåº§æ¨™ã®ã¿

    # ----------------------------------------------------------------------------
    # 2. terrain_yã‹ã‚‰ã€Œæœ€ã‚‚è¿‘ã„ã‚µãƒ–ãƒ†ãƒ¬ã‚¤ãƒ³ã€ã‚’æ¤œç´¢
    # ----------------------------------------------------------------------------
    #   terrain_y: {ã‚µãƒ–ãƒ†ãƒ¬ã‚¤ãƒ³å: yåº§æ¨™}
    terrain_names = list(terrain_y.keys())                        
    terrain_vals = torch.tensor(list(terrain_y.values()),
                                device=y_positions.device,
                                dtype=torch.float32)               

    # è·é›¢è¡Œåˆ— dist: shape = (len(env_ids), num_terrains)
    dist = torch.abs(y_positions.unsqueeze(1) - terrain_vals.unsqueeze(0))
    # å„envã«ã¤ã„ã¦æœ€å°è·é›¢ã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    min_idx = torch.argmin(dist, dim=1)

    # ã‚µãƒ–ãƒ†ãƒ¬ã‚¤ãƒ³ã«å¯¾å¿œã™ã‚‹ãƒ¢ãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«ã‚’ã¾ã¨ã‚ã¦å–å¾—
    terrain_labels_list = []
    for idx in min_idx:
        terrain_name = terrain_names[idx]
        terrain_labels_list.append(desirable_mode[terrain_name])  # -1 or 1
    terrain_labels = torch.tensor(terrain_labels_list, device=y_positions.device, dtype=torch.float32)
    
    # print(terrain_labels)

    # # ----------------------------------------------------------------------------
    # # 3. actions_change_modeï¼ˆå®Ÿéš›ã«é¸æŠã—ã¦ã„ã‚‹ãƒ¢ãƒ¼ãƒ‰ï¼‰ã¨æ¯”è¼ƒã—ã€å ±é…¬ã‚’åŠ ç®—
    # # ----------------------------------------------------------------------------
    # #   - actions_change_mode[env_id] ãŒ 0.5ä»¥ä¸Š â†’ 1 (æ­©è¡Œãƒ¢ãƒ¼ãƒ‰)
    # #   - ãã‚Œæœªæº€ â†’ -1 (è»Šè¼ªãƒ¢ãƒ¼ãƒ‰)
    # mode_raw = actions_change_mode[env_ids].squeeze(-1)
    # chosen_mode = torch.where(mode_raw > 0.5,
    #                           torch.tensor(1.0, device=y_positions.device),
    #                           torch.tensor(-1.0, device=y_positions.device))

    # # ä¸€è‡´ã™ã‚Œã° +1ã€é•ãˆã° -1 ã®ç°¡æ˜“å ±é…¬ä¾‹
    # reward = torch.where(chosen_mode == terrain_labels,
    #                      torch.tensor(1.0, device=y_positions.device),
    #                      torch.tensor(-1.0, device=y_positions.device))

    # # ç’°å¢ƒã® reward_buf ã«åŠ ç®—
    # env.reward_buf[env_ids] += reward

    # # ----------------------------------------------------------------------------
    # # 4. ãƒ‡ãƒãƒƒã‚°å‡ºåŠ› (ä»»æ„)
    # # ----------------------------------------------------------------------------
    # if debug_mode:
    #     for i, e_id in enumerate(env_ids):
    #         print(f"[DEBUG] Env {int(e_id.item())}: y={y_positions[i].item():.2f}, "
    #               f"Nearest Terrain={terrain_names[min_idx[i]]}, Label={terrain_labels[i].item()}, "
    #               f"ChosenMode={chosen_mode[i].item()}, Reward={reward[i].item()}")
