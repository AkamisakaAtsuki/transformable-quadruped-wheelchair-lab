{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][AppLauncher]: Loading experience file: C:\\Users\\admin5050\\akamisaka\\IsaacLab\\apps\\isaaclab.python.kit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin5050\\anaconda3\\envs\\env_isaaclab\\lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.4 when it was built against 1.14.6, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_WindowsSelectorEventLoop' object has no attribute '_old_agen_hooks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:/users/admin5050/anaconda3/envs/env_isaaclab/lib/site-packages/omni/extscore/omni.kit.async_engine/omni/kit/async_engine/async_engine.py:123\u001b[0m, in \u001b[0;36m_AsyncEngineDriver._patch_event_loop.<locals>.run_forever\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_forever_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:/users/admin5050/anaconda3/envs/env_isaaclab/lib/site-packages/omni/extscore/omni.kit.async_engine/omni/kit/async_engine/async_engine.py:85\u001b[0m, in \u001b[0;36m_AsyncEngineDriver._patch_event_loop.<locals>._run_forever_setup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_coroutine_origin_tracking(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug)\n",
      "File \u001b[1;32mc:\\Users\\admin5050\\anaconda3\\envs\\env_isaaclab\\lib\\asyncio\\base_events.py:584\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m--> 584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:/users/admin5050/anaconda3/envs/env_isaaclab/lib/site-packages/omni/extscore/omni.kit.async_engine/omni/kit/async_engine/async_engine.py:263\u001b[0m, in \u001b[0;36m_AsyncEngineDriver.on_startup.<locals>.<lambda>\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    257\u001b[0m settings\u001b[38;5;241m.\u001b[39mset_default(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/exts/omni.kit.async_engine/updateSubscriptionOrder\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    258\u001b[0m     omni\u001b[38;5;241m.\u001b[39mkit\u001b[38;5;241m.\u001b[39mapp\u001b[38;5;241m.\u001b[39mUPDATE_ORDER_PYTHON_EXEC_END_UPDATE)\n\u001b[0;32m    260\u001b[0m update_subscription_order \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/exts/omni.kit.async_engine/updateSubscriptionOrder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_end_sub \u001b[38;5;241m=\u001b[39m app\u001b[38;5;241m.\u001b[39mget_update_event_stream()\u001b[38;5;241m.\u001b[39mcreate_subscription_to_pop(\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    264\u001b[0m     order\u001b[38;5;241m=\u001b[39mupdate_subscription_order,\n\u001b[0;32m    265\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ext: omni.async_engine] AsyncEngineDriver::EndUpdate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03mCurrently disabled until we update carb_sdk to better handle new subscriptions\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03mwhile in the middle of processing pump of that event stream\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m)\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    285\u001b[0m MainEventLoopWrapper\u001b[38;5;241m.\u001b[39mg_main_event_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\n",
      "File \u001b[1;32mc:/users/admin5050/anaconda3/envs/env_isaaclab/lib/site-packages/omni/extscore/omni.kit.async_engine/omni/kit/async_engine/async_engine.py:219\u001b[0m, in \u001b[0;36m_AsyncEngineDriver._patch_event_loop.<locals>.run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m--> 219\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_forever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# The proactor loop always has the self_reading_future event, so <= 1\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_until_done \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ready) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_proactor \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ready):\n",
      "File \u001b[1;32mc:/users/admin5050/anaconda3/envs/env_isaaclab/lib/site-packages/omni/extscore/omni.kit.async_engine/omni/kit/async_engine/async_engine.py:129\u001b[0m, in \u001b[0;36m_AsyncEngineDriver._patch_event_loop.<locals>.run_forever\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_forever_cleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:/users/admin5050/anaconda3/envs/env_isaaclab/lib/site-packages/omni/extscore/omni.kit.async_engine/omni/kit/async_engine/async_engine.py:103\u001b[0m, in \u001b[0;36m_AsyncEngineDriver._patch_event_loop.<locals>._run_forever_cleanup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m events\u001b[38;5;241m.\u001b[39m_set_running_loop(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_coroutine_origin_tracking(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_agen_hooks\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     sys\u001b[38;5;241m.\u001b[39mset_asyncgen_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_old_agen_hooks)\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_old_agen_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_WindowsSelectorEventLoop' object has no attribute '_old_agen_hooks'"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agent_cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"Train with RSL-RL agent.\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# override configurations with non-hydra CLI arguments\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m agent_cfg \u001b[38;5;241m=\u001b[39m cli_args\u001b[38;5;241m.\u001b[39mupdate_rsl_rl_cfg(\u001b[43magent_cfg\u001b[49m, args_cli)\n\u001b[0;32m     88\u001b[0m env_cfg\u001b[38;5;241m.\u001b[39mscene\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m=\u001b[39m args_cli\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;28;01mif\u001b[39;00m args_cli\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m env_cfg\u001b[38;5;241m.\u001b[39mscene\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m     89\u001b[0m agent_cfg\u001b[38;5;241m.\u001b[39mmax_iterations \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     90\u001b[0m     args_cli\u001b[38;5;241m.\u001b[39mmax_iterations \u001b[38;5;28;01mif\u001b[39;00m args_cli\u001b[38;5;241m.\u001b[39mmax_iterations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m agent_cfg\u001b[38;5;241m.\u001b[39mmax_iterations\n\u001b[0;32m     91\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'agent_cfg' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2022-2025, The Isaac Lab Project Developers.\n",
    "# All rights reserved.\n",
    "#\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "\"\"\"Script to train RL agent with RSL-RL.\"\"\"\n",
    "\n",
    "\"\"\"Launch Isaac Sim Simulator first.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from isaaclab.app import AppLauncher\n",
    "\n",
    "# local imports\n",
    "import cli_args  # isort: skip\n",
    "\n",
    "\n",
    "# add argparse arguments\n",
    "parser = argparse.ArgumentParser(description=\"Train an RL agent with RSL-RL.\")\n",
    "parser.add_argument(\"--video\", action=\"store_true\", default=False, help=\"Record videos during training.\")\n",
    "parser.add_argument(\"--video_length\", type=int, default=200, help=\"Length of the recorded video (in steps).\")\n",
    "parser.add_argument(\"--video_interval\", type=int, default=2000, help=\"Interval between video recordings (in steps).\")\n",
    "parser.add_argument(\"--num_envs\", type=int, default=None, help=\"Number of environments to simulate.\")\n",
    "parser.add_argument(\"--task\", type=str, default=None, help=\"Name of the task.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=None, help=\"Seed used for the environment\")\n",
    "parser.add_argument(\"--max_iterations\", type=int, default=None, help=\"RL Policy training iterations.\")\n",
    "\n",
    "parser.set_defaults(\n",
    "    task=\"Isaac-Cartpole-v0\",\n",
    "    num_envs=128\n",
    ")\n",
    "\n",
    "# append RSL-RL cli arguments\n",
    "cli_args.add_rsl_rl_args(parser)\n",
    "# append AppLauncher cli args\n",
    "AppLauncher.add_app_launcher_args(parser)\n",
    "args_cli, hydra_args = parser.parse_known_args()\n",
    "\n",
    "# always enable cameras to record video\n",
    "if args_cli.video:\n",
    "    args_cli.enable_cameras = True\n",
    "\n",
    "# clear out sys.argv for Hydra\n",
    "sys.argv = [sys.argv[0]] + hydra_args\n",
    "\n",
    "# launch omniverse app\n",
    "app_launcher = AppLauncher(args_cli)\n",
    "simulation_app = app_launcher.app\n",
    "\n",
    "\"\"\"Rest everything follows.\"\"\"\n",
    "\n",
    "import gymnasium as gym\n",
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "from rsl_rl.runners import OnPolicyRunner\n",
    "\n",
    "from isaaclab.envs import (\n",
    "    DirectMARLEnv,\n",
    "    DirectMARLEnvCfg,\n",
    "    DirectRLEnvCfg,\n",
    "    ManagerBasedRLEnvCfg,\n",
    "    multi_agent_to_single_agent,\n",
    ")\n",
    "from isaaclab.utils.dict import print_dict\n",
    "from isaaclab.utils.io import dump_pickle, dump_yaml\n",
    "\n",
    "from isaaclab_rl.rsl_rl import RslRlOnPolicyRunnerCfg, RslRlVecEnvWrapper\n",
    "\n",
    "import isaaclab_tasks  # noqa: F401\n",
    "from isaaclab_tasks.utils import get_checkpoint_path\n",
    "from isaaclab_tasks.utils.hydra import hydra_task_config\n",
    "\n",
    "import transformable_quadruped_wheelchair_isaaclab.tasks.locomotion # 追加\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# @hydra_task_config(args_cli.task, \"rsl_rl_cfg_entry_point\")\n",
    "# def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: RslRlOnPolicyRunnerCfg):\n",
    "\"\"\"Train with RSL-RL agent.\"\"\"\n",
    "# override configurations with non-hydra CLI arguments\n",
    "agent_cfg = cli_args.update_rsl_rl_cfg(agent_cfg, args_cli)\n",
    "env_cfg.scene.num_envs = args_cli.num_envs if args_cli.num_envs is not None else env_cfg.scene.num_envs\n",
    "agent_cfg.max_iterations = (\n",
    "    args_cli.max_iterations if args_cli.max_iterations is not None else agent_cfg.max_iterations\n",
    ")\n",
    "\n",
    "# set the environment seed\n",
    "# note: certain randomizations occur in the environment initialization so we set the seed here\n",
    "env_cfg.seed = agent_cfg.seed\n",
    "env_cfg.sim.device = args_cli.device if args_cli.device is not None else env_cfg.sim.device\n",
    "\n",
    "# specify directory for logging experiments\n",
    "log_root_path = os.path.join(\"logs\", \"rsl_rl\", agent_cfg.experiment_name)\n",
    "log_root_path = os.path.abspath(log_root_path)\n",
    "print(f\"[INFO] Logging experiment in directory: {log_root_path}\")\n",
    "# specify directory for logging runs: {time-stamp}_{run_name}\n",
    "log_dir = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# This way, the Ray Tune workflow can extract experiment name.\n",
    "print(f\"Exact experiment name requested from command line: {log_dir}\")\n",
    "if agent_cfg.run_name:\n",
    "    log_dir += f\"_{agent_cfg.run_name}\"\n",
    "log_dir = os.path.join(log_root_path, log_dir)\n",
    "\n",
    "# create isaac environment\n",
    "env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\n",
    "\n",
    "# convert to single-agent instance if required by the RL algorithm\n",
    "if isinstance(env.unwrapped, DirectMARLEnv):\n",
    "    env = multi_agent_to_single_agent(env)\n",
    "\n",
    "# save resume path before creating a new log_dir\n",
    "if agent_cfg.resume:\n",
    "    resume_path = get_checkpoint_path(log_root_path, agent_cfg.load_run, agent_cfg.load_checkpoint)\n",
    "\n",
    "# wrap for video recording\n",
    "if args_cli.video:\n",
    "    video_kwargs = {\n",
    "        \"video_folder\": os.path.join(log_dir, \"videos\", \"train\"),\n",
    "        \"step_trigger\": lambda step: step % args_cli.video_interval == 0,\n",
    "        \"video_length\": args_cli.video_length,\n",
    "        \"disable_logger\": True,\n",
    "    }\n",
    "    print(\"[INFO] Recording videos during training.\")\n",
    "    print_dict(video_kwargs, nesting=4)\n",
    "    env = gym.wrappers.RecordVideo(env, **video_kwargs)\n",
    "\n",
    "# wrap around environment for rsl-rl\n",
    "env = RslRlVecEnvWrapper(env)\n",
    "\n",
    "# create runner from rsl-rl\n",
    "runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)\n",
    "# write git state to logs\n",
    "runner.add_git_repo_to_log(__file__)\n",
    "# load the checkpoint\n",
    "if agent_cfg.resume:\n",
    "    print(f\"[INFO]: Loading model checkpoint from: {resume_path}\")\n",
    "    # load previously trained model\n",
    "    runner.load(resume_path)\n",
    "\n",
    "# dump the configuration into log-directory\n",
    "dump_yaml(os.path.join(log_dir, \"params\", \"env.yaml\"), env_cfg)\n",
    "dump_yaml(os.path.join(log_dir, \"params\", \"agent.yaml\"), agent_cfg)\n",
    "dump_pickle(os.path.join(log_dir, \"params\", \"env.pkl\"), env_cfg)\n",
    "dump_pickle(os.path.join(log_dir, \"params\", \"agent.pkl\"), agent_cfg)\n",
    "\n",
    "# run training\n",
    "runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)\n",
    "\n",
    "# close the simulator\n",
    "env.close()\n",
    "simulation_app.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026a745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d39151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "deac2f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorCritic.__init__ got unexpected arguments, which will be ignored: ['apply_tanh']\n",
      "Actor MLP: Sequential(\n",
      "  (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): Tanh()\n",
      "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Critic MLP: Sequential(\n",
      "  (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): Tanh()\n",
      "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "=== Expert (PPO) Training Start ===\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 0/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 216 steps/s (collection: 0.800s, learning 0.124s)\n",
      "               Value function loss: 17217.4637\n",
      "                    Surrogate loss: 0.0163\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1666.02\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 200\n",
      "                    Iteration time: 0.92s\n",
      "                        Total time: 0.92s\n",
      "                               ETA: 924.0s\n",
      "\n",
      "Could not find git repository in c:\\Users\\admin5050\\anaconda3\\envs\\env_isaaclab\\lib\\site-packages\\rsl_rl\\__init__.py. Skipping.\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 1/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 290 steps/s (collection: 0.560s, learning 0.127s)\n",
      "               Value function loss: 3061.3369\n",
      "                    Surrogate loss: 0.0020\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1145.23\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 1.61s\n",
      "                               ETA: 804.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 2/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 290 steps/s (collection: 0.551s, learning 0.137s)\n",
      "               Value function loss: 6951.8413\n",
      "                    Surrogate loss: -0.0075\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1112.65\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 2.30s\n",
      "                               ETA: 765.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 3/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.570s, learning 0.127s)\n",
      "               Value function loss: 7239.2208\n",
      "                    Surrogate loss: 0.0061\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1101.81\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 3.00s\n",
      "                               ETA: 746.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 4/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.582s, learning 0.136s)\n",
      "               Value function loss: 4988.2212\n",
      "                    Surrogate loss: -0.0003\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1036.68\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 3.71s\n",
      "                               ETA: 739.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 5/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.570s, learning 0.125s)\n",
      "               Value function loss: 16912.5249\n",
      "                    Surrogate loss: 0.0156\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1139.12\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1200\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 4.41s\n",
      "                               ETA: 731.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 6/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 274 steps/s (collection: 0.603s, learning 0.125s)\n",
      "               Value function loss: 12350.6664\n",
      "                    Surrogate loss: 0.0182\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1178.78\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1400\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 5.14s\n",
      "                               ETA: 729.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 7/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.556s, learning 0.122s)\n",
      "               Value function loss: 4979.6236\n",
      "                    Surrogate loss: 0.0013\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1139.13\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 5.82s\n",
      "                               ETA: 721.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 8/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 295 steps/s (collection: 0.556s, learning 0.121s)\n",
      "               Value function loss: 3952.2347\n",
      "                    Surrogate loss: 0.0449\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1094.54\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1800\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 6.49s\n",
      "                               ETA: 715.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 9/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.555s, learning 0.128s)\n",
      "               Value function loss: 5823.5532\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1075.62\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 7.18s\n",
      "                               ETA: 711.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 10/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.562s, learning 0.123s)\n",
      "               Value function loss: 5364.8018\n",
      "                    Surrogate loss: 0.0264\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1057.82\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2200\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 7.86s\n",
      "                               ETA: 707.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 11/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 298 steps/s (collection: 0.546s, learning 0.124s)\n",
      "               Value function loss: 11630.5744\n",
      "                    Surrogate loss: 0.0009\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1084.54\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2400\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 8.53s\n",
      "                               ETA: 703.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 12/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 298 steps/s (collection: 0.541s, learning 0.130s)\n",
      "               Value function loss: 5975.0699\n",
      "                    Surrogate loss: 0.0481\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1074.72\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2600\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 9.20s\n",
      "                               ETA: 699.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 13/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.575s, learning 0.125s)\n",
      "               Value function loss: 8688.1468\n",
      "                    Surrogate loss: -0.0002\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1082.09\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 9.90s\n",
      "                               ETA: 698.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 14/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 305 steps/s (collection: 0.533s, learning 0.122s)\n",
      "               Value function loss: 10075.1934\n",
      "                    Surrogate loss: -0.0002\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1095.78\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3000\n",
      "                    Iteration time: 0.65s\n",
      "                        Total time: 10.56s\n",
      "                               ETA: 693.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 15/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 300 steps/s (collection: 0.544s, learning 0.121s)\n",
      "               Value function loss: 5802.1717\n",
      "                    Surrogate loss: 0.0028\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1087.72\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3200\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 11.22s\n",
      "                               ETA: 690.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 16/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.567s, learning 0.124s)\n",
      "               Value function loss: 15757.6728\n",
      "                    Surrogate loss: 0.0087\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1118.38\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 11.91s\n",
      "                               ETA: 689.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 17/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.553s, learning 0.122s)\n",
      "               Value function loss: 9064.2225\n",
      "                    Surrogate loss: -0.0028\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1124.18\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 12.59s\n",
      "                               ETA: 687.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 18/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 300 steps/s (collection: 0.541s, learning 0.124s)\n",
      "               Value function loss: 18820.7949\n",
      "                    Surrogate loss: -0.0002\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1157.59\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3800\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 13.25s\n",
      "                               ETA: 684.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 19/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.551s, learning 0.124s)\n",
      "               Value function loss: 5929.0745\n",
      "                    Surrogate loss: 0.0075\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1148.77\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 13.93s\n",
      "                               ETA: 683.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 20/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.573s, learning 0.128s)\n",
      "               Value function loss: 6619.4387\n",
      "                    Surrogate loss: 0.0030\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1142.72\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 14.63s\n",
      "                               ETA: 682.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 21/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 305 steps/s (collection: 0.536s, learning 0.119s)\n",
      "               Value function loss: 13529.9443\n",
      "                    Surrogate loss: -0.0025\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1158.97\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4400\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 15.28s\n",
      "                               ETA: 680.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 22/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 295 steps/s (collection: 0.555s, learning 0.121s)\n",
      "               Value function loss: 6083.6819\n",
      "                    Surrogate loss: 0.0209\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1151.93\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 15.96s\n",
      "                               ETA: 678.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 23/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.567s, learning 0.132s)\n",
      "               Value function loss: 6682.3418\n",
      "                    Surrogate loss: 0.0035\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1146.61\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 16.66s\n",
      "                               ETA: 678.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 24/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.567s, learning 0.124s)\n",
      "               Value function loss: 6118.8260\n",
      "                    Surrogate loss: 0.0173\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1140.35\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5000\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 17.35s\n",
      "                               ETA: 677.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 25/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.577s, learning 0.125s)\n",
      "               Value function loss: 5983.5052\n",
      "                    Surrogate loss: 0.0050\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1133.91\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 18.05s\n",
      "                               ETA: 676.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 26/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 298 steps/s (collection: 0.551s, learning 0.118s)\n",
      "               Value function loss: 6819.3090\n",
      "                    Surrogate loss: 0.0544\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1131.70\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5400\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 18.72s\n",
      "                               ETA: 675.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 27/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 298 steps/s (collection: 0.551s, learning 0.120s)\n",
      "               Value function loss: 10743.0535\n",
      "                    Surrogate loss: 0.0049\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1138.83\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5600\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 19.39s\n",
      "                               ETA: 673.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 28/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 297 steps/s (collection: 0.551s, learning 0.121s)\n",
      "               Value function loss: 6886.1622\n",
      "                    Surrogate loss: 0.0848\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1136.80\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5800\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 20.06s\n",
      "                               ETA: 672.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 29/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.555s, learning 0.120s)\n",
      "               Value function loss: 11236.8414\n",
      "                    Surrogate loss: 0.0057\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1144.38\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 20.74s\n",
      "                               ETA: 671.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 30/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.565s, learning 0.119s)\n",
      "               Value function loss: 7144.1321\n",
      "                    Surrogate loss: 0.0598\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1142.82\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 21.42s\n",
      "                               ETA: 670.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 31/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 293 steps/s (collection: 0.554s, learning 0.127s)\n",
      "               Value function loss: 13771.4224\n",
      "                    Surrogate loss: 0.0066\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1154.73\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6400\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 22.10s\n",
      "                               ETA: 669.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 32/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 304 steps/s (collection: 0.542s, learning 0.115s)\n",
      "               Value function loss: 7032.8257\n",
      "                    Surrogate loss: 0.0213\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1151.47\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6600\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 22.76s\n",
      "                               ETA: 667.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 33/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 300 steps/s (collection: 0.543s, learning 0.123s)\n",
      "               Value function loss: 6261.9072\n",
      "                    Surrogate loss: 0.0061\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1147.21\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6800\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 23.43s\n",
      "                               ETA: 666.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 34/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 298 steps/s (collection: 0.551s, learning 0.119s)\n",
      "               Value function loss: 10637.1267\n",
      "                    Surrogate loss: 0.0059\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1152.58\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 24.10s\n",
      "                               ETA: 665.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 35/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.587s, learning 0.129s)\n",
      "               Value function loss: 7383.6973\n",
      "                    Surrogate loss: 0.0058\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1150.35\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7200\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 24.81s\n",
      "                               ETA: 665.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 36/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.581s, learning 0.124s)\n",
      "               Value function loss: 7035.1436\n",
      "                    Surrogate loss: 0.0295\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1148.56\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 25.52s\n",
      "                               ETA: 664.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 37/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 261 steps/s (collection: 0.627s, learning 0.139s)\n",
      "               Value function loss: 6039.4068\n",
      "                    Surrogate loss: 0.0178\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1144.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7600\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 26.28s\n",
      "                               ETA: 666.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 38/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.589s, learning 0.137s)\n",
      "               Value function loss: 6377.6086\n",
      "                    Surrogate loss: -0.0027\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1141.31\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7800\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 27.01s\n",
      "                               ETA: 666.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 39/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.600s, learning 0.123s)\n",
      "               Value function loss: 6627.4252\n",
      "                    Surrogate loss: 0.0689\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1139.36\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 27.73s\n",
      "                               ETA: 666.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 40/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 270 steps/s (collection: 0.617s, learning 0.123s)\n",
      "               Value function loss: 7733.6549\n",
      "                    Surrogate loss: 0.0078\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1137.79\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8200\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 28.47s\n",
      "                               ETA: 666.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 41/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.582s, learning 0.120s)\n",
      "               Value function loss: 9396.1003\n",
      "                    Surrogate loss: 0.0098\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1140.94\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 29.17s\n",
      "                               ETA: 666.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 42/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 295 steps/s (collection: 0.556s, learning 0.121s)\n",
      "               Value function loss: 7099.7717\n",
      "                    Surrogate loss: 0.0256\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1138.97\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 29.85s\n",
      "                               ETA: 665.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 43/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 298 steps/s (collection: 0.543s, learning 0.128s)\n",
      "               Value function loss: 7289.4417\n",
      "                    Surrogate loss: 0.0107\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1137.21\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8800\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 30.52s\n",
      "                               ETA: 663.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 44/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.553s, learning 0.121s)\n",
      "               Value function loss: 7250.3516\n",
      "                    Surrogate loss: 0.0142\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1135.38\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 9000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 31.20s\n",
      "                               ETA: 662.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 45/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 293 steps/s (collection: 0.556s, learning 0.125s)\n",
      "               Value function loss: 8256.7301\n",
      "                    Surrogate loss: 0.0020\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1136.37\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 9200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 31.88s\n",
      "                               ETA: 661.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 46/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.568s, learning 0.125s)\n",
      "               Value function loss: 7189.3682\n",
      "                    Surrogate loss: 0.0577\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1134.58\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 9400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 32.57s\n",
      "                               ETA: 661.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 47/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.560s, learning 0.123s)\n",
      "               Value function loss: 6048.3756\n",
      "                    Surrogate loss: 0.0045\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1131.91\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 9600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 33.25s\n",
      "                               ETA: 660.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 48/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 295 steps/s (collection: 0.556s, learning 0.121s)\n",
      "               Value function loss: 21353.8551\n",
      "                    Surrogate loss: -0.0096\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1147.64\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 9800\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 33.93s\n",
      "                               ETA: 659.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 49/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.559s, learning 0.120s)\n",
      "               Value function loss: 6116.1900\n",
      "                    Surrogate loss: 0.0159\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1144.96\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 10000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 34.61s\n",
      "                               ETA: 658.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 50/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 302 steps/s (collection: 0.541s, learning 0.120s)\n",
      "               Value function loss: 20388.8930\n",
      "                    Surrogate loss: 0.0017\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1159.02\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 10200\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 35.27s\n",
      "                               ETA: 657.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 51/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.562s, learning 0.121s)\n",
      "               Value function loss: 7220.5711\n",
      "                    Surrogate loss: 0.0074\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1156.96\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 10400\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 35.95s\n",
      "                               ETA: 656.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 52/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 303 steps/s (collection: 0.537s, learning 0.121s)\n",
      "               Value function loss: 6283.6856\n",
      "                    Surrogate loss: 0.0043\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1154.49\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 10600\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 36.61s\n",
      "                               ETA: 654.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 53/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.563s, learning 0.124s)\n",
      "               Value function loss: 11834.3792\n",
      "                    Surrogate loss: 0.0202\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1159.70\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 10800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 37.30s\n",
      "                               ETA: 654.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 54/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 290 steps/s (collection: 0.565s, learning 0.123s)\n",
      "               Value function loss: 5776.8637\n",
      "                    Surrogate loss: 0.0111\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1156.72\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 11000\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 37.99s\n",
      "                               ETA: 653.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 55/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 300 steps/s (collection: 0.546s, learning 0.120s)\n",
      "               Value function loss: 6374.8051\n",
      "                    Surrogate loss: 0.0063\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1154.90\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 11200\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 38.65s\n",
      "                               ETA: 652.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 56/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 309 steps/s (collection: 0.525s, learning 0.121s)\n",
      "               Value function loss: 6146.5567\n",
      "                    Surrogate loss: 0.0039\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1152.52\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 11400\n",
      "                    Iteration time: 0.65s\n",
      "                        Total time: 39.30s\n",
      "                               ETA: 650.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 57/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 312 steps/s (collection: 0.523s, learning 0.118s)\n",
      "               Value function loss: 10056.2981\n",
      "                    Surrogate loss: -0.0074\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1155.37\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 11600\n",
      "                    Iteration time: 0.64s\n",
      "                        Total time: 39.94s\n",
      "                               ETA: 649.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 58/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 301 steps/s (collection: 0.549s, learning 0.115s)\n",
      "               Value function loss: 5136.0151\n",
      "                    Surrogate loss: 0.0443\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1151.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 11800\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 40.60s\n",
      "                               ETA: 648.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 59/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 299 steps/s (collection: 0.548s, learning 0.119s)\n",
      "               Value function loss: 9341.1163\n",
      "                    Surrogate loss: 0.0102\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1153.78\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 12000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 41.27s\n",
      "                               ETA: 647.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 60/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 295 steps/s (collection: 0.546s, learning 0.130s)\n",
      "               Value function loss: 12369.2826\n",
      "                    Surrogate loss: -0.0005\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1159.00\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 12200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 41.95s\n",
      "                               ETA: 646.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 61/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.561s, learning 0.123s)\n",
      "               Value function loss: 7472.0823\n",
      "                    Surrogate loss: 0.0134\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1158.13\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 12400\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 42.63s\n",
      "                               ETA: 645.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 62/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 293 steps/s (collection: 0.559s, learning 0.123s)\n",
      "               Value function loss: 7067.2106\n",
      "                    Surrogate loss: 0.0001\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1156.62\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 12600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 43.31s\n",
      "                               ETA: 644.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 63/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.565s, learning 0.125s)\n",
      "               Value function loss: 6374.1858\n",
      "                    Surrogate loss: 0.0133\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1154.70\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 12800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 44.00s\n",
      "                               ETA: 644.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 64/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.554s, learning 0.120s)\n",
      "               Value function loss: 6952.7606\n",
      "                    Surrogate loss: 0.0001\n",
      "             Mean action noise std: 0.10\n",
      "                 Mean total reward: -1153.20\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 13000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 44.68s\n",
      "                               ETA: 643.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 65/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.552s, learning 0.123s)\n",
      "               Value function loss: 13544.8114\n",
      "                    Surrogate loss: 0.0158\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1159.05\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 13200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 45.35s\n",
      "                               ETA: 642.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 66/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 300 steps/s (collection: 0.544s, learning 0.121s)\n",
      "               Value function loss: 16882.7946\n",
      "                    Surrogate loss: 0.0301\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1167.33\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 13400\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 46.02s\n",
      "                               ETA: 641.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 67/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.556s, learning 0.119s)\n",
      "               Value function loss: 6177.0906\n",
      "                    Surrogate loss: 0.0034\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1165.05\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 13600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 46.69s\n",
      "                               ETA: 640.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 68/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 301 steps/s (collection: 0.546s, learning 0.118s)\n",
      "               Value function loss: 17101.0572\n",
      "                    Surrogate loss: 0.0219\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1173.10\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 13800\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 47.36s\n",
      "                               ETA: 639.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 69/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 293 steps/s (collection: 0.562s, learning 0.119s)\n",
      "               Value function loss: 10768.6856\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1175.97\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 14000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 48.04s\n",
      "                               ETA: 638.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 70/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 295 steps/s (collection: 0.559s, learning 0.118s)\n",
      "               Value function loss: 6844.6698\n",
      "                    Surrogate loss: 0.0182\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1174.30\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 14200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 48.71s\n",
      "                               ETA: 638.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 71/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 298 steps/s (collection: 0.551s, learning 0.120s)\n",
      "               Value function loss: 4892.2622\n",
      "                    Surrogate loss: 0.0122\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1170.91\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 14400\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 49.38s\n",
      "                               ETA: 637.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 72/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 293 steps/s (collection: 0.558s, learning 0.123s)\n",
      "               Value function loss: 6014.0922\n",
      "                    Surrogate loss: 0.0035\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1168.93\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 14600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 50.07s\n",
      "                               ETA: 636.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 73/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.565s, learning 0.121s)\n",
      "               Value function loss: 6479.2288\n",
      "                    Surrogate loss: 0.0413\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1167.20\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 14800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 50.75s\n",
      "                               ETA: 635.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 74/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 304 steps/s (collection: 0.537s, learning 0.119s)\n",
      "               Value function loss: 9406.7126\n",
      "                    Surrogate loss: 0.0028\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1169.00\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 15000\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 51.41s\n",
      "                               ETA: 634.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 75/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 300 steps/s (collection: 0.542s, learning 0.123s)\n",
      "               Value function loss: 8264.7038\n",
      "                    Surrogate loss: 0.0332\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1169.36\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 15200\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 52.07s\n",
      "                               ETA: 633.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 76/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.556s, learning 0.124s)\n",
      "               Value function loss: 5619.2767\n",
      "                    Surrogate loss: 0.0180\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1167.03\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 15400\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 52.75s\n",
      "                               ETA: 633.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 77/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 303 steps/s (collection: 0.541s, learning 0.119s)\n",
      "               Value function loss: 5177.0598\n",
      "                    Surrogate loss: -0.0003\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1163.82\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 15600\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 53.41s\n",
      "                               ETA: 632.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 78/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 301 steps/s (collection: 0.549s, learning 0.115s)\n",
      "               Value function loss: 6118.5152\n",
      "                    Surrogate loss: 0.1061\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1162.52\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 15800\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 54.08s\n",
      "                               ETA: 631.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 79/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 298 steps/s (collection: 0.550s, learning 0.121s)\n",
      "               Value function loss: 14695.6176\n",
      "                    Surrogate loss: 0.0151\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1168.12\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 16000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 54.75s\n",
      "                               ETA: 630.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 80/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 301 steps/s (collection: 0.544s, learning 0.119s)\n",
      "               Value function loss: 11503.2641\n",
      "                    Surrogate loss: 0.0753\n",
      "             Mean action noise std: 0.11\n",
      "                 Mean total reward: -1171.38\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 16200\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 55.41s\n",
      "                               ETA: 629.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 81/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.577s, learning 0.128s)\n",
      "               Value function loss: 20102.1978\n",
      "                    Surrogate loss: 0.0044\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1179.94\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 16400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 56.12s\n",
      "                               ETA: 628.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 82/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.581s, learning 0.127s)\n",
      "               Value function loss: 7498.6188\n",
      "                    Surrogate loss: 0.0167\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1179.52\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 16600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 56.82s\n",
      "                               ETA: 628.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 83/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.574s, learning 0.122s)\n",
      "               Value function loss: 8074.6458\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1179.39\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 16800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 57.52s\n",
      "                               ETA: 627.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 84/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.564s, learning 0.119s)\n",
      "               Value function loss: 12817.0371\n",
      "                    Surrogate loss: 0.0036\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1183.35\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 17000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 58.20s\n",
      "                               ETA: 627.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 85/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.560s, learning 0.119s)\n",
      "               Value function loss: 14677.3419\n",
      "                    Surrogate loss: 0.0244\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1188.35\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 17200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 58.88s\n",
      "                               ETA: 626.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 86/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.555s, learning 0.124s)\n",
      "               Value function loss: 6887.8527\n",
      "                    Surrogate loss: 0.0038\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1187.11\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 17400\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 59.56s\n",
      "                               ETA: 625.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 87/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 293 steps/s (collection: 0.560s, learning 0.121s)\n",
      "               Value function loss: 6809.7748\n",
      "                    Surrogate loss: 0.0306\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1185.58\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 17600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 60.24s\n",
      "                               ETA: 625.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 88/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 293 steps/s (collection: 0.560s, learning 0.121s)\n",
      "               Value function loss: 6944.7492\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1184.20\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 17800\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 60.92s\n",
      "                               ETA: 624.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 89/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 300 steps/s (collection: 0.545s, learning 0.120s)\n",
      "               Value function loss: 6807.6191\n",
      "                    Surrogate loss: 0.0256\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1182.73\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 18000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 61.59s\n",
      "                               ETA: 623.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 90/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 295 steps/s (collection: 0.549s, learning 0.128s)\n",
      "               Value function loss: 6461.4737\n",
      "                    Surrogate loss: 0.0060\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1181.86\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 18200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 62.26s\n",
      "                               ETA: 622.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 91/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 295 steps/s (collection: 0.556s, learning 0.121s)\n",
      "               Value function loss: 8023.4036\n",
      "                    Surrogate loss: 0.0071\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1181.96\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 18400\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 62.94s\n",
      "                               ETA: 621.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 92/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.561s, learning 0.125s)\n",
      "               Value function loss: 5990.1348\n",
      "                    Surrogate loss: 0.0086\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1180.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 18600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 63.63s\n",
      "                               ETA: 621.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 93/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.554s, learning 0.121s)\n",
      "               Value function loss: 6908.5530\n",
      "                    Surrogate loss: 0.0030\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1179.54\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 18800\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 64.30s\n",
      "                               ETA: 620.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 94/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.571s, learning 0.125s)\n",
      "               Value function loss: 6918.4865\n",
      "                    Surrogate loss: 0.0000\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1178.77\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 19000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 65.00s\n",
      "                               ETA: 619.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 95/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 300 steps/s (collection: 0.546s, learning 0.120s)\n",
      "               Value function loss: 8481.2647\n",
      "                    Surrogate loss: -0.0023\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1178.91\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 19200\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 65.66s\n",
      "                               ETA: 619.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 96/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 297 steps/s (collection: 0.551s, learning 0.122s)\n",
      "               Value function loss: 6944.0115\n",
      "                    Surrogate loss: 0.0238\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1178.46\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 19400\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 66.34s\n",
      "                               ETA: 618.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 97/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.559s, learning 0.120s)\n",
      "               Value function loss: 7952.7116\n",
      "                    Surrogate loss: 0.0481\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1178.46\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 19600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 67.02s\n",
      "                               ETA: 617.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 98/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 300 steps/s (collection: 0.546s, learning 0.119s)\n",
      "               Value function loss: 7059.5359\n",
      "                    Surrogate loss: 0.0156\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1177.99\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 19800\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 67.68s\n",
      "                               ETA: 616.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 99/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 298 steps/s (collection: 0.551s, learning 0.119s)\n",
      "               Value function loss: 7236.8434\n",
      "                    Surrogate loss: 0.0398\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1177.74\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 20000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 68.35s\n",
      "                               ETA: 615.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 100/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.587s, learning 0.122s)\n",
      "               Value function loss: 7576.2354\n",
      "                    Surrogate loss: 0.0098\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1172.74\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 20200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 69.06s\n",
      "                               ETA: 615.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 101/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 293 steps/s (collection: 0.565s, learning 0.116s)\n",
      "               Value function loss: 8436.9128\n",
      "                    Surrogate loss: 0.0279\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1178.54\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 20400\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 69.74s\n",
      "                               ETA: 614.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 102/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 297 steps/s (collection: 0.552s, learning 0.121s)\n",
      "               Value function loss: 8455.5352\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1180.17\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 20600\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 70.41s\n",
      "                               ETA: 613.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 103/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 304 steps/s (collection: 0.539s, learning 0.118s)\n",
      "               Value function loss: 8134.3746\n",
      "                    Surrogate loss: 0.0253\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1181.58\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 20800\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 71.07s\n",
      "                               ETA: 613.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 104/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 297 steps/s (collection: 0.549s, learning 0.123s)\n",
      "               Value function loss: 12801.7016\n",
      "                    Surrogate loss: 0.0068\n",
      "             Mean action noise std: 0.12\n",
      "                 Mean total reward: -1189.15\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 21000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 71.74s\n",
      "                               ETA: 612.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 105/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 303 steps/s (collection: 0.541s, learning 0.117s)\n",
      "               Value function loss: 7376.3181\n",
      "                    Surrogate loss: 0.0213\n",
      "             Mean action noise std: 0.13\n",
      "                 Mean total reward: -1183.69\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 21200\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 72.40s\n",
      "                               ETA: 611.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 106/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 300 steps/s (collection: 0.546s, learning 0.119s)\n",
      "               Value function loss: 6602.4168\n",
      "                    Surrogate loss: 0.0042\n",
      "             Mean action noise std: 0.13\n",
      "                 Mean total reward: -1180.50\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 21400\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 73.07s\n",
      "                               ETA: 610.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 107/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 298 steps/s (collection: 0.546s, learning 0.123s)\n",
      "               Value function loss: 8606.6529\n",
      "                    Surrogate loss: 0.0249\n",
      "             Mean action noise std: 0.13\n",
      "                 Mean total reward: -1184.02\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 21600\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 73.74s\n",
      "                               ETA: 609.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 108/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.578s, learning 0.121s)\n",
      "               Value function loss: 6460.3402\n",
      "                    Surrogate loss: 0.0065\n",
      "             Mean action noise std: 0.13\n",
      "                 Mean total reward: -1187.69\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 21800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 74.43s\n",
      "                               ETA: 609.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 109/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.592s, learning 0.120s)\n",
      "               Value function loss: 10313.4064\n",
      "                    Surrogate loss: 0.0576\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1192.34\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 22000\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 75.15s\n",
      "                               ETA: 608.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 110/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.569s, learning 0.118s)\n",
      "               Value function loss: 10102.0366\n",
      "                    Surrogate loss: 0.0050\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1196.96\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 22200\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 75.83s\n",
      "                               ETA: 608.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 111/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.550s, learning 0.125s)\n",
      "               Value function loss: 7404.9003\n",
      "                    Surrogate loss: 0.0050\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1194.85\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 22400\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 76.51s\n",
      "                               ETA: 607.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 112/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.564s, learning 0.121s)\n",
      "               Value function loss: 7948.2134\n",
      "                    Surrogate loss: 0.0344\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1197.20\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 22600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 77.19s\n",
      "                               ETA: 606.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 113/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.593s, learning 0.128s)\n",
      "               Value function loss: 6489.5878\n",
      "                    Surrogate loss: 0.0143\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1196.59\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 22800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 77.91s\n",
      "                               ETA: 606.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 114/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 290 steps/s (collection: 0.565s, learning 0.123s)\n",
      "               Value function loss: 8947.2282\n",
      "                    Surrogate loss: 0.0102\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1196.27\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 23000\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 78.60s\n",
      "                               ETA: 605.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 115/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 295 steps/s (collection: 0.558s, learning 0.119s)\n",
      "               Value function loss: 8756.3164\n",
      "                    Surrogate loss: 0.0081\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1199.01\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 23200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 79.28s\n",
      "                               ETA: 604.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 116/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.572s, learning 0.120s)\n",
      "               Value function loss: 6539.5036\n",
      "                    Surrogate loss: 0.0012\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1193.52\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 23400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 79.97s\n",
      "                               ETA: 604.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 117/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.568s, learning 0.119s)\n",
      "               Value function loss: 8288.9440\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1193.56\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 23600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 80.66s\n",
      "                               ETA: 603.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 118/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.564s, learning 0.119s)\n",
      "               Value function loss: 7261.4482\n",
      "                    Surrogate loss: 0.0137\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1187.56\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 23800\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 81.34s\n",
      "                               ETA: 602.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 119/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.572s, learning 0.127s)\n",
      "               Value function loss: 5452.8406\n",
      "                    Surrogate loss: 0.0360\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1187.89\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 24000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 82.04s\n",
      "                               ETA: 602.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 120/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 231 steps/s (collection: 0.723s, learning 0.140s)\n",
      "               Value function loss: 7488.0258\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1189.79\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 24200\n",
      "                    Iteration time: 0.86s\n",
      "                        Total time: 82.90s\n",
      "                               ETA: 602.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 121/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 304 steps/s (collection: 0.536s, learning 0.121s)\n",
      "               Value function loss: 7609.5572\n",
      "                    Surrogate loss: 0.0145\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1186.59\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 24400\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 83.56s\n",
      "                               ETA: 602.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 122/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.564s, learning 0.126s)\n",
      "               Value function loss: 6605.8686\n",
      "                    Surrogate loss: 0.0037\n",
      "             Mean action noise std: 0.14\n",
      "                 Mean total reward: -1187.88\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 24600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 84.25s\n",
      "                               ETA: 601.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 123/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.587s, learning 0.126s)\n",
      "               Value function loss: 7193.8419\n",
      "                    Surrogate loss: 0.0163\n",
      "             Mean action noise std: 0.15\n",
      "                 Mean total reward: -1189.29\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 24800\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 84.96s\n",
      "                               ETA: 600.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 124/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.580s, learning 0.114s)\n",
      "               Value function loss: 6453.6683\n",
      "                    Surrogate loss: 0.0259\n",
      "             Mean action noise std: 0.15\n",
      "                 Mean total reward: -1189.98\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 25000\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 85.66s\n",
      "                               ETA: 600.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 125/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 302 steps/s (collection: 0.540s, learning 0.122s)\n",
      "               Value function loss: 12540.9179\n",
      "                    Surrogate loss: 0.0072\n",
      "             Mean action noise std: 0.15\n",
      "                 Mean total reward: -1195.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 25200\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 86.32s\n",
      "                               ETA: 599.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 126/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.569s, learning 0.124s)\n",
      "               Value function loss: 6279.1907\n",
      "                    Surrogate loss: 0.0258\n",
      "             Mean action noise std: 0.15\n",
      "                 Mean total reward: -1195.94\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 25400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 87.01s\n",
      "                               ETA: 598.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 127/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.576s, learning 0.131s)\n",
      "               Value function loss: 6326.5670\n",
      "                    Surrogate loss: 0.0020\n",
      "             Mean action noise std: 0.15\n",
      "                 Mean total reward: -1193.31\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 25600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 87.72s\n",
      "                               ETA: 598.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 128/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.575s, learning 0.119s)\n",
      "               Value function loss: 6803.3147\n",
      "                    Surrogate loss: 0.1415\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1193.78\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 25800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 88.41s\n",
      "                               ETA: 597.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 129/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 298 steps/s (collection: 0.551s, learning 0.120s)\n",
      "               Value function loss: 7492.8923\n",
      "                    Surrogate loss: 0.0115\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1192.17\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 26000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 89.09s\n",
      "                               ETA: 596.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 130/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.573s, learning 0.126s)\n",
      "               Value function loss: 19152.2840\n",
      "                    Surrogate loss: 0.0079\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1199.93\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 26200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 89.78s\n",
      "                               ETA: 596.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 131/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.568s, learning 0.122s)\n",
      "               Value function loss: 12763.8894\n",
      "                    Surrogate loss: 0.0110\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1200.22\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 26400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 90.47s\n",
      "                               ETA: 595.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 132/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.560s, learning 0.120s)\n",
      "               Value function loss: 5560.2068\n",
      "                    Surrogate loss: 0.0074\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1200.04\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 26600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 91.15s\n",
      "                               ETA: 594.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 133/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.556s, learning 0.122s)\n",
      "               Value function loss: 9227.5857\n",
      "                    Surrogate loss: 0.0097\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1203.35\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 26800\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 91.83s\n",
      "                               ETA: 594.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 134/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 293 steps/s (collection: 0.554s, learning 0.128s)\n",
      "               Value function loss: 16488.0721\n",
      "                    Surrogate loss: 0.0150\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1207.44\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 27000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 92.51s\n",
      "                               ETA: 593.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 135/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.567s, learning 0.120s)\n",
      "               Value function loss: 5907.5995\n",
      "                    Surrogate loss: 0.0025\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1207.60\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 27200\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 93.20s\n",
      "                               ETA: 592.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 136/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.571s, learning 0.123s)\n",
      "               Value function loss: 6950.8109\n",
      "                    Surrogate loss: 0.0952\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1208.23\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 27400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 93.90s\n",
      "                               ETA: 592.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 137/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.578s, learning 0.121s)\n",
      "               Value function loss: 6378.0862\n",
      "                    Surrogate loss: 0.0173\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1208.93\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 27600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 94.59s\n",
      "                               ETA: 591.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 138/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.581s, learning 0.126s)\n",
      "               Value function loss: 12538.9985\n",
      "                    Surrogate loss: 0.0047\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1214.07\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 27800\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 95.30s\n",
      "                               ETA: 591.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 139/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.569s, learning 0.122s)\n",
      "               Value function loss: 8208.4285\n",
      "                    Surrogate loss: 0.0148\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1215.53\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 28000\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 95.99s\n",
      "                               ETA: 590.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 140/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.576s, learning 0.123s)\n",
      "               Value function loss: 7602.3122\n",
      "                    Surrogate loss: 0.0029\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1216.67\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 28200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 96.69s\n",
      "                               ETA: 589.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 141/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 299 steps/s (collection: 0.547s, learning 0.120s)\n",
      "               Value function loss: 5290.1068\n",
      "                    Surrogate loss: 0.0209\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1214.12\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 28400\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 97.36s\n",
      "                               ETA: 589.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 142/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 298 steps/s (collection: 0.549s, learning 0.120s)\n",
      "               Value function loss: 8556.3819\n",
      "                    Surrogate loss: 0.0004\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1215.84\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 28600\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 98.03s\n",
      "                               ETA: 588.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 143/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.569s, learning 0.121s)\n",
      "               Value function loss: 8439.2232\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1217.42\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 28800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 98.72s\n",
      "                               ETA: 587.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 144/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 297 steps/s (collection: 0.549s, learning 0.124s)\n",
      "               Value function loss: 8045.9209\n",
      "                    Surrogate loss: 0.1211\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1218.80\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 29000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 99.39s\n",
      "                               ETA: 586.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 145/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 300 steps/s (collection: 0.548s, learning 0.117s)\n",
      "               Value function loss: 8296.1805\n",
      "                    Surrogate loss: 0.0022\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1219.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 29200\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 100.06s\n",
      "                               ETA: 585.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 146/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 295 steps/s (collection: 0.547s, learning 0.129s)\n",
      "               Value function loss: 8590.2336\n",
      "                    Surrogate loss: 0.0026\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1221.28\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 29400\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 100.73s\n",
      "                               ETA: 585.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 147/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.558s, learning 0.121s)\n",
      "               Value function loss: 6050.7459\n",
      "                    Surrogate loss: 0.0168\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1222.21\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 29600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 101.41s\n",
      "                               ETA: 584.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 148/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.572s, learning 0.128s)\n",
      "               Value function loss: 7479.4214\n",
      "                    Surrogate loss: 0.0083\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1214.87\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 29800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 102.11s\n",
      "                               ETA: 583.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 149/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.554s, learning 0.121s)\n",
      "               Value function loss: 5442.4332\n",
      "                    Surrogate loss: 0.0248\n",
      "             Mean action noise std: 0.15\n",
      "                 Mean total reward: -1215.15\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 30000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 102.79s\n",
      "                               ETA: 583.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 150/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.570s, learning 0.127s)\n",
      "               Value function loss: 9253.7853\n",
      "                    Surrogate loss: 0.0003\n",
      "             Mean action noise std: 0.15\n",
      "                 Mean total reward: -1209.66\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 30200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 103.48s\n",
      "                               ETA: 582.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 151/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.557s, learning 0.118s)\n",
      "               Value function loss: 8597.3932\n",
      "                    Surrogate loss: 0.0296\n",
      "             Mean action noise std: 0.15\n",
      "                 Mean total reward: -1211.69\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 30400\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 104.16s\n",
      "                               ETA: 581.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 152/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.583s, learning 0.124s)\n",
      "               Value function loss: 14921.7830\n",
      "                    Surrogate loss: 0.0199\n",
      "             Mean action noise std: 0.15\n",
      "                 Mean total reward: -1218.26\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 30600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 104.86s\n",
      "                               ETA: 581.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 153/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.572s, learning 0.118s)\n",
      "               Value function loss: 13575.6678\n",
      "                    Surrogate loss: 0.0658\n",
      "             Mean action noise std: 0.15\n",
      "                 Mean total reward: -1220.03\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 30800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 105.55s\n",
      "                               ETA: 580.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 154/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.576s, learning 0.122s)\n",
      "               Value function loss: 9849.9348\n",
      "                    Surrogate loss: 0.0065\n",
      "             Mean action noise std: 0.15\n",
      "                 Mean total reward: -1223.53\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 31000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 106.25s\n",
      "                               ETA: 579.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 155/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.561s, learning 0.118s)\n",
      "               Value function loss: 11259.6045\n",
      "                    Surrogate loss: 0.0284\n",
      "             Mean action noise std: 0.15\n",
      "                 Mean total reward: -1227.81\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 31200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 106.93s\n",
      "                               ETA: 579.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 156/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.580s, learning 0.116s)\n",
      "               Value function loss: 9012.8297\n",
      "                    Surrogate loss: 0.0038\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1230.87\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 31400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 107.63s\n",
      "                               ETA: 578.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 157/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 290 steps/s (collection: 0.569s, learning 0.120s)\n",
      "               Value function loss: 9009.3587\n",
      "                    Surrogate loss: 0.0223\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1230.06\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 31600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 108.32s\n",
      "                               ETA: 577.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 158/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.564s, learning 0.121s)\n",
      "               Value function loss: 8394.4031\n",
      "                    Surrogate loss: -0.0025\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1233.00\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 31800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 109.00s\n",
      "                               ETA: 577.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 159/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 300 steps/s (collection: 0.543s, learning 0.122s)\n",
      "               Value function loss: 9778.8121\n",
      "                    Surrogate loss: 0.0556\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1233.63\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 32000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 109.67s\n",
      "                               ETA: 576.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 160/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.570s, learning 0.125s)\n",
      "               Value function loss: 11982.5581\n",
      "                    Surrogate loss: 0.0276\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1234.22\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 32200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 110.36s\n",
      "                               ETA: 575.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 161/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.605s, learning 0.121s)\n",
      "               Value function loss: 7730.9813\n",
      "                    Surrogate loss: 0.0067\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1235.16\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 32400\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 111.09s\n",
      "                               ETA: 575.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 162/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.581s, learning 0.125s)\n",
      "               Value function loss: 5281.1402\n",
      "                    Surrogate loss: 0.0202\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1234.69\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 32600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 111.79s\n",
      "                               ETA: 574.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 163/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.595s, learning 0.125s)\n",
      "               Value function loss: 4113.0846\n",
      "                    Surrogate loss: 0.0054\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1233.75\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 32800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 112.51s\n",
      "                               ETA: 574.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 164/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.566s, learning 0.118s)\n",
      "               Value function loss: 9564.1476\n",
      "                    Surrogate loss: 0.0245\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1236.91\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 33000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 113.20s\n",
      "                               ETA: 573.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 165/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 293 steps/s (collection: 0.558s, learning 0.123s)\n",
      "               Value function loss: 7321.8321\n",
      "                    Surrogate loss: 0.0081\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1233.26\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 33200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 113.88s\n",
      "                               ETA: 572.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 166/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.605s, learning 0.125s)\n",
      "               Value function loss: 12843.6954\n",
      "                    Surrogate loss: 0.0458\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1231.97\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 33400\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 114.61s\n",
      "                               ETA: 572.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 167/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.570s, learning 0.129s)\n",
      "               Value function loss: 5899.8660\n",
      "                    Surrogate loss: 0.0122\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1232.43\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 33600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 115.31s\n",
      "                               ETA: 571.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 168/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.598s, learning 0.133s)\n",
      "               Value function loss: 11433.7137\n",
      "                    Surrogate loss: 0.0292\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1230.31\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 33800\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 116.04s\n",
      "                               ETA: 571.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 169/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 270 steps/s (collection: 0.616s, learning 0.125s)\n",
      "               Value function loss: 6139.9681\n",
      "                    Surrogate loss: 0.0027\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1227.20\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 34000\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 116.78s\n",
      "                               ETA: 570.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 170/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.587s, learning 0.120s)\n",
      "               Value function loss: 7617.7635\n",
      "                    Surrogate loss: 0.0491\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1228.08\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 34200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 117.49s\n",
      "                               ETA: 570.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 171/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 297 steps/s (collection: 0.555s, learning 0.118s)\n",
      "               Value function loss: 8787.0046\n",
      "                    Surrogate loss: 0.0008\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1231.97\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 34400\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 118.16s\n",
      "                               ETA: 569.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 172/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.598s, learning 0.124s)\n",
      "               Value function loss: 9369.8837\n",
      "                    Surrogate loss: 0.0041\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1235.30\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 34600\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 118.88s\n",
      "                               ETA: 569.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 173/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.573s, learning 0.120s)\n",
      "               Value function loss: 5044.3168\n",
      "                    Surrogate loss: 0.0094\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1234.84\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 34800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 119.57s\n",
      "                               ETA: 568.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 174/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 297 steps/s (collection: 0.560s, learning 0.113s)\n",
      "               Value function loss: 7941.4758\n",
      "                    Surrogate loss: 0.0217\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1233.95\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 35000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 120.25s\n",
      "                               ETA: 567.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 175/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.575s, learning 0.119s)\n",
      "               Value function loss: 4729.3885\n",
      "                    Surrogate loss: 0.0019\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1231.90\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 35200\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 120.94s\n",
      "                               ETA: 566.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 176/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 299 steps/s (collection: 0.550s, learning 0.118s)\n",
      "               Value function loss: 7009.7044\n",
      "                    Surrogate loss: 0.0040\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1233.99\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 35400\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 121.61s\n",
      "                               ETA: 566.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 177/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.567s, learning 0.138s)\n",
      "               Value function loss: 8194.5119\n",
      "                    Surrogate loss: 0.0033\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1237.77\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 35600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 122.31s\n",
      "                               ETA: 565.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 178/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.586s, learning 0.122s)\n",
      "               Value function loss: 3800.2272\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1236.18\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 35800\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 123.02s\n",
      "                               ETA: 564.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 179/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 290 steps/s (collection: 0.570s, learning 0.118s)\n",
      "               Value function loss: 4389.5686\n",
      "                    Surrogate loss: 0.0301\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1229.89\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 36000\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 123.71s\n",
      "                               ETA: 564.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 180/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.550s, learning 0.125s)\n",
      "               Value function loss: 4163.8103\n",
      "                    Surrogate loss: 0.0149\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1224.81\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 36200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 124.39s\n",
      "                               ETA: 563.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 181/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.563s, learning 0.122s)\n",
      "               Value function loss: 3822.3674\n",
      "                    Surrogate loss: 0.0263\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1214.83\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 36400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 125.07s\n",
      "                               ETA: 562.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 182/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.585s, learning 0.137s)\n",
      "               Value function loss: 14913.1879\n",
      "                    Surrogate loss: 0.0047\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1220.45\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 36600\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 125.79s\n",
      "                               ETA: 562.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 183/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 301 steps/s (collection: 0.545s, learning 0.119s)\n",
      "               Value function loss: 4874.7513\n",
      "                    Surrogate loss: 0.0189\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1218.65\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 36800\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 126.46s\n",
      "                               ETA: 561.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 184/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 299 steps/s (collection: 0.546s, learning 0.122s)\n",
      "               Value function loss: 7395.1578\n",
      "                    Surrogate loss: -0.0022\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1215.48\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 37000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 127.12s\n",
      "                               ETA: 560.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 185/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.598s, learning 0.129s)\n",
      "               Value function loss: 12361.8991\n",
      "                    Surrogate loss: 0.0219\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1215.04\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 37200\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 127.85s\n",
      "                               ETA: 560.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 186/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.587s, learning 0.122s)\n",
      "               Value function loss: 10772.9981\n",
      "                    Surrogate loss: 0.0315\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1219.01\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 37400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 128.56s\n",
      "                               ETA: 559.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 187/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.579s, learning 0.132s)\n",
      "               Value function loss: 7962.0743\n",
      "                    Surrogate loss: 0.0337\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1220.66\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 37600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 129.27s\n",
      "                               ETA: 559.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 188/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.585s, learning 0.123s)\n",
      "               Value function loss: 8031.1428\n",
      "                    Surrogate loss: 0.0127\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1222.91\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 37800\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 129.98s\n",
      "                               ETA: 558.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 189/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 265 steps/s (collection: 0.630s, learning 0.124s)\n",
      "               Value function loss: 4843.9900\n",
      "                    Surrogate loss: -0.0011\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1222.30\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 38000\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 130.73s\n",
      "                               ETA: 558.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 190/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.557s, learning 0.126s)\n",
      "               Value function loss: 7783.3055\n",
      "                    Surrogate loss: -0.0003\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1223.37\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 38200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 131.42s\n",
      "                               ETA: 557.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 191/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 290 steps/s (collection: 0.561s, learning 0.128s)\n",
      "               Value function loss: 7865.8848\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1223.67\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 38400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 132.11s\n",
      "                               ETA: 556.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 192/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.572s, learning 0.131s)\n",
      "               Value function loss: 4622.9646\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1223.03\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 38600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 132.81s\n",
      "                               ETA: 556.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 193/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.585s, learning 0.119s)\n",
      "               Value function loss: 7065.5075\n",
      "                    Surrogate loss: 0.0043\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1224.38\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 38800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 133.51s\n",
      "                               ETA: 555.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 194/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.565s, learning 0.119s)\n",
      "               Value function loss: 7589.4944\n",
      "                    Surrogate loss: 0.0085\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1225.11\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 39000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 134.20s\n",
      "                               ETA: 554.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 195/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.557s, learning 0.123s)\n",
      "               Value function loss: 4821.4640\n",
      "                    Surrogate loss: 0.0248\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1223.13\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 39200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 134.88s\n",
      "                               ETA: 554.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 196/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.593s, learning 0.123s)\n",
      "               Value function loss: 11913.1083\n",
      "                    Surrogate loss: 0.0147\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1227.29\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 39400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 135.59s\n",
      "                               ETA: 553.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 197/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.578s, learning 0.122s)\n",
      "               Value function loss: 5589.1570\n",
      "                    Surrogate loss: 0.0231\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1226.46\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 39600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 136.29s\n",
      "                               ETA: 552.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 198/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 258 steps/s (collection: 0.647s, learning 0.127s)\n",
      "               Value function loss: 7821.6244\n",
      "                    Surrogate loss: -0.0013\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1227.86\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 39800\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 137.07s\n",
      "                               ETA: 552.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 199/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 268 steps/s (collection: 0.602s, learning 0.144s)\n",
      "               Value function loss: 6333.7426\n",
      "                    Surrogate loss: 0.0280\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1227.96\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 40000\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 137.81s\n",
      "                               ETA: 551.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 200/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.601s, learning 0.121s)\n",
      "               Value function loss: 4020.3332\n",
      "                    Surrogate loss: -0.0012\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1225.81\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 40200\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 138.53s\n",
      "                               ETA: 551.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 201/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.572s, learning 0.147s)\n",
      "               Value function loss: 8414.7909\n",
      "                    Surrogate loss: 0.0083\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1227.13\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 40400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 139.25s\n",
      "                               ETA: 550.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 202/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 264 steps/s (collection: 0.607s, learning 0.150s)\n",
      "               Value function loss: 9259.3480\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1229.00\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 40600\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 140.01s\n",
      "                               ETA: 550.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 203/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 262 steps/s (collection: 0.634s, learning 0.129s)\n",
      "               Value function loss: 6276.4129\n",
      "                    Surrogate loss: 0.0249\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1227.57\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 40800\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 140.77s\n",
      "                               ETA: 550.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 204/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 267 steps/s (collection: 0.616s, learning 0.132s)\n",
      "               Value function loss: 17896.2260\n",
      "                    Surrogate loss: 0.0009\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1231.01\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 41000\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 141.52s\n",
      "                               ETA: 549.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 205/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 271 steps/s (collection: 0.615s, learning 0.123s)\n",
      "               Value function loss: 7594.4927\n",
      "                    Surrogate loss: 0.0894\n",
      "             Mean action noise std: 0.15\n",
      "                 Mean total reward: -1232.11\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 41200\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 142.26s\n",
      "                               ETA: 549.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 206/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 271 steps/s (collection: 0.606s, learning 0.130s)\n",
      "               Value function loss: 15171.1053\n",
      "                    Surrogate loss: 0.0099\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1238.50\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 41400\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 143.00s\n",
      "                               ETA: 548.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 207/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 262 steps/s (collection: 0.625s, learning 0.136s)\n",
      "               Value function loss: 7389.9738\n",
      "                    Surrogate loss: 0.0073\n",
      "             Mean action noise std: 0.16\n",
      "                 Mean total reward: -1238.40\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 41600\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 143.76s\n",
      "                               ETA: 548.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 208/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 264 steps/s (collection: 0.618s, learning 0.138s)\n",
      "               Value function loss: 4079.8318\n",
      "                    Surrogate loss: 0.0242\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1237.08\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 41800\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 144.51s\n",
      "                               ETA: 547.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 209/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 259 steps/s (collection: 0.635s, learning 0.135s)\n",
      "               Value function loss: 5548.1278\n",
      "                    Surrogate loss: 0.0061\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1234.37\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 42000\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 145.28s\n",
      "                               ETA: 547.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 210/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 256 steps/s (collection: 0.647s, learning 0.134s)\n",
      "               Value function loss: 4970.5290\n",
      "                    Surrogate loss: 0.0258\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1231.49\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 42200\n",
      "                    Iteration time: 0.78s\n",
      "                        Total time: 146.06s\n",
      "                               ETA: 546.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 211/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 234 steps/s (collection: 0.717s, learning 0.136s)\n",
      "               Value function loss: 14031.9646\n",
      "                    Surrogate loss: -0.0005\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1236.64\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 42400\n",
      "                    Iteration time: 0.85s\n",
      "                        Total time: 146.92s\n",
      "                               ETA: 546.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 212/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 240 steps/s (collection: 0.699s, learning 0.134s)\n",
      "               Value function loss: 5985.2642\n",
      "                    Surrogate loss: 0.0025\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1235.57\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 42600\n",
      "                    Iteration time: 0.83s\n",
      "                        Total time: 147.75s\n",
      "                               ETA: 546.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 213/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.577s, learning 0.122s)\n",
      "               Value function loss: 4627.7135\n",
      "                    Surrogate loss: 0.0078\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1234.45\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 42800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 148.45s\n",
      "                               ETA: 545.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 214/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 270 steps/s (collection: 0.615s, learning 0.124s)\n",
      "               Value function loss: 6623.3438\n",
      "                    Surrogate loss: 0.1501\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1233.97\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 43000\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 149.19s\n",
      "                               ETA: 545.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 215/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.551s, learning 0.133s)\n",
      "               Value function loss: 7496.2772\n",
      "                    Surrogate loss: 0.0021\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1233.74\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 43200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 149.87s\n",
      "                               ETA: 544.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 216/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.599s, learning 0.123s)\n",
      "               Value function loss: 12668.3163\n",
      "                    Surrogate loss: 0.0031\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1239.27\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 43400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 150.59s\n",
      "                               ETA: 544.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 217/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.599s, learning 0.127s)\n",
      "               Value function loss: 5656.7733\n",
      "                    Surrogate loss: -0.0023\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1237.90\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 43600\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 151.32s\n",
      "                               ETA: 543.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 218/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.584s, learning 0.133s)\n",
      "               Value function loss: 5788.8759\n",
      "                    Surrogate loss: -0.0008\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1237.43\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 43800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 152.04s\n",
      "                               ETA: 542.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 219/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 264 steps/s (collection: 0.618s, learning 0.137s)\n",
      "               Value function loss: 5529.4385\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 0.17\n",
      "                 Mean total reward: -1238.11\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 44000\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 152.79s\n",
      "                               ETA: 542.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 220/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.604s, learning 0.121s)\n",
      "               Value function loss: 7177.3500\n",
      "                    Surrogate loss: 0.0093\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1238.06\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 44200\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 153.52s\n",
      "                               ETA: 541.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 221/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.573s, learning 0.126s)\n",
      "               Value function loss: 6424.1185\n",
      "                    Surrogate loss: 0.0011\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1237.99\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 44400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 154.22s\n",
      "                               ETA: 541.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 222/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.581s, learning 0.122s)\n",
      "               Value function loss: 6044.2332\n",
      "                    Surrogate loss: 0.1518\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1238.23\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 44600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 154.92s\n",
      "                               ETA: 540.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 223/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 293 steps/s (collection: 0.557s, learning 0.125s)\n",
      "               Value function loss: 4788.8636\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1236.86\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 44800\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 155.60s\n",
      "                               ETA: 539.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 224/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.578s, learning 0.122s)\n",
      "               Value function loss: 13490.5642\n",
      "                    Surrogate loss: 0.0108\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1242.84\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 45000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 156.30s\n",
      "                               ETA: 539.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 225/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.571s, learning 0.116s)\n",
      "               Value function loss: 7679.6294\n",
      "                    Surrogate loss: 0.0108\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1240.41\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 45200\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 156.99s\n",
      "                               ETA: 538.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 226/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.566s, learning 0.128s)\n",
      "               Value function loss: 13655.8700\n",
      "                    Surrogate loss: 0.0203\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1246.04\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 45400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 157.68s\n",
      "                               ETA: 537.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 227/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.577s, learning 0.123s)\n",
      "               Value function loss: 7704.8878\n",
      "                    Surrogate loss: 0.0705\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1247.59\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 45600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 158.38s\n",
      "                               ETA: 537.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 228/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.575s, learning 0.123s)\n",
      "               Value function loss: 8177.1764\n",
      "                    Surrogate loss: -0.0005\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1248.77\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 45800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 159.08s\n",
      "                               ETA: 536.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 229/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.578s, learning 0.123s)\n",
      "               Value function loss: 7483.9014\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1248.88\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 46000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 159.78s\n",
      "                               ETA: 535.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 230/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 297 steps/s (collection: 0.554s, learning 0.119s)\n",
      "               Value function loss: 4428.2919\n",
      "                    Surrogate loss: 0.0500\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1239.84\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 46200\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 160.45s\n",
      "                               ETA: 534.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 231/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.575s, learning 0.119s)\n",
      "               Value function loss: 6106.0361\n",
      "                    Surrogate loss: 0.0043\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1235.87\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 46400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 161.15s\n",
      "                               ETA: 534.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 232/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.598s, learning 0.117s)\n",
      "               Value function loss: 6975.3640\n",
      "                    Surrogate loss: 0.0116\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1237.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 46600\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 161.86s\n",
      "                               ETA: 533.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 233/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 297 steps/s (collection: 0.548s, learning 0.125s)\n",
      "               Value function loss: 8207.2823\n",
      "                    Surrogate loss: 0.0246\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1237.32\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 46800\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 162.54s\n",
      "                               ETA: 532.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 234/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 295 steps/s (collection: 0.558s, learning 0.118s)\n",
      "               Value function loss: 6309.4094\n",
      "                    Surrogate loss: 0.0099\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1231.96\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 47000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 163.21s\n",
      "                               ETA: 532.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 235/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.577s, learning 0.127s)\n",
      "               Value function loss: 7505.9664\n",
      "                    Surrogate loss: 0.0004\n",
      "             Mean action noise std: 0.21\n",
      "                 Mean total reward: -1233.27\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 47200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 163.92s\n",
      "                               ETA: 531.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 236/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.582s, learning 0.119s)\n",
      "               Value function loss: 5857.5062\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 0.21\n",
      "                 Mean total reward: -1233.15\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 47400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 164.62s\n",
      "                               ETA: 530.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 237/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.587s, learning 0.119s)\n",
      "               Value function loss: 11085.1406\n",
      "                    Surrogate loss: -0.0099\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1237.77\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 47600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 165.32s\n",
      "                               ETA: 530.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 238/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 290 steps/s (collection: 0.570s, learning 0.119s)\n",
      "               Value function loss: 6398.0424\n",
      "                    Surrogate loss: -0.0043\n",
      "             Mean action noise std: 0.19\n",
      "                 Mean total reward: -1233.23\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 47800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 166.01s\n",
      "                               ETA: 529.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 239/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.577s, learning 0.119s)\n",
      "               Value function loss: 7320.2579\n",
      "                    Surrogate loss: 0.0146\n",
      "             Mean action noise std: 0.19\n",
      "                 Mean total reward: -1233.24\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 48000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 166.71s\n",
      "                               ETA: 528.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 240/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.576s, learning 0.119s)\n",
      "               Value function loss: 7582.4663\n",
      "                    Surrogate loss: 0.0082\n",
      "             Mean action noise std: 0.19\n",
      "                 Mean total reward: -1233.58\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 48200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 167.40s\n",
      "                               ETA: 527.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 241/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.580s, learning 0.122s)\n",
      "               Value function loss: 12802.9060\n",
      "                    Surrogate loss: 0.0287\n",
      "             Mean action noise std: 0.19\n",
      "                 Mean total reward: -1239.83\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 48400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 168.10s\n",
      "                               ETA: 527.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 242/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.587s, learning 0.122s)\n",
      "               Value function loss: 5754.2813\n",
      "                    Surrogate loss: 0.0162\n",
      "             Mean action noise std: 0.19\n",
      "                 Mean total reward: -1238.60\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 48600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 168.81s\n",
      "                               ETA: 526.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 243/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.569s, learning 0.130s)\n",
      "               Value function loss: 5590.5360\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 0.19\n",
      "                 Mean total reward: -1237.81\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 48800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 169.51s\n",
      "                               ETA: 525.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 244/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.578s, learning 0.123s)\n",
      "               Value function loss: 5056.4135\n",
      "                    Surrogate loss: 0.0461\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1236.34\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 49000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 170.21s\n",
      "                               ETA: 525.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 245/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.585s, learning 0.120s)\n",
      "               Value function loss: 5339.8845\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1234.62\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 49200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 170.92s\n",
      "                               ETA: 524.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 246/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.578s, learning 0.122s)\n",
      "               Value function loss: 5276.1718\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 0.19\n",
      "                 Mean total reward: -1232.86\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 49400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 171.62s\n",
      "                               ETA: 523.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 247/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.575s, learning 0.125s)\n",
      "               Value function loss: 12282.8627\n",
      "                    Surrogate loss: 0.0056\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1238.00\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 49600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 172.32s\n",
      "                               ETA: 523.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 248/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.562s, learning 0.117s)\n",
      "               Value function loss: 4647.6380\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1236.44\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 49800\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 173.00s\n",
      "                               ETA: 522.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 249/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 299 steps/s (collection: 0.548s, learning 0.120s)\n",
      "               Value function loss: 13862.9304\n",
      "                    Surrogate loss: 0.0068\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1243.04\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 50000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 173.67s\n",
      "                               ETA: 521.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 250/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.561s, learning 0.123s)\n",
      "               Value function loss: 4553.2839\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1240.08\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 50200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 174.35s\n",
      "                               ETA: 521.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 251/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 290 steps/s (collection: 0.567s, learning 0.121s)\n",
      "               Value function loss: 5632.8945\n",
      "                    Surrogate loss: 0.0014\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1238.52\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 50400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 175.04s\n",
      "                               ETA: 520.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 252/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.595s, learning 0.122s)\n",
      "               Value function loss: 4624.0034\n",
      "                    Surrogate loss: 0.1444\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1231.96\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 50600\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 175.75s\n",
      "                               ETA: 519.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 253/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.592s, learning 0.130s)\n",
      "               Value function loss: 4855.2426\n",
      "                    Surrogate loss: 0.0232\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1226.71\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 50800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 176.48s\n",
      "                               ETA: 519.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 254/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.584s, learning 0.123s)\n",
      "               Value function loss: 5105.9996\n",
      "                    Surrogate loss: 0.0080\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1223.77\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 51000\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 177.18s\n",
      "                               ETA: 518.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 255/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 267 steps/s (collection: 0.620s, learning 0.127s)\n",
      "               Value function loss: 5247.5500\n",
      "                    Surrogate loss: 0.0362\n",
      "             Mean action noise std: 0.18\n",
      "                 Mean total reward: -1220.14\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 51200\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 177.93s\n",
      "                               ETA: 517.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 256/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.593s, learning 0.130s)\n",
      "               Value function loss: 5999.4223\n",
      "                    Surrogate loss: 0.0187\n",
      "             Mean action noise std: 0.19\n",
      "                 Mean total reward: -1218.83\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 51400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 178.65s\n",
      "                               ETA: 517.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 257/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.568s, learning 0.130s)\n",
      "               Value function loss: 6302.5653\n",
      "                    Surrogate loss: 0.0098\n",
      "             Mean action noise std: 0.19\n",
      "                 Mean total reward: -1218.35\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 51600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 179.35s\n",
      "                               ETA: 516.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 258/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 301 steps/s (collection: 0.545s, learning 0.118s)\n",
      "               Value function loss: 6201.4916\n",
      "                    Surrogate loss: 0.0292\n",
      "             Mean action noise std: 0.19\n",
      "                 Mean total reward: -1217.95\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 51800\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 180.01s\n",
      "                               ETA: 515.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 259/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 299 steps/s (collection: 0.549s, learning 0.119s)\n",
      "               Value function loss: 5481.2531\n",
      "                    Surrogate loss: 0.0002\n",
      "             Mean action noise std: 0.19\n",
      "                 Mean total reward: -1215.61\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 52000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 180.68s\n",
      "                               ETA: 514.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 260/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 297 steps/s (collection: 0.550s, learning 0.122s)\n",
      "               Value function loss: 6045.6188\n",
      "                    Surrogate loss: 0.1118\n",
      "             Mean action noise std: 0.21\n",
      "                 Mean total reward: -1212.06\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 52200\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 181.35s\n",
      "                               ETA: 514.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 261/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 297 steps/s (collection: 0.550s, learning 0.122s)\n",
      "               Value function loss: 3797.0823\n",
      "                    Surrogate loss: 0.0293\n",
      "             Mean action noise std: 0.21\n",
      "                 Mean total reward: -1209.71\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 52400\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 182.03s\n",
      "                               ETA: 513.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 262/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.571s, learning 0.128s)\n",
      "               Value function loss: 6619.7674\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 0.21\n",
      "                 Mean total reward: -1211.69\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 52600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 182.73s\n",
      "                               ETA: 512.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 263/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.582s, learning 0.124s)\n",
      "               Value function loss: 6873.9999\n",
      "                    Surrogate loss: 0.0174\n",
      "             Mean action noise std: 0.22\n",
      "                 Mean total reward: -1215.03\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 52800\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 183.43s\n",
      "                               ETA: 512.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 264/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.551s, learning 0.123s)\n",
      "               Value function loss: 4350.8275\n",
      "                    Surrogate loss: 0.0016\n",
      "             Mean action noise std: 0.22\n",
      "                 Mean total reward: -1211.11\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 53000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 184.11s\n",
      "                               ETA: 511.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 265/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.571s, learning 0.123s)\n",
      "               Value function loss: 4445.5298\n",
      "                    Surrogate loss: -0.0013\n",
      "             Mean action noise std: 0.21\n",
      "                 Mean total reward: -1209.61\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 53200\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 184.80s\n",
      "                               ETA: 510.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 266/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.562s, learning 0.132s)\n",
      "               Value function loss: 4813.2657\n",
      "                    Surrogate loss: 0.0063\n",
      "             Mean action noise std: 0.22\n",
      "                 Mean total reward: -1204.18\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 53400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 185.49s\n",
      "                               ETA: 509.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 267/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.576s, learning 0.144s)\n",
      "               Value function loss: 5190.3521\n",
      "                    Surrogate loss: 0.0019\n",
      "             Mean action noise std: 0.21\n",
      "                 Mean total reward: -1204.21\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 53600\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 186.21s\n",
      "                               ETA: 509.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 268/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 268 steps/s (collection: 0.593s, learning 0.151s)\n",
      "               Value function loss: 12999.0866\n",
      "                    Surrogate loss: -0.0122\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1205.85\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 53800\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 186.96s\n",
      "                               ETA: 508.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 269/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.554s, learning 0.125s)\n",
      "               Value function loss: 3875.4203\n",
      "                    Surrogate loss: -0.0027\n",
      "             Mean action noise std: 0.19\n",
      "                 Mean total reward: -1204.51\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 54000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 187.64s\n",
      "                               ETA: 508.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 270/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.553s, learning 0.121s)\n",
      "               Value function loss: 6159.1532\n",
      "                    Surrogate loss: 0.0709\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1204.86\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 54200\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 188.31s\n",
      "                               ETA: 507.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 271/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 262 steps/s (collection: 0.626s, learning 0.137s)\n",
      "               Value function loss: 11400.8927\n",
      "                    Surrogate loss: 0.0010\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1207.38\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 54400\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 189.07s\n",
      "                               ETA: 506.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 272/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.566s, learning 0.119s)\n",
      "               Value function loss: 5541.6345\n",
      "                    Surrogate loss: 0.0006\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1204.76\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 54600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 189.76s\n",
      "                               ETA: 506.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 273/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.550s, learning 0.124s)\n",
      "               Value function loss: 4701.0765\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 0.20\n",
      "                 Mean total reward: -1205.15\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 54800\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 190.43s\n",
      "                               ETA: 505.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 274/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.552s, learning 0.133s)\n",
      "               Value function loss: 4257.2009\n",
      "                    Surrogate loss: 0.0810\n",
      "             Mean action noise std: 0.21\n",
      "                 Mean total reward: -1203.15\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 55000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 191.12s\n",
      "                               ETA: 504.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 275/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 267 steps/s (collection: 0.623s, learning 0.124s)\n",
      "               Value function loss: 12286.1380\n",
      "                    Surrogate loss: 0.0086\n",
      "             Mean action noise std: 0.21\n",
      "                 Mean total reward: -1209.58\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 55200\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 191.86s\n",
      "                               ETA: 504.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 276/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 266 steps/s (collection: 0.629s, learning 0.123s)\n",
      "               Value function loss: 2973.2247\n",
      "                    Surrogate loss: 0.0087\n",
      "             Mean action noise std: 0.21\n",
      "                 Mean total reward: -1205.95\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 55400\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 192.62s\n",
      "                               ETA: 503.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 277/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.585s, learning 0.122s)\n",
      "               Value function loss: 6072.6193\n",
      "                    Surrogate loss: 0.0032\n",
      "             Mean action noise std: 0.22\n",
      "                 Mean total reward: -1204.91\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 55600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 193.32s\n",
      "                               ETA: 502.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 278/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.569s, learning 0.123s)\n",
      "               Value function loss: 5140.2416\n",
      "                    Surrogate loss: 0.0004\n",
      "             Mean action noise std: 0.22\n",
      "                 Mean total reward: -1206.41\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 55800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 194.02s\n",
      "                               ETA: 502.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 279/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.599s, learning 0.125s)\n",
      "               Value function loss: 12626.0018\n",
      "                    Surrogate loss: 0.1168\n",
      "             Mean action noise std: 0.24\n",
      "                 Mean total reward: -1213.11\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 56000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 194.74s\n",
      "                               ETA: 501.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 280/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 269 steps/s (collection: 0.612s, learning 0.132s)\n",
      "               Value function loss: 8032.0337\n",
      "                    Surrogate loss: 0.0114\n",
      "             Mean action noise std: 0.24\n",
      "                 Mean total reward: -1217.30\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 56200\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 195.48s\n",
      "                               ETA: 500.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 281/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.582s, learning 0.122s)\n",
      "               Value function loss: 5658.6383\n",
      "                    Surrogate loss: 0.0111\n",
      "             Mean action noise std: 0.25\n",
      "                 Mean total reward: -1220.20\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 56400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 196.19s\n",
      "                               ETA: 500.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 282/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.558s, learning 0.128s)\n",
      "               Value function loss: 5767.4957\n",
      "                    Surrogate loss: 0.0048\n",
      "             Mean action noise std: 0.26\n",
      "                 Mean total reward: -1214.85\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 56600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 196.87s\n",
      "                               ETA: 499.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 283/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.565s, learning 0.127s)\n",
      "               Value function loss: 6562.4478\n",
      "                    Surrogate loss: 0.0032\n",
      "             Mean action noise std: 0.27\n",
      "                 Mean total reward: -1217.42\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 56800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 197.56s\n",
      "                               ETA: 498.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 284/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.562s, learning 0.134s)\n",
      "               Value function loss: 7614.9302\n",
      "                    Surrogate loss: -0.0058\n",
      "             Mean action noise std: 0.26\n",
      "                 Mean total reward: -1218.70\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 57000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 198.26s\n",
      "                               ETA: 498.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 285/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.577s, learning 0.129s)\n",
      "               Value function loss: 6292.6987\n",
      "                    Surrogate loss: 0.0029\n",
      "             Mean action noise std: 0.26\n",
      "                 Mean total reward: -1215.04\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 57200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 198.97s\n",
      "                               ETA: 497.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 286/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.596s, learning 0.134s)\n",
      "               Value function loss: 7274.8507\n",
      "                    Surrogate loss: -0.0011\n",
      "             Mean action noise std: 0.25\n",
      "                 Mean total reward: -1213.24\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 57400\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 199.70s\n",
      "                               ETA: 496.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 287/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.586s, learning 0.125s)\n",
      "               Value function loss: 9617.3790\n",
      "                    Surrogate loss: -0.0072\n",
      "             Mean action noise std: 0.25\n",
      "                 Mean total reward: -1215.80\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 57600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 200.41s\n",
      "                               ETA: 496.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 288/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.569s, learning 0.132s)\n",
      "               Value function loss: 9275.2290\n",
      "                    Surrogate loss: 0.0014\n",
      "             Mean action noise std: 0.24\n",
      "                 Mean total reward: -1217.38\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 57800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 201.11s\n",
      "                               ETA: 495.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 289/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.567s, learning 0.124s)\n",
      "               Value function loss: 9755.6328\n",
      "                    Surrogate loss: 0.0621\n",
      "             Mean action noise std: 0.23\n",
      "                 Mean total reward: -1222.24\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 58000\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 201.80s\n",
      "                               ETA: 494.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 290/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 293 steps/s (collection: 0.556s, learning 0.126s)\n",
      "               Value function loss: 8029.1408\n",
      "                    Surrogate loss: 0.0178\n",
      "             Mean action noise std: 0.24\n",
      "                 Mean total reward: -1223.60\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 58200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 202.48s\n",
      "                               ETA: 494.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 291/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 290 steps/s (collection: 0.565s, learning 0.124s)\n",
      "               Value function loss: 9422.0209\n",
      "                    Surrogate loss: 0.0005\n",
      "             Mean action noise std: 0.23\n",
      "                 Mean total reward: -1226.01\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 58400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 203.17s\n",
      "                               ETA: 493.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 292/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.585s, learning 0.126s)\n",
      "               Value function loss: 5346.7361\n",
      "                    Surrogate loss: -0.0078\n",
      "             Mean action noise std: 0.23\n",
      "                 Mean total reward: -1227.45\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 58600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 203.88s\n",
      "                               ETA: 492.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 293/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 268 steps/s (collection: 0.611s, learning 0.135s)\n",
      "               Value function loss: 8158.2475\n",
      "                    Surrogate loss: 0.0159\n",
      "             Mean action noise std: 0.25\n",
      "                 Mean total reward: -1228.94\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 58800\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 204.63s\n",
      "                               ETA: 492.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 294/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.588s, learning 0.128s)\n",
      "               Value function loss: 7995.7060\n",
      "                    Surrogate loss: 0.0216\n",
      "             Mean action noise std: 0.25\n",
      "                 Mean total reward: -1230.68\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 59000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 205.34s\n",
      "                               ETA: 491.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 295/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 266 steps/s (collection: 0.621s, learning 0.130s)\n",
      "               Value function loss: 11819.5897\n",
      "                    Surrogate loss: 0.0003\n",
      "             Mean action noise std: 0.25\n",
      "                 Mean total reward: -1236.88\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 59200\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 206.09s\n",
      "                               ETA: 490.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 296/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 263 steps/s (collection: 0.627s, learning 0.133s)\n",
      "               Value function loss: 9207.6960\n",
      "                    Surrogate loss: 0.1218\n",
      "             Mean action noise std: 0.26\n",
      "                 Mean total reward: -1235.81\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 59400\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 206.85s\n",
      "                               ETA: 490.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 297/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 270 steps/s (collection: 0.604s, learning 0.135s)\n",
      "               Value function loss: 8069.9146\n",
      "                    Surrogate loss: 0.0207\n",
      "             Mean action noise std: 0.26\n",
      "                 Mean total reward: -1238.48\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 59600\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 207.59s\n",
      "                               ETA: 489.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 298/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.590s, learning 0.130s)\n",
      "               Value function loss: 10132.6356\n",
      "                    Surrogate loss: 0.0017\n",
      "             Mean action noise std: 0.26\n",
      "                 Mean total reward: -1240.92\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 59800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 208.31s\n",
      "                               ETA: 489.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 299/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.596s, learning 0.122s)\n",
      "               Value function loss: 6361.2138\n",
      "                    Surrogate loss: 0.0123\n",
      "             Mean action noise std: 0.27\n",
      "                 Mean total reward: -1241.43\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 60000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 209.03s\n",
      "                               ETA: 488.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 300/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.593s, learning 0.125s)\n",
      "               Value function loss: 9566.9771\n",
      "                    Surrogate loss: 0.0038\n",
      "             Mean action noise std: 0.26\n",
      "                 Mean total reward: -1246.70\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 60200\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 209.75s\n",
      "                               ETA: 487.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 301/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.584s, learning 0.125s)\n",
      "               Value function loss: 11195.9362\n",
      "                    Surrogate loss: 0.0202\n",
      "             Mean action noise std: 0.26\n",
      "                 Mean total reward: -1249.22\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 60400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 210.46s\n",
      "                               ETA: 487.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 302/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.569s, learning 0.127s)\n",
      "               Value function loss: 5405.0290\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 0.26\n",
      "                 Mean total reward: -1246.36\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 60600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 211.15s\n",
      "                               ETA: 486.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 303/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.565s, learning 0.136s)\n",
      "               Value function loss: 6310.8796\n",
      "                    Surrogate loss: 0.0027\n",
      "             Mean action noise std: 0.26\n",
      "                 Mean total reward: -1247.79\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 60800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 211.85s\n",
      "                               ETA: 485.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 304/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.588s, learning 0.117s)\n",
      "               Value function loss: 9380.9809\n",
      "                    Surrogate loss: 0.0071\n",
      "             Mean action noise std: 0.27\n",
      "                 Mean total reward: -1243.64\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 61000\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 212.56s\n",
      "                               ETA: 485.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 305/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.573s, learning 0.125s)\n",
      "               Value function loss: 6958.6401\n",
      "                    Surrogate loss: 0.0034\n",
      "             Mean action noise std: 0.27\n",
      "                 Mean total reward: -1244.31\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 61200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 213.26s\n",
      "                               ETA: 484.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 306/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.582s, learning 0.122s)\n",
      "               Value function loss: 10353.3908\n",
      "                    Surrogate loss: 0.0001\n",
      "             Mean action noise std: 0.29\n",
      "                 Mean total reward: -1242.32\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 61400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 213.96s\n",
      "                               ETA: 483.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 307/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.579s, learning 0.119s)\n",
      "               Value function loss: 10743.0090\n",
      "                    Surrogate loss: 0.0391\n",
      "             Mean action noise std: 0.29\n",
      "                 Mean total reward: -1245.91\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 61600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 214.66s\n",
      "                               ETA: 483.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 308/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 263 steps/s (collection: 0.624s, learning 0.135s)\n",
      "               Value function loss: 7823.6754\n",
      "                    Surrogate loss: 0.0000\n",
      "             Mean action noise std: 0.29\n",
      "                 Mean total reward: -1249.69\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 61800\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 215.42s\n",
      "                               ETA: 482.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 309/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 274 steps/s (collection: 0.596s, learning 0.133s)\n",
      "               Value function loss: 16075.4861\n",
      "                    Surrogate loss: 0.0017\n",
      "             Mean action noise std: 0.28\n",
      "                 Mean total reward: -1257.29\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 62000\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 216.15s\n",
      "                               ETA: 481.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 310/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 268 steps/s (collection: 0.615s, learning 0.129s)\n",
      "               Value function loss: 14379.7750\n",
      "                    Surrogate loss: -0.0028\n",
      "             Mean action noise std: 0.29\n",
      "                 Mean total reward: -1264.43\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 62200\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 216.89s\n",
      "                               ETA: 481.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 311/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.584s, learning 0.129s)\n",
      "               Value function loss: 3037.0355\n",
      "                    Surrogate loss: 0.0846\n",
      "             Mean action noise std: 0.30\n",
      "                 Mean total reward: -1255.91\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 62400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 217.60s\n",
      "                               ETA: 480.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 312/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.564s, learning 0.122s)\n",
      "               Value function loss: 10617.7884\n",
      "                    Surrogate loss: 0.0028\n",
      "             Mean action noise std: 0.31\n",
      "                 Mean total reward: -1260.44\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 62600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 218.29s\n",
      "                               ETA: 479.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 313/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.578s, learning 0.123s)\n",
      "               Value function loss: 15680.6876\n",
      "                    Surrogate loss: 0.0001\n",
      "             Mean action noise std: 0.31\n",
      "                 Mean total reward: -1268.78\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 62800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 218.99s\n",
      "                               ETA: 479.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 314/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.573s, learning 0.129s)\n",
      "               Value function loss: 8126.2831\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 0.32\n",
      "                 Mean total reward: -1270.49\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 63000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 219.69s\n",
      "                               ETA: 478.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 315/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.590s, learning 0.140s)\n",
      "               Value function loss: 5595.3256\n",
      "                    Surrogate loss: 0.1055\n",
      "             Mean action noise std: 0.32\n",
      "                 Mean total reward: -1269.49\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 63200\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 220.42s\n",
      "                               ETA: 477.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 316/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 240 steps/s (collection: 0.696s, learning 0.137s)\n",
      "               Value function loss: 12927.2831\n",
      "                    Surrogate loss: 0.0075\n",
      "             Mean action noise std: 0.32\n",
      "                 Mean total reward: -1270.26\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 63400\n",
      "                    Iteration time: 0.83s\n",
      "                        Total time: 221.26s\n",
      "                               ETA: 477.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 317/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 261 steps/s (collection: 0.631s, learning 0.134s)\n",
      "               Value function loss: 10987.7530\n",
      "                    Surrogate loss: 0.0032\n",
      "             Mean action noise std: 0.32\n",
      "                 Mean total reward: -1275.07\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 63600\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 222.02s\n",
      "                               ETA: 476.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 318/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.595s, learning 0.137s)\n",
      "               Value function loss: 9849.9138\n",
      "                    Surrogate loss: 0.0007\n",
      "             Mean action noise std: 0.32\n",
      "                 Mean total reward: -1278.99\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 63800\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 222.75s\n",
      "                               ETA: 476.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 319/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 267 steps/s (collection: 0.620s, learning 0.128s)\n",
      "               Value function loss: 8466.4056\n",
      "                    Surrogate loss: 0.0215\n",
      "             Mean action noise std: 0.32\n",
      "                 Mean total reward: -1282.03\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 64000\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 223.50s\n",
      "                               ETA: 475.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 320/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 272 steps/s (collection: 0.607s, learning 0.128s)\n",
      "               Value function loss: 14135.3281\n",
      "                    Surrogate loss: 0.0015\n",
      "             Mean action noise std: 0.32\n",
      "                 Mean total reward: -1287.67\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 64200\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 224.24s\n",
      "                               ETA: 475.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 321/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.576s, learning 0.130s)\n",
      "               Value function loss: 7741.5548\n",
      "                    Surrogate loss: 0.0190\n",
      "             Mean action noise std: 0.32\n",
      "                 Mean total reward: -1288.98\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 64400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 224.94s\n",
      "                               ETA: 474.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 322/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.571s, learning 0.131s)\n",
      "               Value function loss: 7010.6327\n",
      "                    Surrogate loss: 0.0035\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1289.85\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 64600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 225.64s\n",
      "                               ETA: 473.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 323/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.584s, learning 0.134s)\n",
      "               Value function loss: 13001.4209\n",
      "                    Surrogate loss: 0.0072\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1296.56\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 64800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 226.36s\n",
      "                               ETA: 473.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 324/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.574s, learning 0.127s)\n",
      "               Value function loss: 9237.8784\n",
      "                    Surrogate loss: 0.0106\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1294.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 65000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 227.06s\n",
      "                               ETA: 472.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 325/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.576s, learning 0.122s)\n",
      "               Value function loss: 6200.8879\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1293.78\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 65200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 227.76s\n",
      "                               ETA: 471.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 326/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.559s, learning 0.127s)\n",
      "               Value function loss: 7816.0979\n",
      "                    Surrogate loss: 0.0014\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1290.50\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 65400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 228.45s\n",
      "                               ETA: 470.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 327/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.586s, learning 0.139s)\n",
      "               Value function loss: 12003.1321\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1294.73\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 65600\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 229.17s\n",
      "                               ETA: 470.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 328/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 259 steps/s (collection: 0.635s, learning 0.135s)\n",
      "               Value function loss: 7228.2039\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1295.11\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 65800\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 229.94s\n",
      "                               ETA: 469.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 329/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.585s, learning 0.124s)\n",
      "               Value function loss: 12797.8287\n",
      "                    Surrogate loss: 0.0022\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1299.80\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 66000\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 230.65s\n",
      "                               ETA: 469.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 330/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.570s, learning 0.129s)\n",
      "               Value function loss: 5399.8606\n",
      "                    Surrogate loss: 0.0026\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1301.22\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 66200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 231.35s\n",
      "                               ETA: 468.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 331/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.578s, learning 0.123s)\n",
      "               Value function loss: 8927.3817\n",
      "                    Surrogate loss: 0.0006\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1304.04\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 66400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 232.05s\n",
      "                               ETA: 467.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 332/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.597s, learning 0.130s)\n",
      "               Value function loss: 9336.6337\n",
      "                    Surrogate loss: 0.0675\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1306.73\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 66600\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 232.78s\n",
      "                               ETA: 467.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 333/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.577s, learning 0.153s)\n",
      "               Value function loss: 7053.3136\n",
      "                    Surrogate loss: 0.0057\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1306.43\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 66800\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 233.51s\n",
      "                               ETA: 466.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 334/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 268 steps/s (collection: 0.621s, learning 0.125s)\n",
      "               Value function loss: 11591.5686\n",
      "                    Surrogate loss: 0.0018\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1310.52\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 67000\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 234.25s\n",
      "                               ETA: 465.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 335/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 265 steps/s (collection: 0.624s, learning 0.129s)\n",
      "               Value function loss: 9562.7740\n",
      "                    Surrogate loss: 0.0067\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1313.12\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 67200\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 235.01s\n",
      "                               ETA: 465.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 336/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 271 steps/s (collection: 0.554s, learning 0.182s)\n",
      "               Value function loss: 5369.5759\n",
      "                    Surrogate loss: 0.0134\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1312.71\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 67400\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 235.74s\n",
      "                               ETA: 464.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 337/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 258 steps/s (collection: 0.634s, learning 0.140s)\n",
      "               Value function loss: 13955.4766\n",
      "                    Surrogate loss: 0.0065\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1314.99\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 67600\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 236.52s\n",
      "                               ETA: 463.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 338/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.584s, learning 0.140s)\n",
      "               Value function loss: 6895.6817\n",
      "                    Surrogate loss: 0.0070\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1317.05\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 67800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 237.24s\n",
      "                               ETA: 463.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 339/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.592s, learning 0.124s)\n",
      "               Value function loss: 5447.7326\n",
      "                    Surrogate loss: -0.0005\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1316.51\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 68000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 237.96s\n",
      "                               ETA: 462.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 340/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 252 steps/s (collection: 0.661s, learning 0.132s)\n",
      "               Value function loss: 14479.4783\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1322.10\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 68200\n",
      "                    Iteration time: 0.79s\n",
      "                        Total time: 238.75s\n",
      "                               ETA: 462.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 341/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.584s, learning 0.124s)\n",
      "               Value function loss: 8494.2604\n",
      "                    Surrogate loss: -0.0043\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1319.84\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 68400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 239.46s\n",
      "                               ETA: 461.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 342/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.560s, learning 0.132s)\n",
      "               Value function loss: 8050.9781\n",
      "                    Surrogate loss: -0.0012\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1322.61\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 68600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 240.15s\n",
      "                               ETA: 460.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 343/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.580s, learning 0.135s)\n",
      "               Value function loss: 8512.4063\n",
      "                    Surrogate loss: -0.0008\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1325.44\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 68800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 240.87s\n",
      "                               ETA: 460.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 344/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.589s, learning 0.133s)\n",
      "               Value function loss: 13874.8629\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 0.32\n",
      "                 Mean total reward: -1332.58\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 69000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 241.59s\n",
      "                               ETA: 459.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 345/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.590s, learning 0.133s)\n",
      "               Value function loss: 6657.1423\n",
      "                    Surrogate loss: 0.0357\n",
      "             Mean action noise std: 0.32\n",
      "                 Mean total reward: -1334.25\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 69200\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 242.31s\n",
      "                               ETA: 458.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 346/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 263 steps/s (collection: 0.632s, learning 0.128s)\n",
      "               Value function loss: 9034.3137\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1338.38\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 69400\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 243.07s\n",
      "                               ETA: 458.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 347/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 266 steps/s (collection: 0.610s, learning 0.140s)\n",
      "               Value function loss: 7254.3658\n",
      "                    Surrogate loss: 0.0017\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1335.30\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 69600\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 243.82s\n",
      "                               ETA: 457.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 348/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 274 steps/s (collection: 0.601s, learning 0.127s)\n",
      "               Value function loss: 6554.3414\n",
      "                    Surrogate loss: 0.0380\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1337.72\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 69800\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 244.55s\n",
      "                               ETA: 456.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 349/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 264 steps/s (collection: 0.616s, learning 0.141s)\n",
      "               Value function loss: 8430.9445\n",
      "                    Surrogate loss: -0.0010\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1334.86\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 70000\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 245.31s\n",
      "                               ETA: 456.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 350/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.595s, learning 0.127s)\n",
      "               Value function loss: 11919.6736\n",
      "                    Surrogate loss: 0.0990\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1341.18\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 70200\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 246.03s\n",
      "                               ETA: 455.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 351/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.593s, learning 0.128s)\n",
      "               Value function loss: 6174.3569\n",
      "                    Surrogate loss: 0.0062\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1342.41\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 70400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 246.75s\n",
      "                               ETA: 454.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 352/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.564s, learning 0.131s)\n",
      "               Value function loss: 8204.1706\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1346.13\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 70600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 247.44s\n",
      "                               ETA: 454.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 353/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 259 steps/s (collection: 0.610s, learning 0.162s)\n",
      "               Value function loss: 7182.1854\n",
      "                    Surrogate loss: -0.0140\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1348.47\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 70800\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 248.22s\n",
      "                               ETA: 453.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 354/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 267 steps/s (collection: 0.612s, learning 0.136s)\n",
      "               Value function loss: 5942.9621\n",
      "                    Surrogate loss: -0.0029\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1350.07\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 71000\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 248.96s\n",
      "                               ETA: 453.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 355/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 238 steps/s (collection: 0.711s, learning 0.127s)\n",
      "               Value function loss: 5420.9998\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1350.60\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 71200\n",
      "                    Iteration time: 0.84s\n",
      "                        Total time: 249.80s\n",
      "                               ETA: 452.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 356/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 269 steps/s (collection: 0.614s, learning 0.128s)\n",
      "               Value function loss: 13033.0159\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1355.88\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 71400\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 250.54s\n",
      "                               ETA: 452.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 357/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 261 steps/s (collection: 0.625s, learning 0.140s)\n",
      "               Value function loss: 5352.7965\n",
      "                    Surrogate loss: 0.0067\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1355.71\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 71600\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 251.31s\n",
      "                               ETA: 451.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 358/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.558s, learning 0.127s)\n",
      "               Value function loss: 5172.2469\n",
      "                    Surrogate loss: -0.0030\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1355.49\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 71800\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 251.99s\n",
      "                               ETA: 450.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 359/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.558s, learning 0.127s)\n",
      "               Value function loss: 5284.1208\n",
      "                    Surrogate loss: 0.0047\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1356.01\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 72000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 252.68s\n",
      "                               ETA: 449.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 360/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.560s, learning 0.137s)\n",
      "               Value function loss: 3153.3951\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1352.88\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 72200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 253.38s\n",
      "                               ETA: 449.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 361/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.559s, learning 0.127s)\n",
      "               Value function loss: 6677.8924\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1356.18\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 72400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 254.06s\n",
      "                               ETA: 448.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 362/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.569s, learning 0.126s)\n",
      "               Value function loss: 5190.3221\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1355.70\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 72600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 254.76s\n",
      "                               ETA: 447.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 363/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 267 steps/s (collection: 0.618s, learning 0.131s)\n",
      "               Value function loss: 4821.1866\n",
      "                    Surrogate loss: -0.0012\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1353.88\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 72800\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 255.51s\n",
      "                               ETA: 447.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 364/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 247 steps/s (collection: 0.678s, learning 0.129s)\n",
      "               Value function loss: 10262.2155\n",
      "                    Surrogate loss: 0.0774\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1359.63\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 73000\n",
      "                    Iteration time: 0.81s\n",
      "                        Total time: 256.31s\n",
      "                               ETA: 446.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 365/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 272 steps/s (collection: 0.610s, learning 0.125s)\n",
      "               Value function loss: 4009.2592\n",
      "                    Surrogate loss: -0.0009\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1359.82\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 73200\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 257.05s\n",
      "                               ETA: 446.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 366/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 272 steps/s (collection: 0.607s, learning 0.126s)\n",
      "               Value function loss: 3116.0602\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1358.24\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 73400\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 257.78s\n",
      "                               ETA: 445.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 367/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.603s, learning 0.127s)\n",
      "               Value function loss: 9956.0878\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1362.90\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 73600\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 258.51s\n",
      "                               ETA: 444.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 368/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 254 steps/s (collection: 0.642s, learning 0.144s)\n",
      "               Value function loss: 9580.0787\n",
      "                    Surrogate loss: -0.0070\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1361.24\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 73800\n",
      "                    Iteration time: 0.79s\n",
      "                        Total time: 259.30s\n",
      "                               ETA: 444.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 369/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.578s, learning 0.127s)\n",
      "               Value function loss: 5450.0040\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1363.68\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 74000\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 260.00s\n",
      "                               ETA: 443.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 370/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 261 steps/s (collection: 0.633s, learning 0.131s)\n",
      "               Value function loss: 8751.1895\n",
      "                    Surrogate loss: -0.0030\n",
      "             Mean action noise std: 0.32\n",
      "                 Mean total reward: -1366.37\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 74200\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 260.77s\n",
      "                               ETA: 442.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 371/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.587s, learning 0.128s)\n",
      "               Value function loss: 8591.2131\n",
      "                    Surrogate loss: 0.0060\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1365.04\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 74400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 261.48s\n",
      "                               ETA: 442.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 372/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 272 steps/s (collection: 0.602s, learning 0.131s)\n",
      "               Value function loss: 15668.1067\n",
      "                    Surrogate loss: 0.0181\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1372.72\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 74600\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 262.21s\n",
      "                               ETA: 441.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 373/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.598s, learning 0.134s)\n",
      "               Value function loss: 4931.4131\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1373.12\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 74800\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 262.95s\n",
      "                               ETA: 440.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 374/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.594s, learning 0.132s)\n",
      "               Value function loss: 5885.7149\n",
      "                    Surrogate loss: -0.0028\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1375.00\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 75000\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 263.67s\n",
      "                               ETA: 440.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 375/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.580s, learning 0.130s)\n",
      "               Value function loss: 3658.5561\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1367.70\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 75200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 264.38s\n",
      "                               ETA: 439.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 376/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 267 steps/s (collection: 0.609s, learning 0.140s)\n",
      "               Value function loss: 3538.8184\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1368.09\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 75400\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 265.13s\n",
      "                               ETA: 438.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 377/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.601s, learning 0.124s)\n",
      "               Value function loss: 5354.5849\n",
      "                    Surrogate loss: 0.1180\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1367.96\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 75600\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 265.86s\n",
      "                               ETA: 438.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 378/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.581s, learning 0.140s)\n",
      "               Value function loss: 6987.4028\n",
      "                    Surrogate loss: 0.0026\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1370.41\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 75800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 266.58s\n",
      "                               ETA: 437.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 379/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 257 steps/s (collection: 0.647s, learning 0.130s)\n",
      "               Value function loss: 5970.1454\n",
      "                    Surrogate loss: 0.0238\n",
      "             Mean action noise std: 0.32\n",
      "                 Mean total reward: -1365.83\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 76000\n",
      "                    Iteration time: 0.78s\n",
      "                        Total time: 267.35s\n",
      "                               ETA: 436.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 380/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 262 steps/s (collection: 0.624s, learning 0.139s)\n",
      "               Value function loss: 7658.9604\n",
      "                    Surrogate loss: 0.0151\n",
      "             Mean action noise std: 0.32\n",
      "                 Mean total reward: -1365.85\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 76200\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 268.12s\n",
      "                               ETA: 436.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 381/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 261 steps/s (collection: 0.634s, learning 0.132s)\n",
      "               Value function loss: 5973.0695\n",
      "                    Surrogate loss: 0.0439\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1366.12\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 76400\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 268.88s\n",
      "                               ETA: 435.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 382/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 268 steps/s (collection: 0.621s, learning 0.125s)\n",
      "               Value function loss: 6241.0647\n",
      "                    Surrogate loss: 0.0004\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1366.47\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 76600\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 269.63s\n",
      "                               ETA: 435.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 383/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.590s, learning 0.132s)\n",
      "               Value function loss: 6747.5762\n",
      "                    Surrogate loss: 0.0785\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1367.06\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 76800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 270.35s\n",
      "                               ETA: 434.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 384/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.564s, learning 0.133s)\n",
      "               Value function loss: 8409.1343\n",
      "                    Surrogate loss: -0.0005\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1367.82\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 77000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 271.05s\n",
      "                               ETA: 433.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 385/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 274 steps/s (collection: 0.600s, learning 0.129s)\n",
      "               Value function loss: 7624.0347\n",
      "                    Surrogate loss: 0.1054\n",
      "             Mean action noise std: 0.33\n",
      "                 Mean total reward: -1369.88\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 77200\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 271.78s\n",
      "                               ETA: 433.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 386/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 254 steps/s (collection: 0.634s, learning 0.152s)\n",
      "               Value function loss: 7835.6714\n",
      "                    Surrogate loss: 0.0013\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1370.54\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 77400\n",
      "                    Iteration time: 0.79s\n",
      "                        Total time: 272.56s\n",
      "                               ETA: 432.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 387/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 265 steps/s (collection: 0.623s, learning 0.131s)\n",
      "               Value function loss: 7361.8504\n",
      "                    Surrogate loss: 0.0198\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1369.19\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 77600\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 273.32s\n",
      "                               ETA: 431.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 388/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 252 steps/s (collection: 0.647s, learning 0.144s)\n",
      "               Value function loss: 7346.6395\n",
      "                    Surrogate loss: -0.0016\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1368.04\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 77800\n",
      "                    Iteration time: 0.79s\n",
      "                        Total time: 274.11s\n",
      "                               ETA: 431.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 389/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.586s, learning 0.128s)\n",
      "               Value function loss: 8036.5864\n",
      "                    Surrogate loss: 0.0049\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1366.92\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 78000\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 274.82s\n",
      "                               ETA: 430.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 390/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.549s, learning 0.130s)\n",
      "               Value function loss: 8394.5601\n",
      "                    Surrogate loss: 0.0024\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1367.98\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 78200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 275.50s\n",
      "                               ETA: 429.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 391/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 262 steps/s (collection: 0.616s, learning 0.147s)\n",
      "               Value function loss: 3869.6544\n",
      "                    Surrogate loss: -0.0079\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1363.44\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 78400\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 276.26s\n",
      "                               ETA: 429.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 392/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.562s, learning 0.121s)\n",
      "               Value function loss: 6955.0474\n",
      "                    Surrogate loss: 0.0003\n",
      "             Mean action noise std: 0.34\n",
      "                 Mean total reward: -1365.20\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 78600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 276.95s\n",
      "                               ETA: 428.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 393/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.574s, learning 0.122s)\n",
      "               Value function loss: 6938.5952\n",
      "                    Surrogate loss: 0.0046\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1364.54\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 78800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 277.64s\n",
      "                               ETA: 427.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 394/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.557s, learning 0.127s)\n",
      "               Value function loss: 6597.3914\n",
      "                    Surrogate loss: -0.0023\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1363.90\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 79000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 278.33s\n",
      "                               ETA: 427.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 395/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 251 steps/s (collection: 0.672s, learning 0.124s)\n",
      "               Value function loss: 5559.0455\n",
      "                    Surrogate loss: 0.0030\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1360.03\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 79200\n",
      "                    Iteration time: 0.80s\n",
      "                        Total time: 279.12s\n",
      "                               ETA: 426.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 396/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 267 steps/s (collection: 0.623s, learning 0.125s)\n",
      "               Value function loss: 6129.0471\n",
      "                    Surrogate loss: 0.0196\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1357.57\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 79400\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 279.87s\n",
      "                               ETA: 425.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 397/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 272 steps/s (collection: 0.602s, learning 0.133s)\n",
      "               Value function loss: 8146.4836\n",
      "                    Surrogate loss: 0.0630\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1357.73\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 79600\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 280.61s\n",
      "                               ETA: 425.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 398/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 271 steps/s (collection: 0.614s, learning 0.124s)\n",
      "               Value function loss: 6678.8530\n",
      "                    Surrogate loss: 0.0011\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1355.81\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 79800\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 281.34s\n",
      "                               ETA: 424.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 399/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.605s, learning 0.125s)\n",
      "               Value function loss: 8279.5037\n",
      "                    Surrogate loss: 0.0059\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1357.42\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 80000\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 282.07s\n",
      "                               ETA: 423.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 400/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 264 steps/s (collection: 0.628s, learning 0.127s)\n",
      "               Value function loss: 7259.8751\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 0.35\n",
      "                 Mean total reward: -1356.15\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 80200\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 282.83s\n",
      "                               ETA: 423.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 401/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.596s, learning 0.128s)\n",
      "               Value function loss: 8827.5388\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1354.40\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 80400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 283.55s\n",
      "                               ETA: 422.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 402/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 264 steps/s (collection: 0.637s, learning 0.118s)\n",
      "               Value function loss: 7748.6304\n",
      "                    Surrogate loss: 0.0955\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1357.25\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 80600\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 284.31s\n",
      "                               ETA: 421.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 403/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 265 steps/s (collection: 0.622s, learning 0.131s)\n",
      "               Value function loss: 8553.3562\n",
      "                    Surrogate loss: 0.0178\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1359.06\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 80800\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 285.06s\n",
      "                               ETA: 421.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 404/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.593s, learning 0.131s)\n",
      "               Value function loss: 7429.1195\n",
      "                    Surrogate loss: 0.0008\n",
      "             Mean action noise std: 0.37\n",
      "                 Mean total reward: -1357.86\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 81000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 285.78s\n",
      "                               ETA: 420.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 405/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.585s, learning 0.121s)\n",
      "               Value function loss: 8862.6757\n",
      "                    Surrogate loss: 0.0048\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1359.07\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 81200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 286.49s\n",
      "                               ETA: 419.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 406/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.574s, learning 0.139s)\n",
      "               Value function loss: 7667.5985\n",
      "                    Surrogate loss: -0.0023\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1357.85\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 81400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 287.20s\n",
      "                               ETA: 419.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 407/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.584s, learning 0.130s)\n",
      "               Value function loss: 8742.8060\n",
      "                    Surrogate loss: 0.0058\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1356.88\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 81600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 287.92s\n",
      "                               ETA: 418.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 408/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.576s, learning 0.118s)\n",
      "               Value function loss: 6446.5858\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1355.85\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 81800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 288.61s\n",
      "                               ETA: 417.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 409/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.584s, learning 0.121s)\n",
      "               Value function loss: 9075.3568\n",
      "                    Surrogate loss: 0.0308\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1351.66\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 82000\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 289.32s\n",
      "                               ETA: 417.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 410/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.562s, learning 0.123s)\n",
      "               Value function loss: 10304.0355\n",
      "                    Surrogate loss: -0.0016\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1349.35\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 82200\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 290.00s\n",
      "                               ETA: 416.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 411/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.600s, learning 0.119s)\n",
      "               Value function loss: 10384.1755\n",
      "                    Surrogate loss: 0.0188\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1356.40\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 82400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 290.72s\n",
      "                               ETA: 415.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 412/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 267 steps/s (collection: 0.622s, learning 0.125s)\n",
      "               Value function loss: 5924.4320\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1353.37\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 82600\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 291.47s\n",
      "                               ETA: 415.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 413/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 268 steps/s (collection: 0.616s, learning 0.130s)\n",
      "               Value function loss: 9759.0587\n",
      "                    Surrogate loss: 0.0119\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1350.13\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 82800\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 292.21s\n",
      "                               ETA: 414.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 414/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.594s, learning 0.133s)\n",
      "               Value function loss: 7617.6466\n",
      "                    Surrogate loss: 0.0041\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1349.85\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 83000\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 292.94s\n",
      "                               ETA: 413.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 415/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 262 steps/s (collection: 0.632s, learning 0.130s)\n",
      "               Value function loss: 9807.1585\n",
      "                    Surrogate loss: -0.0011\n",
      "             Mean action noise std: 0.36\n",
      "                 Mean total reward: -1353.99\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 83200\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 293.70s\n",
      "                               ETA: 413.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 416/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 255 steps/s (collection: 0.648s, learning 0.135s)\n",
      "               Value function loss: 9032.2304\n",
      "                    Surrogate loss: 0.2365\n",
      "             Mean action noise std: 0.37\n",
      "                 Mean total reward: -1351.78\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 83400\n",
      "                    Iteration time: 0.78s\n",
      "                        Total time: 294.49s\n",
      "                               ETA: 412.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 417/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 264 steps/s (collection: 0.616s, learning 0.141s)\n",
      "               Value function loss: 11968.7684\n",
      "                    Surrogate loss: -0.0069\n",
      "             Mean action noise std: 0.37\n",
      "                 Mean total reward: -1352.20\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 83600\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 295.24s\n",
      "                               ETA: 411.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 418/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.588s, learning 0.126s)\n",
      "               Value function loss: 8482.3870\n",
      "                    Surrogate loss: 0.0001\n",
      "             Mean action noise std: 0.38\n",
      "                 Mean total reward: -1351.00\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 83800\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 295.96s\n",
      "                               ETA: 411.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 419/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.590s, learning 0.124s)\n",
      "               Value function loss: 10250.5378\n",
      "                    Surrogate loss: 0.0200\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1352.38\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 84000\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 296.67s\n",
      "                               ETA: 410.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 420/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 259 steps/s (collection: 0.638s, learning 0.132s)\n",
      "               Value function loss: 11989.0818\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1351.32\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 84200\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 297.44s\n",
      "                               ETA: 409.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 421/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.586s, learning 0.129s)\n",
      "               Value function loss: 11631.5843\n",
      "                    Surrogate loss: -0.0073\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1354.16\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 84400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 298.16s\n",
      "                               ETA: 409.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 422/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 271 steps/s (collection: 0.602s, learning 0.134s)\n",
      "               Value function loss: 12003.8515\n",
      "                    Surrogate loss: 0.0047\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1358.45\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 84600\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 298.89s\n",
      "                               ETA: 408.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 423/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.590s, learning 0.132s)\n",
      "               Value function loss: 8682.6020\n",
      "                    Surrogate loss: 0.0364\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1355.44\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 84800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 299.61s\n",
      "                               ETA: 407.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 424/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.583s, learning 0.133s)\n",
      "               Value function loss: 11805.2628\n",
      "                    Surrogate loss: 0.0002\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1356.84\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 85000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 300.33s\n",
      "                               ETA: 407.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 425/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.567s, learning 0.118s)\n",
      "               Value function loss: 10013.0379\n",
      "                    Surrogate loss: 0.0080\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1359.62\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 85200\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 301.01s\n",
      "                               ETA: 406.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 426/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.585s, learning 0.122s)\n",
      "               Value function loss: 11966.4688\n",
      "                    Surrogate loss: 0.0048\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1362.69\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 85400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 301.72s\n",
      "                               ETA: 405.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 427/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.579s, learning 0.120s)\n",
      "               Value function loss: 5451.5734\n",
      "                    Surrogate loss: 0.0002\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1357.17\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 85600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 302.42s\n",
      "                               ETA: 404.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 428/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.590s, learning 0.137s)\n",
      "               Value function loss: 7437.7819\n",
      "                    Surrogate loss: 0.0003\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1358.06\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 85800\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 303.15s\n",
      "                               ETA: 404.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 429/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 269 steps/s (collection: 0.609s, learning 0.134s)\n",
      "               Value function loss: 8994.3389\n",
      "                    Surrogate loss: 0.0003\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1355.49\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 86000\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 303.89s\n",
      "                               ETA: 403.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 430/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 260 steps/s (collection: 0.639s, learning 0.130s)\n",
      "               Value function loss: 6774.1589\n",
      "                    Surrogate loss: 0.0215\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1357.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 86200\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 304.66s\n",
      "                               ETA: 402.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 431/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 274 steps/s (collection: 0.585s, learning 0.144s)\n",
      "               Value function loss: 7622.2157\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1356.79\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 86400\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 305.39s\n",
      "                               ETA: 402.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 432/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 274 steps/s (collection: 0.602s, learning 0.126s)\n",
      "               Value function loss: 11200.2833\n",
      "                    Surrogate loss: 0.0094\n",
      "             Mean action noise std: 0.39\n",
      "                 Mean total reward: -1358.02\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 86600\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 306.12s\n",
      "                               ETA: 401.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 433/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 264 steps/s (collection: 0.625s, learning 0.132s)\n",
      "               Value function loss: 9852.2310\n",
      "                    Surrogate loss: 0.0172\n",
      "             Mean action noise std: 0.40\n",
      "                 Mean total reward: -1360.51\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 86800\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 306.87s\n",
      "                               ETA: 400.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 434/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.594s, learning 0.129s)\n",
      "               Value function loss: 11434.1231\n",
      "                    Surrogate loss: 0.0231\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1360.81\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 87000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 307.60s\n",
      "                               ETA: 400.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 435/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 268 steps/s (collection: 0.621s, learning 0.124s)\n",
      "               Value function loss: 4443.8662\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1356.63\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 87200\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 308.34s\n",
      "                               ETA: 399.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 436/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.578s, learning 0.129s)\n",
      "               Value function loss: 4739.2686\n",
      "                    Surrogate loss: -0.0006\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1356.67\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 87400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 309.05s\n",
      "                               ETA: 398.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 437/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 256 steps/s (collection: 0.658s, learning 0.122s)\n",
      "               Value function loss: 11728.0194\n",
      "                    Surrogate loss: 0.0088\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1355.70\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 87600\n",
      "                    Iteration time: 0.78s\n",
      "                        Total time: 309.83s\n",
      "                               ETA: 398.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 438/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.567s, learning 0.130s)\n",
      "               Value function loss: 10716.5608\n",
      "                    Surrogate loss: 0.0454\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1359.00\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 87800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 310.53s\n",
      "                               ETA: 397.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 439/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 295 steps/s (collection: 0.550s, learning 0.126s)\n",
      "               Value function loss: 11037.3651\n",
      "                    Surrogate loss: -0.0027\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1363.25\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 88000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 311.20s\n",
      "                               ETA: 396.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 440/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.583s, learning 0.123s)\n",
      "               Value function loss: 4891.6567\n",
      "                    Surrogate loss: 0.0241\n",
      "             Mean action noise std: 0.40\n",
      "                 Mean total reward: -1356.16\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 88200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 311.91s\n",
      "                               ETA: 396.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 441/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.588s, learning 0.127s)\n",
      "               Value function loss: 8176.2611\n",
      "                    Surrogate loss: 0.0009\n",
      "             Mean action noise std: 0.40\n",
      "                 Mean total reward: -1356.49\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 88400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 312.62s\n",
      "                               ETA: 395.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 442/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.593s, learning 0.124s)\n",
      "               Value function loss: 11630.6188\n",
      "                    Surrogate loss: 0.0044\n",
      "             Mean action noise std: 0.40\n",
      "                 Mean total reward: -1359.35\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 88600\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 313.34s\n",
      "                               ETA: 394.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 443/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 266 steps/s (collection: 0.617s, learning 0.133s)\n",
      "               Value function loss: 10893.4407\n",
      "                    Surrogate loss: -0.0008\n",
      "             Mean action noise std: 0.40\n",
      "                 Mean total reward: -1360.85\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 88800\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 314.09s\n",
      "                               ETA: 394.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 444/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.576s, learning 0.135s)\n",
      "               Value function loss: 10744.8907\n",
      "                    Surrogate loss: 0.0021\n",
      "             Mean action noise std: 0.40\n",
      "                 Mean total reward: -1359.40\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 89000\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 314.80s\n",
      "                               ETA: 393.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 445/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 260 steps/s (collection: 0.622s, learning 0.146s)\n",
      "               Value function loss: 11213.1774\n",
      "                    Surrogate loss: 0.0152\n",
      "             Mean action noise std: 0.40\n",
      "                 Mean total reward: -1363.33\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 89200\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 315.57s\n",
      "                               ETA: 392.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 446/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 258 steps/s (collection: 0.645s, learning 0.129s)\n",
      "               Value function loss: 10916.9745\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 0.40\n",
      "                 Mean total reward: -1364.42\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 89400\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 316.34s\n",
      "                               ETA: 392.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 447/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.602s, learning 0.124s)\n",
      "               Value function loss: 11741.9659\n",
      "                    Surrogate loss: 0.0153\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1368.03\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 89600\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 317.07s\n",
      "                               ETA: 391.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 448/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 270 steps/s (collection: 0.605s, learning 0.134s)\n",
      "               Value function loss: 10193.9690\n",
      "                    Surrogate loss: 0.0214\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1370.84\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 89800\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 317.81s\n",
      "                               ETA: 390.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 449/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 272 steps/s (collection: 0.593s, learning 0.141s)\n",
      "               Value function loss: 10904.3094\n",
      "                    Surrogate loss: -0.0058\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1372.91\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 90000\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 318.54s\n",
      "                               ETA: 390.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 450/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 238 steps/s (collection: 0.691s, learning 0.149s)\n",
      "               Value function loss: 10761.7935\n",
      "                    Surrogate loss: 0.0057\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1372.72\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 90200\n",
      "                    Iteration time: 0.84s\n",
      "                        Total time: 319.38s\n",
      "                               ETA: 389.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 451/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 252 steps/s (collection: 0.664s, learning 0.129s)\n",
      "               Value function loss: 11674.9069\n",
      "                    Surrogate loss: 0.0121\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1376.64\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 90400\n",
      "                    Iteration time: 0.79s\n",
      "                        Total time: 320.17s\n",
      "                               ETA: 388.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 452/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 254 steps/s (collection: 0.651s, learning 0.134s)\n",
      "               Value function loss: 10527.8842\n",
      "                    Surrogate loss: 0.0046\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1378.54\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 90600\n",
      "                    Iteration time: 0.78s\n",
      "                        Total time: 320.96s\n",
      "                               ETA: 388.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 453/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 260 steps/s (collection: 0.639s, learning 0.128s)\n",
      "               Value function loss: 11714.9122\n",
      "                    Surrogate loss: 0.0236\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1381.99\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 90800\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 321.73s\n",
      "                               ETA: 387.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 454/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.577s, learning 0.123s)\n",
      "               Value function loss: 10627.1551\n",
      "                    Surrogate loss: 0.0031\n",
      "             Mean action noise std: 0.42\n",
      "                 Mean total reward: -1386.13\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 91000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 322.43s\n",
      "                               ETA: 386.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 455/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.599s, learning 0.131s)\n",
      "               Value function loss: 11134.3242\n",
      "                    Surrogate loss: 0.0609\n",
      "             Mean action noise std: 0.42\n",
      "                 Mean total reward: -1390.82\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 91200\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 323.16s\n",
      "                               ETA: 386.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 456/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.584s, learning 0.130s)\n",
      "               Value function loss: 10367.8964\n",
      "                    Surrogate loss: 0.0078\n",
      "             Mean action noise std: 0.42\n",
      "                 Mean total reward: -1389.60\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 91400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 323.87s\n",
      "                               ETA: 385.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 457/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 270 steps/s (collection: 0.598s, learning 0.141s)\n",
      "               Value function loss: 4718.3910\n",
      "                    Surrogate loss: 0.0437\n",
      "             Mean action noise std: 0.42\n",
      "                 Mean total reward: -1388.67\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 91600\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 324.61s\n",
      "                               ETA: 384.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 458/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 240 steps/s (collection: 0.683s, learning 0.149s)\n",
      "               Value function loss: 2926.5107\n",
      "                    Surrogate loss: 0.0193\n",
      "             Mean action noise std: 0.42\n",
      "                 Mean total reward: -1386.61\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 91800\n",
      "                    Iteration time: 0.83s\n",
      "                        Total time: 325.44s\n",
      "                               ETA: 384.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 459/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 258 steps/s (collection: 0.629s, learning 0.144s)\n",
      "               Value function loss: 11697.1885\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 0.42\n",
      "                 Mean total reward: -1391.83\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 92000\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 326.21s\n",
      "                               ETA: 383.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 460/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 260 steps/s (collection: 0.639s, learning 0.130s)\n",
      "               Value function loss: 11621.3883\n",
      "                    Surrogate loss: 0.0096\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1399.97\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 92200\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 326.98s\n",
      "                               ETA: 383.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 461/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.594s, learning 0.133s)\n",
      "               Value function loss: 10459.3390\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1403.34\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 92400\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 327.71s\n",
      "                               ETA: 382.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 462/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 258 steps/s (collection: 0.633s, learning 0.141s)\n",
      "               Value function loss: 11479.6299\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1408.39\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 92600\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 328.48s\n",
      "                               ETA: 381.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 463/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 249 steps/s (collection: 0.645s, learning 0.156s)\n",
      "               Value function loss: 11626.0772\n",
      "                    Surrogate loss: 0.0139\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1414.33\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 92800\n",
      "                    Iteration time: 0.80s\n",
      "                        Total time: 329.29s\n",
      "                               ETA: 381.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 464/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 242 steps/s (collection: 0.688s, learning 0.137s)\n",
      "               Value function loss: 7658.4461\n",
      "                    Surrogate loss: 0.0057\n",
      "             Mean action noise std: 0.42\n",
      "                 Mean total reward: -1412.47\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 93000\n",
      "                    Iteration time: 0.82s\n",
      "                        Total time: 330.11s\n",
      "                               ETA: 380.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 465/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 267 steps/s (collection: 0.613s, learning 0.133s)\n",
      "               Value function loss: 9702.7448\n",
      "                    Surrogate loss: 0.0416\n",
      "             Mean action noise std: 0.42\n",
      "                 Mean total reward: -1417.33\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 93200\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 330.86s\n",
      "                               ETA: 379.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 466/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.565s, learning 0.138s)\n",
      "               Value function loss: 10744.0876\n",
      "                    Surrogate loss: 0.0153\n",
      "             Mean action noise std: 0.42\n",
      "                 Mean total reward: -1424.67\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 93400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 331.56s\n",
      "                               ETA: 379.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 467/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.599s, learning 0.128s)\n",
      "               Value function loss: 10454.7907\n",
      "                    Surrogate loss: -0.0032\n",
      "             Mean action noise std: 0.42\n",
      "                 Mean total reward: -1425.01\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 93600\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 332.29s\n",
      "                               ETA: 378.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 468/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.607s, learning 0.125s)\n",
      "               Value function loss: 11585.2324\n",
      "                    Surrogate loss: 0.0196\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1426.73\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 93800\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 333.02s\n",
      "                               ETA: 377.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 469/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 257 steps/s (collection: 0.641s, learning 0.137s)\n",
      "               Value function loss: 10648.0269\n",
      "                    Surrogate loss: 0.0040\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1430.70\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 94000\n",
      "                    Iteration time: 0.78s\n",
      "                        Total time: 333.79s\n",
      "                               ETA: 377.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 470/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.580s, learning 0.130s)\n",
      "               Value function loss: 10969.0160\n",
      "                    Surrogate loss: 0.0143\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1432.45\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 94200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 334.51s\n",
      "                               ETA: 376.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 471/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 267 steps/s (collection: 0.615s, learning 0.131s)\n",
      "               Value function loss: 8073.5606\n",
      "                    Surrogate loss: 0.0109\n",
      "             Mean action noise std: 0.41\n",
      "                 Mean total reward: -1432.81\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 94400\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 335.25s\n",
      "                               ETA: 375.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 472/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 257 steps/s (collection: 0.638s, learning 0.138s)\n",
      "               Value function loss: 11015.6947\n",
      "                    Surrogate loss: 0.0198\n",
      "             Mean action noise std: 0.42\n",
      "                 Mean total reward: -1430.60\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 94600\n",
      "                    Iteration time: 0.78s\n",
      "                        Total time: 336.03s\n",
      "                               ETA: 375.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 473/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 257 steps/s (collection: 0.650s, learning 0.126s)\n",
      "               Value function loss: 10404.4478\n",
      "                    Surrogate loss: 0.0183\n",
      "             Mean action noise std: 0.43\n",
      "                 Mean total reward: -1436.12\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 94800\n",
      "                    Iteration time: 0.78s\n",
      "                        Total time: 336.80s\n",
      "                               ETA: 374.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 474/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 243 steps/s (collection: 0.678s, learning 0.142s)\n",
      "               Value function loss: 10418.9605\n",
      "                    Surrogate loss: 0.0011\n",
      "             Mean action noise std: 0.43\n",
      "                 Mean total reward: -1439.77\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 95000\n",
      "                    Iteration time: 0.82s\n",
      "                        Total time: 337.62s\n",
      "                               ETA: 373.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 475/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 234 steps/s (collection: 0.687s, learning 0.164s)\n",
      "               Value function loss: 6188.6421\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 0.43\n",
      "                 Mean total reward: -1443.22\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 95200\n",
      "                    Iteration time: 0.85s\n",
      "                        Total time: 338.48s\n",
      "                               ETA: 373.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 476/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 250 steps/s (collection: 0.642s, learning 0.158s)\n",
      "               Value function loss: 11223.2111\n",
      "                    Surrogate loss: 0.0018\n",
      "             Mean action noise std: 0.44\n",
      "                 Mean total reward: -1450.91\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 95400\n",
      "                    Iteration time: 0.80s\n",
      "                        Total time: 339.28s\n",
      "                               ETA: 372.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 477/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 236 steps/s (collection: 0.709s, learning 0.137s)\n",
      "               Value function loss: 11448.4029\n",
      "                    Surrogate loss: 0.0046\n",
      "             Mean action noise std: 0.45\n",
      "                 Mean total reward: -1455.28\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 95600\n",
      "                    Iteration time: 0.85s\n",
      "                        Total time: 340.12s\n",
      "                               ETA: 372.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 478/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 245 steps/s (collection: 0.688s, learning 0.127s)\n",
      "               Value function loss: 10895.3820\n",
      "                    Surrogate loss: 0.0106\n",
      "             Mean action noise std: 0.45\n",
      "                 Mean total reward: -1458.34\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 95800\n",
      "                    Iteration time: 0.82s\n",
      "                        Total time: 340.94s\n",
      "                               ETA: 371.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 479/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 247 steps/s (collection: 0.658s, learning 0.150s)\n",
      "               Value function loss: 11501.6414\n",
      "                    Surrogate loss: -0.0009\n",
      "             Mean action noise std: 0.45\n",
      "                 Mean total reward: -1462.89\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 96000\n",
      "                    Iteration time: 0.81s\n",
      "                        Total time: 341.74s\n",
      "                               ETA: 370.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 480/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 247 steps/s (collection: 0.654s, learning 0.155s)\n",
      "               Value function loss: 4631.1104\n",
      "                    Surrogate loss: 0.0034\n",
      "             Mean action noise std: 0.45\n",
      "                 Mean total reward: -1460.39\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 96200\n",
      "                    Iteration time: 0.81s\n",
      "                        Total time: 342.55s\n",
      "                               ETA: 370.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 481/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 242 steps/s (collection: 0.674s, learning 0.150s)\n",
      "               Value function loss: 7328.8742\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 0.45\n",
      "                 Mean total reward: -1461.36\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 96400\n",
      "                    Iteration time: 0.82s\n",
      "                        Total time: 343.38s\n",
      "                               ETA: 369.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 482/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 239 steps/s (collection: 0.703s, learning 0.133s)\n",
      "               Value function loss: 11280.3998\n",
      "                    Surrogate loss: 0.0032\n",
      "             Mean action noise std: 0.45\n",
      "                 Mean total reward: -1465.26\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 96600\n",
      "                    Iteration time: 0.84s\n",
      "                        Total time: 344.21s\n",
      "                               ETA: 369.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 483/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 235 steps/s (collection: 0.718s, learning 0.133s)\n",
      "               Value function loss: 10181.1500\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 0.45\n",
      "                 Mean total reward: -1467.80\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 96800\n",
      "                    Iteration time: 0.85s\n",
      "                        Total time: 345.06s\n",
      "                               ETA: 368.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 484/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 253 steps/s (collection: 0.652s, learning 0.136s)\n",
      "               Value function loss: 10657.2147\n",
      "                    Surrogate loss: 0.0218\n",
      "             Mean action noise std: 0.47\n",
      "                 Mean total reward: -1469.58\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 97000\n",
      "                    Iteration time: 0.79s\n",
      "                        Total time: 345.85s\n",
      "                               ETA: 368.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 485/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 249 steps/s (collection: 0.666s, learning 0.137s)\n",
      "               Value function loss: 9847.8966\n",
      "                    Surrogate loss: 0.0013\n",
      "             Mean action noise std: 0.48\n",
      "                 Mean total reward: -1470.68\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 97200\n",
      "                    Iteration time: 0.80s\n",
      "                        Total time: 346.66s\n",
      "                               ETA: 367.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 486/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 239 steps/s (collection: 0.684s, learning 0.151s)\n",
      "               Value function loss: 10998.8310\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 0.49\n",
      "                 Mean total reward: -1473.51\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 97400\n",
      "                    Iteration time: 0.84s\n",
      "                        Total time: 347.49s\n",
      "                               ETA: 366.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 487/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 244 steps/s (collection: 0.681s, learning 0.137s)\n",
      "               Value function loss: 10076.1646\n",
      "                    Surrogate loss: -0.0141\n",
      "             Mean action noise std: 0.49\n",
      "                 Mean total reward: -1475.62\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 97600\n",
      "                    Iteration time: 0.82s\n",
      "                        Total time: 348.31s\n",
      "                               ETA: 366.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 488/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 252 steps/s (collection: 0.665s, learning 0.126s)\n",
      "               Value function loss: 10206.2442\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 0.49\n",
      "                 Mean total reward: -1478.37\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 97800\n",
      "                    Iteration time: 0.79s\n",
      "                        Total time: 349.10s\n",
      "                               ETA: 365.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 489/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 244 steps/s (collection: 0.664s, learning 0.155s)\n",
      "               Value function loss: 11415.5703\n",
      "                    Surrogate loss: 0.0750\n",
      "             Mean action noise std: 0.49\n",
      "                 Mean total reward: -1481.08\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 98000\n",
      "                    Iteration time: 0.82s\n",
      "                        Total time: 349.92s\n",
      "                               ETA: 364.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 490/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 243 steps/s (collection: 0.683s, learning 0.138s)\n",
      "               Value function loss: 8332.6020\n",
      "                    Surrogate loss: 0.0012\n",
      "             Mean action noise std: 0.49\n",
      "                 Mean total reward: -1481.26\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 98200\n",
      "                    Iteration time: 0.82s\n",
      "                        Total time: 350.74s\n",
      "                               ETA: 364.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 491/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 253 steps/s (collection: 0.659s, learning 0.131s)\n",
      "               Value function loss: 11427.3992\n",
      "                    Surrogate loss: 0.0101\n",
      "             Mean action noise std: 0.49\n",
      "                 Mean total reward: -1487.99\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 98400\n",
      "                    Iteration time: 0.79s\n",
      "                        Total time: 351.53s\n",
      "                               ETA: 363.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 492/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.581s, learning 0.123s)\n",
      "               Value function loss: 9403.9776\n",
      "                    Surrogate loss: -0.0012\n",
      "             Mean action noise std: 0.49\n",
      "                 Mean total reward: -1489.66\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 98600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 352.23s\n",
      "                               ETA: 363.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 493/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 269 steps/s (collection: 0.617s, learning 0.126s)\n",
      "               Value function loss: 11406.7602\n",
      "                    Surrogate loss: 0.0006\n",
      "             Mean action noise std: 0.48\n",
      "                 Mean total reward: -1493.29\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 98800\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 352.98s\n",
      "                               ETA: 362.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 494/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.594s, learning 0.125s)\n",
      "               Value function loss: 11386.0130\n",
      "                    Surrogate loss: 0.0767\n",
      "             Mean action noise std: 0.48\n",
      "                 Mean total reward: -1496.91\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 99000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 353.69s\n",
      "                               ETA: 361.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 495/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.576s, learning 0.147s)\n",
      "               Value function loss: 11051.4249\n",
      "                    Surrogate loss: 0.0167\n",
      "             Mean action noise std: 0.48\n",
      "                 Mean total reward: -1501.02\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 99200\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 354.42s\n",
      "                               ETA: 360.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 496/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.595s, learning 0.124s)\n",
      "               Value function loss: 10624.1962\n",
      "                    Surrogate loss: 0.0031\n",
      "             Mean action noise std: 0.49\n",
      "                 Mean total reward: -1504.63\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 99400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 355.14s\n",
      "                               ETA: 360.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 497/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.568s, learning 0.126s)\n",
      "               Value function loss: 10123.0153\n",
      "                    Surrogate loss: 0.0111\n",
      "             Mean action noise std: 0.50\n",
      "                 Mean total reward: -1506.56\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 99600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 355.83s\n",
      "                               ETA: 359.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 498/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 270 steps/s (collection: 0.614s, learning 0.125s)\n",
      "               Value function loss: 11305.1716\n",
      "                    Surrogate loss: 0.0158\n",
      "             Mean action noise std: 0.50\n",
      "                 Mean total reward: -1509.46\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 99800\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 356.57s\n",
      "                               ETA: 358.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 499/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.569s, learning 0.128s)\n",
      "               Value function loss: 10612.5167\n",
      "                    Surrogate loss: -0.0003\n",
      "             Mean action noise std: 0.50\n",
      "                 Mean total reward: -1511.46\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 100000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 357.27s\n",
      "                               ETA: 358.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 500/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.584s, learning 0.133s)\n",
      "               Value function loss: 9695.7814\n",
      "                    Surrogate loss: 0.0005\n",
      "             Mean action noise std: 0.50\n",
      "                 Mean total reward: -1513.29\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 100200\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 357.98s\n",
      "                               ETA: 357.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 501/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.586s, learning 0.122s)\n",
      "               Value function loss: 9385.6828\n",
      "                    Surrogate loss: 0.0180\n",
      "             Mean action noise std: 0.50\n",
      "                 Mean total reward: -1514.53\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 100400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 358.69s\n",
      "                               ETA: 356.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 502/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.580s, learning 0.126s)\n",
      "               Value function loss: 11004.5824\n",
      "                    Surrogate loss: -0.0012\n",
      "             Mean action noise std: 0.50\n",
      "                 Mean total reward: -1516.46\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 100600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 359.40s\n",
      "                               ETA: 355.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 503/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.588s, learning 0.133s)\n",
      "               Value function loss: 8691.1984\n",
      "                    Surrogate loss: 0.0669\n",
      "             Mean action noise std: 0.51\n",
      "                 Mean total reward: -1517.12\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 100800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 360.12s\n",
      "                               ETA: 355.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 504/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 266 steps/s (collection: 0.618s, learning 0.131s)\n",
      "               Value function loss: 10660.8495\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 0.51\n",
      "                 Mean total reward: -1520.13\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 101000\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 360.87s\n",
      "                               ETA: 354.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 505/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.554s, learning 0.124s)\n",
      "               Value function loss: 8432.7485\n",
      "                    Surrogate loss: 0.0044\n",
      "             Mean action noise std: 0.51\n",
      "                 Mean total reward: -1520.76\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 101200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 361.55s\n",
      "                               ETA: 353.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 506/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.570s, learning 0.140s)\n",
      "               Value function loss: 9841.7566\n",
      "                    Surrogate loss: 0.0151\n",
      "             Mean action noise std: 0.51\n",
      "                 Mean total reward: -1522.69\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 101400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 362.26s\n",
      "                               ETA: 353.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 507/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.562s, learning 0.134s)\n",
      "               Value function loss: 9868.0716\n",
      "                    Surrogate loss: -0.0005\n",
      "             Mean action noise std: 0.51\n",
      "                 Mean total reward: -1523.51\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 101600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 362.95s\n",
      "                               ETA: 352.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 508/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 269 steps/s (collection: 0.589s, learning 0.153s)\n",
      "               Value function loss: 10815.5224\n",
      "                    Surrogate loss: 0.0018\n",
      "             Mean action noise std: 0.50\n",
      "                 Mean total reward: -1527.49\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 101800\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 363.69s\n",
      "                               ETA: 351.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 509/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.586s, learning 0.127s)\n",
      "               Value function loss: 10723.0166\n",
      "                    Surrogate loss: 0.0004\n",
      "             Mean action noise std: 0.50\n",
      "                 Mean total reward: -1529.05\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 102000\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 364.41s\n",
      "                               ETA: 350.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 510/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.582s, learning 0.122s)\n",
      "               Value function loss: 10263.6322\n",
      "                    Surrogate loss: 0.0048\n",
      "             Mean action noise std: 0.49\n",
      "                 Mean total reward: -1528.67\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 102200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 365.11s\n",
      "                               ETA: 350.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 511/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.577s, learning 0.126s)\n",
      "               Value function loss: 10589.4879\n",
      "                    Surrogate loss: 0.0004\n",
      "             Mean action noise std: 0.52\n",
      "                 Mean total reward: -1529.01\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 102400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 365.81s\n",
      "                               ETA: 349.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 512/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.567s, learning 0.125s)\n",
      "               Value function loss: 10025.4062\n",
      "                    Surrogate loss: 0.0027\n",
      "             Mean action noise std: 0.53\n",
      "                 Mean total reward: -1532.46\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 102600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 366.51s\n",
      "                               ETA: 348.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 513/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.575s, learning 0.121s)\n",
      "               Value function loss: 8218.5476\n",
      "                    Surrogate loss: -0.0029\n",
      "             Mean action noise std: 0.53\n",
      "                 Mean total reward: -1531.91\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 102800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 367.20s\n",
      "                               ETA: 347.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 514/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 293 steps/s (collection: 0.555s, learning 0.126s)\n",
      "               Value function loss: 7361.5328\n",
      "                    Surrogate loss: 0.0131\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1531.23\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 103000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 367.88s\n",
      "                               ETA: 347.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 515/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.562s, learning 0.118s)\n",
      "               Value function loss: 10904.8456\n",
      "                    Surrogate loss: 0.0003\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1531.79\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 103200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 368.56s\n",
      "                               ETA: 346.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 516/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.574s, learning 0.132s)\n",
      "               Value function loss: 11022.0510\n",
      "                    Surrogate loss: -0.0069\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1533.77\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 103400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 369.27s\n",
      "                               ETA: 345.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 517/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.571s, learning 0.129s)\n",
      "               Value function loss: 10553.8171\n",
      "                    Surrogate loss: 0.0002\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1534.06\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 103600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 369.97s\n",
      "                               ETA: 345.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 518/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.567s, learning 0.127s)\n",
      "               Value function loss: 10575.9641\n",
      "                    Surrogate loss: -0.0015\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1536.69\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 103800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 370.66s\n",
      "                               ETA: 344.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 519/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.561s, learning 0.123s)\n",
      "               Value function loss: 11197.8083\n",
      "                    Surrogate loss: 0.0012\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1537.87\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 104000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 371.35s\n",
      "                               ETA: 343.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 520/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.570s, learning 0.126s)\n",
      "               Value function loss: 11128.7703\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1537.32\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 104200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 372.04s\n",
      "                               ETA: 342.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 521/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.594s, learning 0.124s)\n",
      "               Value function loss: 9673.5687\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 0.53\n",
      "                 Mean total reward: -1536.84\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 104400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 372.76s\n",
      "                               ETA: 342.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 522/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.577s, learning 0.123s)\n",
      "               Value function loss: 10652.9815\n",
      "                    Surrogate loss: -0.0074\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1536.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 104600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 373.46s\n",
      "                               ETA: 341.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 523/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.580s, learning 0.121s)\n",
      "               Value function loss: 9947.3171\n",
      "                    Surrogate loss: 0.0132\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1538.84\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 104800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 374.16s\n",
      "                               ETA: 340.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 524/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 295 steps/s (collection: 0.548s, learning 0.129s)\n",
      "               Value function loss: 11108.3120\n",
      "                    Surrogate loss: -0.0013\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1539.57\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 105000\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 374.84s\n",
      "                               ETA: 339.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 525/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 267 steps/s (collection: 0.603s, learning 0.144s)\n",
      "               Value function loss: 11181.8226\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1541.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 105200\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 375.59s\n",
      "                               ETA: 339.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 526/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 247 steps/s (collection: 0.674s, learning 0.134s)\n",
      "               Value function loss: 9977.5058\n",
      "                    Surrogate loss: 0.0010\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1540.95\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 105400\n",
      "                    Iteration time: 0.81s\n",
      "                        Total time: 376.39s\n",
      "                               ETA: 338.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 527/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 263 steps/s (collection: 0.606s, learning 0.153s)\n",
      "               Value function loss: 11189.6180\n",
      "                    Surrogate loss: 0.0042\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1546.68\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 105600\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 377.15s\n",
      "                               ETA: 337.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 528/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 266 steps/s (collection: 0.632s, learning 0.119s)\n",
      "               Value function loss: 10538.1050\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1548.99\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 105800\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 377.90s\n",
      "                               ETA: 337.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 529/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.599s, learning 0.127s)\n",
      "               Value function loss: 9668.2672\n",
      "                    Surrogate loss: -0.0146\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1550.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 106000\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 378.63s\n",
      "                               ETA: 336.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 530/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.582s, learning 0.135s)\n",
      "               Value function loss: 10842.1496\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1553.80\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 106200\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 379.35s\n",
      "                               ETA: 335.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 531/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 272 steps/s (collection: 0.596s, learning 0.137s)\n",
      "               Value function loss: 7879.7718\n",
      "                    Surrogate loss: -0.0046\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1554.47\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 106400\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 380.08s\n",
      "                               ETA: 335.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 532/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 271 steps/s (collection: 0.611s, learning 0.126s)\n",
      "               Value function loss: 10822.4221\n",
      "                    Surrogate loss: -0.0011\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1554.48\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 106600\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 380.82s\n",
      "                               ETA: 334.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 533/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.586s, learning 0.131s)\n",
      "               Value function loss: 11089.5947\n",
      "                    Surrogate loss: 0.0042\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1555.82\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 106800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 381.53s\n",
      "                               ETA: 333.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 534/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 272 steps/s (collection: 0.605s, learning 0.130s)\n",
      "               Value function loss: 3012.3749\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1547.23\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 107000\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 382.27s\n",
      "                               ETA: 333.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 535/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.572s, learning 0.127s)\n",
      "               Value function loss: 9951.8692\n",
      "                    Surrogate loss: -0.0008\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1552.54\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 107200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 382.97s\n",
      "                               ETA: 332.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 536/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.566s, learning 0.126s)\n",
      "               Value function loss: 2381.4365\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 0.56\n",
      "                 Mean total reward: -1550.65\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 107400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 383.66s\n",
      "                               ETA: 331.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 537/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 255 steps/s (collection: 0.649s, learning 0.135s)\n",
      "               Value function loss: 10095.8321\n",
      "                    Surrogate loss: -0.0006\n",
      "             Mean action noise std: 0.56\n",
      "                 Mean total reward: -1550.34\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 107600\n",
      "                    Iteration time: 0.78s\n",
      "                        Total time: 384.44s\n",
      "                               ETA: 330.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 538/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 270 steps/s (collection: 0.610s, learning 0.130s)\n",
      "               Value function loss: 9941.6021\n",
      "                    Surrogate loss: -0.0092\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1550.06\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 107800\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 385.18s\n",
      "                               ETA: 330.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 539/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 265 steps/s (collection: 0.632s, learning 0.121s)\n",
      "               Value function loss: 10328.3062\n",
      "                    Surrogate loss: -0.0002\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1550.59\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 108000\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 385.94s\n",
      "                               ETA: 329.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 540/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 272 steps/s (collection: 0.607s, learning 0.128s)\n",
      "               Value function loss: 10273.9368\n",
      "                    Surrogate loss: 0.0049\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1555.57\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 108200\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 386.67s\n",
      "                               ETA: 328.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 541/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 263 steps/s (collection: 0.630s, learning 0.128s)\n",
      "               Value function loss: 9820.9282\n",
      "                    Surrogate loss: 0.0005\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1556.71\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 108400\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 387.43s\n",
      "                               ETA: 328.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 542/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 267 steps/s (collection: 0.615s, learning 0.131s)\n",
      "               Value function loss: 8770.5534\n",
      "                    Surrogate loss: 0.0378\n",
      "             Mean action noise std: 0.56\n",
      "                 Mean total reward: -1555.08\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 108600\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 388.18s\n",
      "                               ETA: 327.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 543/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 258 steps/s (collection: 0.635s, learning 0.138s)\n",
      "               Value function loss: 8256.1449\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1552.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 108800\n",
      "                    Iteration time: 0.77s\n",
      "                        Total time: 388.95s\n",
      "                               ETA: 326.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 544/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.566s, learning 0.131s)\n",
      "               Value function loss: 10274.9989\n",
      "                    Surrogate loss: 0.0073\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1551.89\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 109000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 389.65s\n",
      "                               ETA: 326.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 545/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.568s, learning 0.127s)\n",
      "               Value function loss: 10823.2817\n",
      "                    Surrogate loss: 0.0002\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1552.04\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 109200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 390.34s\n",
      "                               ETA: 325.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 546/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.584s, learning 0.126s)\n",
      "               Value function loss: 10718.5185\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1552.22\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 109400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 391.05s\n",
      "                               ETA: 324.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 547/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.585s, learning 0.131s)\n",
      "               Value function loss: 11035.5557\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1551.63\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 109600\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 391.77s\n",
      "                               ETA: 323.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 548/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.587s, learning 0.126s)\n",
      "               Value function loss: 11064.3400\n",
      "                    Surrogate loss: -0.0066\n",
      "             Mean action noise std: 0.56\n",
      "                 Mean total reward: -1553.01\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 109800\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 392.48s\n",
      "                               ETA: 323.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 549/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.580s, learning 0.130s)\n",
      "               Value function loss: 10734.6327\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 0.56\n",
      "                 Mean total reward: -1552.67\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 110000\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 393.19s\n",
      "                               ETA: 322.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 550/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.574s, learning 0.129s)\n",
      "               Value function loss: 11073.7754\n",
      "                    Surrogate loss: 0.0150\n",
      "             Mean action noise std: 0.56\n",
      "                 Mean total reward: -1553.26\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 110200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 393.89s\n",
      "                               ETA: 321.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 551/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.567s, learning 0.126s)\n",
      "               Value function loss: 10503.4043\n",
      "                    Surrogate loss: 0.0008\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1553.26\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 110400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 394.59s\n",
      "                               ETA: 321.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 552/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.588s, learning 0.132s)\n",
      "               Value function loss: 11021.0916\n",
      "                    Surrogate loss: 0.0184\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1554.09\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 110600\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 395.31s\n",
      "                               ETA: 320.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 553/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.578s, learning 0.130s)\n",
      "               Value function loss: 10102.2988\n",
      "                    Surrogate loss: -0.0024\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1553.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 110800\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 396.01s\n",
      "                               ETA: 319.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 554/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 272 steps/s (collection: 0.604s, learning 0.129s)\n",
      "               Value function loss: 11051.6335\n",
      "                    Surrogate loss: 0.0051\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1553.83\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 111000\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 396.75s\n",
      "                               ETA: 318.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 555/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.586s, learning 0.129s)\n",
      "               Value function loss: 9937.9516\n",
      "                    Surrogate loss: -0.0019\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1553.80\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 111200\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 397.46s\n",
      "                               ETA: 318.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 556/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.576s, learning 0.122s)\n",
      "               Value function loss: 11015.1453\n",
      "                    Surrogate loss: -0.0028\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1554.43\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 111400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 398.16s\n",
      "                               ETA: 317.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 557/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.561s, learning 0.132s)\n",
      "               Value function loss: 10986.2266\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 0.54\n",
      "                 Mean total reward: -1559.76\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 111600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 398.85s\n",
      "                               ETA: 316.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 558/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.585s, learning 0.126s)\n",
      "               Value function loss: 11015.1148\n",
      "                    Surrogate loss: 0.0153\n",
      "             Mean action noise std: 0.55\n",
      "                 Mean total reward: -1566.75\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 111800\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 399.56s\n",
      "                               ETA: 315.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 559/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.572s, learning 0.133s)\n",
      "               Value function loss: 8470.7190\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 0.56\n",
      "                 Mean total reward: -1564.59\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 112000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 400.27s\n",
      "                               ETA: 315.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 560/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.570s, learning 0.132s)\n",
      "               Value function loss: 10991.2635\n",
      "                    Surrogate loss: 0.0016\n",
      "             Mean action noise std: 0.58\n",
      "                 Mean total reward: -1563.99\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 112200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 400.97s\n",
      "                               ETA: 314.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 561/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.583s, learning 0.135s)\n",
      "               Value function loss: 8511.1377\n",
      "                    Surrogate loss: 0.0950\n",
      "             Mean action noise std: 0.61\n",
      "                 Mean total reward: -1562.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 112400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 401.69s\n",
      "                               ETA: 313.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 562/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.577s, learning 0.128s)\n",
      "               Value function loss: 9188.6129\n",
      "                    Surrogate loss: 0.0074\n",
      "             Mean action noise std: 0.61\n",
      "                 Mean total reward: -1560.72\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 112600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 402.39s\n",
      "                               ETA: 313.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 563/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.595s, learning 0.129s)\n",
      "               Value function loss: 6101.0718\n",
      "                    Surrogate loss: 0.0020\n",
      "             Mean action noise std: 0.62\n",
      "                 Mean total reward: -1557.37\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 112800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 403.12s\n",
      "                               ETA: 312.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 564/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.591s, learning 0.126s)\n",
      "               Value function loss: 10322.1336\n",
      "                    Surrogate loss: 0.0077\n",
      "             Mean action noise std: 0.62\n",
      "                 Mean total reward: -1559.56\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 113000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 403.84s\n",
      "                               ETA: 311.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 565/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.563s, learning 0.128s)\n",
      "               Value function loss: 8931.4117\n",
      "                    Surrogate loss: -0.0009\n",
      "             Mean action noise std: 0.62\n",
      "                 Mean total reward: -1559.57\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 113200\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 404.53s\n",
      "                               ETA: 310.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 566/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 291 steps/s (collection: 0.555s, learning 0.131s)\n",
      "               Value function loss: 9379.8018\n",
      "                    Surrogate loss: 0.0024\n",
      "             Mean action noise std: 0.62\n",
      "                 Mean total reward: -1559.31\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 113400\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 405.21s\n",
      "                               ETA: 310.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 567/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 263 steps/s (collection: 0.625s, learning 0.133s)\n",
      "               Value function loss: 10803.5893\n",
      "                    Surrogate loss: -0.0013\n",
      "             Mean action noise std: 0.62\n",
      "                 Mean total reward: -1559.88\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 113600\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 405.97s\n",
      "                               ETA: 309.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 568/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.600s, learning 0.126s)\n",
      "               Value function loss: 8617.4249\n",
      "                    Surrogate loss: -0.0181\n",
      "             Mean action noise std: 0.62\n",
      "                 Mean total reward: -1558.28\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 113800\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 406.70s\n",
      "                               ETA: 308.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 569/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.577s, learning 0.127s)\n",
      "               Value function loss: 7516.6883\n",
      "                    Surrogate loss: 0.0080\n",
      "             Mean action noise std: 0.62\n",
      "                 Mean total reward: -1557.36\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 114000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 407.40s\n",
      "                               ETA: 308.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 570/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 290 steps/s (collection: 0.562s, learning 0.127s)\n",
      "               Value function loss: 10904.3824\n",
      "                    Surrogate loss: 0.0089\n",
      "             Mean action noise std: 0.62\n",
      "                 Mean total reward: -1557.24\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 114200\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 408.09s\n",
      "                               ETA: 307.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 571/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.588s, learning 0.124s)\n",
      "               Value function loss: 8685.0017\n",
      "                    Surrogate loss: 0.0366\n",
      "             Mean action noise std: 0.64\n",
      "                 Mean total reward: -1557.71\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 114400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 408.80s\n",
      "                               ETA: 306.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 572/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.602s, learning 0.130s)\n",
      "               Value function loss: 10766.7770\n",
      "                    Surrogate loss: 0.0012\n",
      "             Mean action noise std: 0.65\n",
      "                 Mean total reward: -1557.20\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 114600\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 409.53s\n",
      "                               ETA: 305.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 573/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.577s, learning 0.122s)\n",
      "               Value function loss: 10902.6443\n",
      "                    Surrogate loss: 0.0755\n",
      "             Mean action noise std: 0.67\n",
      "                 Mean total reward: -1557.80\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 114800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 410.23s\n",
      "                               ETA: 305.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 574/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.575s, learning 0.135s)\n",
      "               Value function loss: 9392.1174\n",
      "                    Surrogate loss: 0.0046\n",
      "             Mean action noise std: 0.68\n",
      "                 Mean total reward: -1557.86\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 115000\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 410.94s\n",
      "                               ETA: 304.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 575/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.572s, learning 0.134s)\n",
      "               Value function loss: 9656.6676\n",
      "                    Surrogate loss: 0.0352\n",
      "             Mean action noise std: 0.71\n",
      "                 Mean total reward: -1561.03\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 115200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 411.65s\n",
      "                               ETA: 303.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 576/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.596s, learning 0.131s)\n",
      "               Value function loss: 10308.7942\n",
      "                    Surrogate loss: 0.0097\n",
      "             Mean action noise std: 0.72\n",
      "                 Mean total reward: -1560.59\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 115400\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 412.38s\n",
      "                               ETA: 303.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 577/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.574s, learning 0.140s)\n",
      "               Value function loss: 9409.6031\n",
      "                    Surrogate loss: -0.0008\n",
      "             Mean action noise std: 0.72\n",
      "                 Mean total reward: -1560.18\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 115600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 413.09s\n",
      "                               ETA: 302.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 578/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.570s, learning 0.121s)\n",
      "               Value function loss: 10887.5244\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1560.52\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 115800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 413.78s\n",
      "                               ETA: 301.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 579/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.575s, learning 0.127s)\n",
      "               Value function loss: 8431.2904\n",
      "                    Surrogate loss: 0.0342\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1558.87\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 116000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 414.48s\n",
      "                               ETA: 300.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 580/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.569s, learning 0.127s)\n",
      "               Value function loss: 10855.9898\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1564.53\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 116200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 415.18s\n",
      "                               ETA: 300.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 581/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.593s, learning 0.131s)\n",
      "               Value function loss: 10860.1625\n",
      "                    Surrogate loss: -0.0024\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1567.89\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 116400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 415.90s\n",
      "                               ETA: 299.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 582/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.585s, learning 0.127s)\n",
      "               Value function loss: 10304.0049\n",
      "                    Surrogate loss: -0.0015\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1567.86\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 116600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 416.62s\n",
      "                               ETA: 298.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 583/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.594s, learning 0.124s)\n",
      "               Value function loss: 9840.7995\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1568.12\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 116800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 417.33s\n",
      "                               ETA: 298.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 584/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 274 steps/s (collection: 0.603s, learning 0.126s)\n",
      "               Value function loss: 10841.6882\n",
      "                    Surrogate loss: -0.0092\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1568.88\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 117000\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 418.06s\n",
      "                               ETA: 297.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 585/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.588s, learning 0.122s)\n",
      "               Value function loss: 10821.4584\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1570.41\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 117200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 418.77s\n",
      "                               ETA: 296.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 586/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.572s, learning 0.130s)\n",
      "               Value function loss: 10702.1855\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1569.99\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 117400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 419.47s\n",
      "                               ETA: 295.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 587/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.600s, learning 0.131s)\n",
      "               Value function loss: 10196.2094\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1570.47\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 117600\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 420.21s\n",
      "                               ETA: 295.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 588/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.593s, learning 0.124s)\n",
      "               Value function loss: 10552.7883\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1570.92\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 117800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 420.92s\n",
      "                               ETA: 294.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 589/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.605s, learning 0.125s)\n",
      "               Value function loss: 9658.4665\n",
      "                    Surrogate loss: 0.0362\n",
      "             Mean action noise std: 0.77\n",
      "                 Mean total reward: -1570.31\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 118000\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 421.65s\n",
      "                               ETA: 293.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 590/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.586s, learning 0.128s)\n",
      "               Value function loss: 10330.6529\n",
      "                    Surrogate loss: 0.0103\n",
      "             Mean action noise std: 0.77\n",
      "                 Mean total reward: -1571.62\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 118200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 422.37s\n",
      "                               ETA: 293.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 591/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 262 steps/s (collection: 0.635s, learning 0.128s)\n",
      "               Value function loss: 10773.8236\n",
      "                    Surrogate loss: -0.0032\n",
      "             Mean action noise std: 0.77\n",
      "                 Mean total reward: -1571.53\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 118400\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 423.13s\n",
      "                               ETA: 292.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 592/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 266 steps/s (collection: 0.619s, learning 0.132s)\n",
      "               Value function loss: 10093.4163\n",
      "                    Surrogate loss: 0.0004\n",
      "             Mean action noise std: 0.77\n",
      "                 Mean total reward: -1572.77\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 118600\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 423.88s\n",
      "                               ETA: 291.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 593/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.595s, learning 0.131s)\n",
      "               Value function loss: 9303.2629\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 0.76\n",
      "                 Mean total reward: -1571.53\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 118800\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 424.61s\n",
      "                               ETA: 290.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 594/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 277 steps/s (collection: 0.592s, learning 0.129s)\n",
      "               Value function loss: 9583.4217\n",
      "                    Surrogate loss: -0.0094\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1570.59\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 119000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 425.33s\n",
      "                               ETA: 290.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 595/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.577s, learning 0.137s)\n",
      "               Value function loss: 10495.7156\n",
      "                    Surrogate loss: -0.0013\n",
      "             Mean action noise std: 0.76\n",
      "                 Mean total reward: -1570.11\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 119200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 426.04s\n",
      "                               ETA: 289.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 596/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.585s, learning 0.134s)\n",
      "               Value function loss: 10478.3604\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.76\n",
      "                 Mean total reward: -1570.85\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 119400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 426.76s\n",
      "                               ETA: 288.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 597/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.582s, learning 0.126s)\n",
      "               Value function loss: 10717.5847\n",
      "                    Surrogate loss: -0.0091\n",
      "             Mean action noise std: 0.76\n",
      "                 Mean total reward: -1571.42\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 119600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 427.47s\n",
      "                               ETA: 288.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 598/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.594s, learning 0.121s)\n",
      "               Value function loss: 9262.7581\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1570.87\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 119800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 428.18s\n",
      "                               ETA: 287.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 599/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 287 steps/s (collection: 0.570s, learning 0.125s)\n",
      "               Value function loss: 10139.2441\n",
      "                    Surrogate loss: -0.0003\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1571.06\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 120000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 428.88s\n",
      "                               ETA: 286.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 600/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.606s, learning 0.124s)\n",
      "               Value function loss: 10542.5790\n",
      "                    Surrogate loss: -0.0076\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1572.21\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 120200\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 429.61s\n",
      "                               ETA: 285.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 601/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.576s, learning 0.154s)\n",
      "               Value function loss: 10720.7094\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1573.06\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 120400\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 430.34s\n",
      "                               ETA: 285.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 602/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 236 steps/s (collection: 0.711s, learning 0.136s)\n",
      "               Value function loss: 10740.6921\n",
      "                    Surrogate loss: 0.0011\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1573.62\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 120600\n",
      "                    Iteration time: 0.85s\n",
      "                        Total time: 431.19s\n",
      "                               ETA: 284.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 603/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.597s, learning 0.120s)\n",
      "               Value function loss: 10690.8066\n",
      "                    Surrogate loss: -0.0073\n",
      "             Mean action noise std: 0.76\n",
      "                 Mean total reward: -1575.77\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 120800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 431.90s\n",
      "                               ETA: 283.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 604/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 300 steps/s (collection: 0.539s, learning 0.127s)\n",
      "               Value function loss: 10164.0016\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 0.76\n",
      "                 Mean total reward: -1575.33\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 121000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 432.57s\n",
      "                               ETA: 283.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 605/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 246 steps/s (collection: 0.662s, learning 0.149s)\n",
      "               Value function loss: 9892.2220\n",
      "                    Surrogate loss: 0.0007\n",
      "             Mean action noise std: 0.76\n",
      "                 Mean total reward: -1576.73\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 121200\n",
      "                    Iteration time: 0.81s\n",
      "                        Total time: 433.38s\n",
      "                               ETA: 282.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 606/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 244 steps/s (collection: 0.678s, learning 0.139s)\n",
      "               Value function loss: 10750.8822\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 0.76\n",
      "                 Mean total reward: -1576.95\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 121400\n",
      "                    Iteration time: 0.82s\n",
      "                        Total time: 434.20s\n",
      "                               ETA: 281.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 607/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 297 steps/s (collection: 0.548s, learning 0.125s)\n",
      "               Value function loss: 9954.7841\n",
      "                    Surrogate loss: 0.0012\n",
      "             Mean action noise std: 0.76\n",
      "                 Mean total reward: -1577.24\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 121600\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 434.87s\n",
      "                               ETA: 281.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 608/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.583s, learning 0.149s)\n",
      "               Value function loss: 10122.3167\n",
      "                    Surrogate loss: 0.0009\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1576.80\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 121800\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 435.60s\n",
      "                               ETA: 280.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 609/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.589s, learning 0.142s)\n",
      "               Value function loss: 10738.0648\n",
      "                    Surrogate loss: 0.0008\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1577.20\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 122000\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 436.33s\n",
      "                               ETA: 279.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 610/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.583s, learning 0.140s)\n",
      "               Value function loss: 10702.7149\n",
      "                    Surrogate loss: 0.0439\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1578.93\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 122200\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 437.06s\n",
      "                               ETA: 279.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 611/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.576s, learning 0.142s)\n",
      "               Value function loss: 10737.7122\n",
      "                    Surrogate loss: 0.0008\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1579.55\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 122400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 437.77s\n",
      "                               ETA: 278.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 612/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.601s, learning 0.126s)\n",
      "               Value function loss: 10375.2852\n",
      "                    Surrogate loss: -0.0009\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1579.96\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 122600\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 438.50s\n",
      "                               ETA: 277.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 613/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 223 steps/s (collection: 0.755s, learning 0.139s)\n",
      "               Value function loss: 10620.2925\n",
      "                    Surrogate loss: -0.0030\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1581.40\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 122800\n",
      "                    Iteration time: 0.89s\n",
      "                        Total time: 439.39s\n",
      "                               ETA: 276.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 614/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 262 steps/s (collection: 0.638s, learning 0.125s)\n",
      "               Value function loss: 10682.1383\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1585.28\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 123000\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 440.16s\n",
      "                               ETA: 276.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 615/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.572s, learning 0.128s)\n",
      "               Value function loss: 9951.0824\n",
      "                    Surrogate loss: -0.0011\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1585.21\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 123200\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 440.86s\n",
      "                               ETA: 275.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 616/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.574s, learning 0.124s)\n",
      "               Value function loss: 10689.2711\n",
      "                    Surrogate loss: 0.0017\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1585.23\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 123400\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 441.56s\n",
      "                               ETA: 274.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 617/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.586s, learning 0.120s)\n",
      "               Value function loss: 10588.6085\n",
      "                    Surrogate loss: 0.0044\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1585.34\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 123600\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 442.26s\n",
      "                               ETA: 274.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 618/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 290 steps/s (collection: 0.556s, learning 0.133s)\n",
      "               Value function loss: 10601.1145\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1584.87\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 123800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 442.95s\n",
      "                               ETA: 273.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 619/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 290 steps/s (collection: 0.564s, learning 0.125s)\n",
      "               Value function loss: 8952.8810\n",
      "                    Surrogate loss: -0.0092\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1584.17\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 124000\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 443.64s\n",
      "                               ETA: 272.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 620/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 282 steps/s (collection: 0.571s, learning 0.136s)\n",
      "               Value function loss: 10641.8238\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1584.57\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 124200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 444.35s\n",
      "                               ETA: 271.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 621/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.580s, learning 0.130s)\n",
      "               Value function loss: 10702.6795\n",
      "                    Surrogate loss: 0.0022\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1585.87\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 124400\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 445.06s\n",
      "                               ETA: 271.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 622/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 273 steps/s (collection: 0.607s, learning 0.125s)\n",
      "               Value function loss: 10456.2571\n",
      "                    Surrogate loss: -0.0043\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1585.72\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 124600\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 445.79s\n",
      "                               ETA: 270.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 623/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 269 steps/s (collection: 0.611s, learning 0.131s)\n",
      "               Value function loss: 9287.7656\n",
      "                    Surrogate loss: 0.0009\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1585.10\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 124800\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 446.53s\n",
      "                               ETA: 269.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 624/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.595s, learning 0.124s)\n",
      "               Value function loss: 10672.4930\n",
      "                    Surrogate loss: -0.0008\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1584.85\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 125000\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 447.25s\n",
      "                               ETA: 269.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 625/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 265 steps/s (collection: 0.629s, learning 0.125s)\n",
      "               Value function loss: 9198.7712\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 0.72\n",
      "                 Mean total reward: -1583.47\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 125200\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 448.00s\n",
      "                               ETA: 268.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 626/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 275 steps/s (collection: 0.594s, learning 0.132s)\n",
      "               Value function loss: 9843.5642\n",
      "                    Surrogate loss: 0.0093\n",
      "             Mean action noise std: 0.70\n",
      "                 Mean total reward: -1583.25\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 125400\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 448.73s\n",
      "                               ETA: 267.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 627/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 288 steps/s (collection: 0.567s, learning 0.127s)\n",
      "               Value function loss: 10617.5442\n",
      "                    Surrogate loss: 0.0010\n",
      "             Mean action noise std: 0.70\n",
      "                 Mean total reward: -1583.24\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 125600\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 449.42s\n",
      "                               ETA: 266.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 628/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.582s, learning 0.142s)\n",
      "               Value function loss: 10406.2250\n",
      "                    Surrogate loss: 0.0055\n",
      "             Mean action noise std: 0.70\n",
      "                 Mean total reward: -1583.11\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 125800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 450.15s\n",
      "                               ETA: 266.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 629/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 266 steps/s (collection: 0.617s, learning 0.133s)\n",
      "               Value function loss: 8850.8576\n",
      "                    Surrogate loss: 0.0025\n",
      "             Mean action noise std: 0.70\n",
      "                 Mean total reward: -1582.77\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 126000\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 450.90s\n",
      "                               ETA: 265.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 630/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 256 steps/s (collection: 0.650s, learning 0.130s)\n",
      "               Value function loss: 9954.1997\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.69\n",
      "                 Mean total reward: -1582.39\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 126200\n",
      "                    Iteration time: 0.78s\n",
      "                        Total time: 451.68s\n",
      "                               ETA: 264.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 631/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 292 steps/s (collection: 0.568s, learning 0.115s)\n",
      "               Value function loss: 9222.4917\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 0.67\n",
      "                 Mean total reward: -1583.65\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 126400\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 452.36s\n",
      "                               ETA: 264.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 632/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 328 steps/s (collection: 0.493s, learning 0.115s)\n",
      "               Value function loss: 8697.5001\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 0.68\n",
      "                 Mean total reward: -1582.25\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 126600\n",
      "                    Iteration time: 0.61s\n",
      "                        Total time: 452.97s\n",
      "                               ETA: 263.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 633/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.584s, learning 0.130s)\n",
      "               Value function loss: 10597.4243\n",
      "                    Surrogate loss: 0.0071\n",
      "             Mean action noise std: 0.68\n",
      "                 Mean total reward: -1582.07\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 126800\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 453.68s\n",
      "                               ETA: 262.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 634/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 311 steps/s (collection: 0.527s, learning 0.116s)\n",
      "               Value function loss: 8841.7023\n",
      "                    Surrogate loss: 0.0009\n",
      "             Mean action noise std: 0.68\n",
      "                 Mean total reward: -1590.04\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 127000\n",
      "                    Iteration time: 0.64s\n",
      "                        Total time: 454.33s\n",
      "                               ETA: 261.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 635/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 327 steps/s (collection: 0.499s, learning 0.111s)\n",
      "               Value function loss: 9854.4023\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 0.68\n",
      "                 Mean total reward: -1589.89\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 127200\n",
      "                    Iteration time: 0.61s\n",
      "                        Total time: 454.94s\n",
      "                               ETA: 261.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 636/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 301 steps/s (collection: 0.541s, learning 0.122s)\n",
      "               Value function loss: 10276.9039\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 0.67\n",
      "                 Mean total reward: -1597.20\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 127400\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 455.60s\n",
      "                               ETA: 260.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 637/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 312 steps/s (collection: 0.519s, learning 0.120s)\n",
      "               Value function loss: 10582.3099\n",
      "                    Surrogate loss: -0.0028\n",
      "             Mean action noise std: 0.66\n",
      "                 Mean total reward: -1597.58\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 127600\n",
      "                    Iteration time: 0.64s\n",
      "                        Total time: 456.24s\n",
      "                               ETA: 259.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 638/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 304 steps/s (collection: 0.535s, learning 0.122s)\n",
      "               Value function loss: 10626.7787\n",
      "                    Surrogate loss: 0.0083\n",
      "             Mean action noise std: 0.67\n",
      "                 Mean total reward: -1598.01\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 127800\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 456.89s\n",
      "                               ETA: 258.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 639/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 314 steps/s (collection: 0.523s, learning 0.113s)\n",
      "               Value function loss: 9114.9316\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 0.67\n",
      "                 Mean total reward: -1597.37\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 128000\n",
      "                    Iteration time: 0.64s\n",
      "                        Total time: 457.53s\n",
      "                               ETA: 258.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 640/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 270 steps/s (collection: 0.590s, learning 0.148s)\n",
      "               Value function loss: 10151.0676\n",
      "                    Surrogate loss: 0.0039\n",
      "             Mean action noise std: 0.67\n",
      "                 Mean total reward: -1597.72\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 128200\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 458.27s\n",
      "                               ETA: 257.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 641/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.598s, learning 0.118s)\n",
      "               Value function loss: 10528.9026\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 0.68\n",
      "                 Mean total reward: -1598.41\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 128400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 458.98s\n",
      "                               ETA: 256.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 642/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 307 steps/s (collection: 0.529s, learning 0.120s)\n",
      "               Value function loss: 10567.7438\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 0.68\n",
      "                 Mean total reward: -1600.10\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 128600\n",
      "                    Iteration time: 0.65s\n",
      "                        Total time: 459.63s\n",
      "                               ETA: 255.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 643/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 270 steps/s (collection: 0.598s, learning 0.142s)\n",
      "               Value function loss: 10491.2553\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 0.69\n",
      "                 Mean total reward: -1602.87\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 128800\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 460.38s\n",
      "                               ETA: 255.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 644/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 305 steps/s (collection: 0.538s, learning 0.117s)\n",
      "               Value function loss: 10504.9089\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 0.69\n",
      "                 Mean total reward: -1603.44\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 129000\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 461.03s\n",
      "                               ETA: 254.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 645/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 333 steps/s (collection: 0.488s, learning 0.112s)\n",
      "               Value function loss: 9119.2245\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 0.70\n",
      "                 Mean total reward: -1602.66\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 129200\n",
      "                    Iteration time: 0.60s\n",
      "                        Total time: 461.63s\n",
      "                               ETA: 253.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 646/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 332 steps/s (collection: 0.485s, learning 0.117s)\n",
      "               Value function loss: 9077.2864\n",
      "                    Surrogate loss: -0.0107\n",
      "             Mean action noise std: 0.72\n",
      "                 Mean total reward: -1602.53\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 129400\n",
      "                    Iteration time: 0.60s\n",
      "                        Total time: 462.23s\n",
      "                               ETA: 252.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 647/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 316 steps/s (collection: 0.514s, learning 0.118s)\n",
      "               Value function loss: 9760.9200\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 0.71\n",
      "                 Mean total reward: -1602.37\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 129600\n",
      "                    Iteration time: 0.63s\n",
      "                        Total time: 462.86s\n",
      "                               ETA: 252.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 648/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 306 steps/s (collection: 0.531s, learning 0.121s)\n",
      "               Value function loss: 10052.1252\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 0.71\n",
      "                 Mean total reward: -1601.62\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 129800\n",
      "                    Iteration time: 0.65s\n",
      "                        Total time: 463.52s\n",
      "                               ETA: 251.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 649/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 283 steps/s (collection: 0.585s, learning 0.120s)\n",
      "               Value function loss: 10552.6869\n",
      "                    Surrogate loss: -0.0010\n",
      "             Mean action noise std: 0.72\n",
      "                 Mean total reward: -1602.31\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 130000\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 464.22s\n",
      "                               ETA: 250.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 650/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.546s, learning 0.132s)\n",
      "               Value function loss: 9266.1725\n",
      "                    Surrogate loss: -0.0027\n",
      "             Mean action noise std: 0.72\n",
      "                 Mean total reward: -1601.07\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 130200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 464.90s\n",
      "                               ETA: 249.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 651/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 266 steps/s (collection: 0.617s, learning 0.134s)\n",
      "               Value function loss: 8987.4060\n",
      "                    Surrogate loss: -0.0027\n",
      "             Mean action noise std: 0.72\n",
      "                 Mean total reward: -1600.42\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 130400\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 465.65s\n",
      "                               ETA: 249.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 652/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 270 steps/s (collection: 0.591s, learning 0.148s)\n",
      "               Value function loss: 10509.9191\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 0.72\n",
      "                 Mean total reward: -1600.05\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 130600\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 466.39s\n",
      "                               ETA: 248.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 653/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.591s, learning 0.125s)\n",
      "               Value function loss: 9199.0364\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 0.71\n",
      "                 Mean total reward: -1599.52\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 130800\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 467.10s\n",
      "                               ETA: 247.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 654/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 318 steps/s (collection: 0.507s, learning 0.121s)\n",
      "               Value function loss: 10528.8484\n",
      "                    Surrogate loss: 0.0496\n",
      "             Mean action noise std: 0.71\n",
      "                 Mean total reward: -1599.60\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 131000\n",
      "                    Iteration time: 0.63s\n",
      "                        Total time: 467.73s\n",
      "                               ETA: 247.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 655/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 310 steps/s (collection: 0.515s, learning 0.128s)\n",
      "               Value function loss: 10484.0431\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 0.71\n",
      "                 Mean total reward: -1599.88\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 131200\n",
      "                    Iteration time: 0.64s\n",
      "                        Total time: 468.38s\n",
      "                               ETA: 246.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 656/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 248 steps/s (collection: 0.670s, learning 0.135s)\n",
      "               Value function loss: 10490.1773\n",
      "                    Surrogate loss: 0.0070\n",
      "             Mean action noise std: 0.71\n",
      "                 Mean total reward: -1600.01\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 131400\n",
      "                    Iteration time: 0.80s\n",
      "                        Total time: 469.18s\n",
      "                               ETA: 245.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 657/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 279 steps/s (collection: 0.577s, learning 0.139s)\n",
      "               Value function loss: 10425.3676\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 0.71\n",
      "                 Mean total reward: -1600.51\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 131600\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 469.90s\n",
      "                               ETA: 244.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 658/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.583s, learning 0.129s)\n",
      "               Value function loss: 9842.6330\n",
      "                    Surrogate loss: 0.0015\n",
      "             Mean action noise std: 0.71\n",
      "                 Mean total reward: -1599.80\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 131800\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 470.61s\n",
      "                               ETA: 244.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 659/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 296 steps/s (collection: 0.549s, learning 0.125s)\n",
      "               Value function loss: 8436.9438\n",
      "                    Surrogate loss: -0.0159\n",
      "             Mean action noise std: 0.71\n",
      "                 Mean total reward: -1600.44\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 132000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 471.28s\n",
      "                               ETA: 243.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 660/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 274 steps/s (collection: 0.589s, learning 0.140s)\n",
      "               Value function loss: 8940.2910\n",
      "                    Surrogate loss: -0.0082\n",
      "             Mean action noise std: 0.71\n",
      "                 Mean total reward: -1599.67\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 132200\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 472.01s\n",
      "                               ETA: 242.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 661/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 278 steps/s (collection: 0.589s, learning 0.129s)\n",
      "               Value function loss: 9998.6138\n",
      "                    Surrogate loss: -0.0003\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1600.82\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 132400\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 472.73s\n",
      "                               ETA: 242.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 662/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.556s, learning 0.123s)\n",
      "               Value function loss: 9363.3328\n",
      "                    Surrogate loss: 0.0238\n",
      "             Mean action noise std: 0.72\n",
      "                 Mean total reward: -1601.58\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 132600\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 473.41s\n",
      "                               ETA: 241.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 663/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 301 steps/s (collection: 0.541s, learning 0.121s)\n",
      "               Value function loss: 10124.1361\n",
      "                    Surrogate loss: 0.0056\n",
      "             Mean action noise std: 0.72\n",
      "                 Mean total reward: -1603.97\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 132800\n",
      "                    Iteration time: 0.66s\n",
      "                        Total time: 474.07s\n",
      "                               ETA: 240.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 664/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 299 steps/s (collection: 0.547s, learning 0.121s)\n",
      "               Value function loss: 9235.5416\n",
      "                    Surrogate loss: 0.0047\n",
      "             Mean action noise std: 0.72\n",
      "                 Mean total reward: -1602.93\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 133000\n",
      "                    Iteration time: 0.67s\n",
      "                        Total time: 474.74s\n",
      "                               ETA: 239.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 665/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 276 steps/s (collection: 0.592s, learning 0.132s)\n",
      "               Value function loss: 10298.5419\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1603.94\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 133200\n",
      "                    Iteration time: 0.72s\n",
      "                        Total time: 475.46s\n",
      "                               ETA: 239.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 666/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 266 steps/s (collection: 0.616s, learning 0.133s)\n",
      "               Value function loss: 9800.6967\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1603.79\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 133400\n",
      "                    Iteration time: 0.75s\n",
      "                        Total time: 476.21s\n",
      "                               ETA: 238.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 667/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 286 steps/s (collection: 0.568s, learning 0.131s)\n",
      "               Value function loss: 9798.0519\n",
      "                    Surrogate loss: -0.0079\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1603.30\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 133600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 476.91s\n",
      "                               ETA: 237.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 668/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 289 steps/s (collection: 0.557s, learning 0.134s)\n",
      "               Value function loss: 10430.1291\n",
      "                    Surrogate loss: -0.0172\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1604.48\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 133800\n",
      "                    Iteration time: 0.69s\n",
      "                        Total time: 477.60s\n",
      "                               ETA: 237.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 669/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 274 steps/s (collection: 0.599s, learning 0.128s)\n",
      "               Value function loss: 8815.1976\n",
      "                    Surrogate loss: -0.0012\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1605.21\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 134000\n",
      "                    Iteration time: 0.73s\n",
      "                        Total time: 478.33s\n",
      "                               ETA: 236.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 670/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 294 steps/s (collection: 0.556s, learning 0.123s)\n",
      "               Value function loss: 10334.0791\n",
      "                    Surrogate loss: -0.0086\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1605.57\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 134200\n",
      "                    Iteration time: 0.68s\n",
      "                        Total time: 479.01s\n",
      "                               ETA: 235.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 671/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 271 steps/s (collection: 0.610s, learning 0.126s)\n",
      "               Value function loss: 9735.9290\n",
      "                    Surrogate loss: 0.0021\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1605.99\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 134400\n",
      "                    Iteration time: 0.74s\n",
      "                        Total time: 479.75s\n",
      "                               ETA: 234.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 672/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 285 steps/s (collection: 0.562s, learning 0.139s)\n",
      "               Value function loss: 9813.0968\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1605.89\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 134600\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 480.45s\n",
      "                               ETA: 234.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 673/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 284 steps/s (collection: 0.577s, learning 0.126s)\n",
      "               Value function loss: 10213.9136\n",
      "                    Surrogate loss: -0.0117\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1605.19\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 134800\n",
      "                    Iteration time: 0.70s\n",
      "                        Total time: 481.15s\n",
      "                               ETA: 233.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 674/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 280 steps/s (collection: 0.572s, learning 0.141s)\n",
      "               Value function loss: 8876.5327\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1605.15\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 135000\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 481.86s\n",
      "                               ETA: 232.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 675/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 281 steps/s (collection: 0.582s, learning 0.129s)\n",
      "               Value function loss: 10179.8898\n",
      "                    Surrogate loss: 0.0089\n",
      "             Mean action noise std: 0.73\n",
      "                 Mean total reward: -1605.46\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 135200\n",
      "                    Iteration time: 0.71s\n",
      "                        Total time: 482.57s\n",
      "                               ETA: 232.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 676/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 238 steps/s (collection: 0.684s, learning 0.155s)\n",
      "               Value function loss: 8583.2626\n",
      "                    Surrogate loss: -0.0032\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1604.45\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 135400\n",
      "                    Iteration time: 0.84s\n",
      "                        Total time: 483.41s\n",
      "                               ETA: 231.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 677/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 253 steps/s (collection: 0.661s, learning 0.128s)\n",
      "               Value function loss: 10320.5683\n",
      "                    Surrogate loss: 0.0018\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1604.85\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 135600\n",
      "                    Iteration time: 0.79s\n",
      "                        Total time: 484.20s\n",
      "                               ETA: 230.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 678/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 254 steps/s (collection: 0.648s, learning 0.139s)\n",
      "               Value function loss: 10195.5834\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1604.67\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 135800\n",
      "                    Iteration time: 0.79s\n",
      "                        Total time: 484.99s\n",
      "                               ETA: 230.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 679/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 263 steps/s (collection: 0.603s, learning 0.156s)\n",
      "               Value function loss: 9938.8695\n",
      "                    Surrogate loss: -0.0071\n",
      "             Mean action noise std: 0.76\n",
      "                 Mean total reward: -1605.75\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 136000\n",
      "                    Iteration time: 0.76s\n",
      "                        Total time: 485.75s\n",
      "                               ETA: 229.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 680/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 248 steps/s (collection: 0.662s, learning 0.143s)\n",
      "               Value function loss: 10322.0002\n",
      "                    Surrogate loss: -0.0005\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1605.17\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 136200\n",
      "                    Iteration time: 0.81s\n",
      "                        Total time: 486.55s\n",
      "                               ETA: 228.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 681/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 245 steps/s (collection: 0.671s, learning 0.144s)\n",
      "               Value function loss: 10088.9130\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 0.76\n",
      "                 Mean total reward: -1605.20\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 136400\n",
      "                    Iteration time: 0.82s\n",
      "                        Total time: 487.37s\n",
      "                               ETA: 228.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 682/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 231 steps/s (collection: 0.717s, learning 0.146s)\n",
      "               Value function loss: 8485.6043\n",
      "                    Surrogate loss: 0.0062\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1604.58\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 136600\n",
      "                    Iteration time: 0.86s\n",
      "                        Total time: 488.23s\n",
      "                               ETA: 227.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 683/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 235 steps/s (collection: 0.699s, learning 0.152s)\n",
      "               Value function loss: 8908.3693\n",
      "                    Surrogate loss: 0.0014\n",
      "             Mean action noise std: 0.74\n",
      "                 Mean total reward: -1604.19\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 136800\n",
      "                    Iteration time: 0.85s\n",
      "                        Total time: 489.08s\n",
      "                               ETA: 226.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 684/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 247 steps/s (collection: 0.671s, learning 0.138s)\n",
      "               Value function loss: 8654.4902\n",
      "                    Surrogate loss: 0.0154\n",
      "             Mean action noise std: 0.75\n",
      "                 Mean total reward: -1602.85\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 137000\n",
      "                    Iteration time: 0.81s\n",
      "                        Total time: 489.89s\n",
      "                               ETA: 226.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 685/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 238 steps/s (collection: 0.696s, learning 0.141s)\n",
      "               Value function loss: 9883.3167\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 0.77\n",
      "                 Mean total reward: -1602.13\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 137200\n",
      "                    Iteration time: 0.84s\n",
      "                        Total time: 490.73s\n",
      "                               ETA: 225.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 686/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 248 steps/s (collection: 0.675s, learning 0.129s)\n",
      "               Value function loss: 7732.6970\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 0.77\n",
      "                 Mean total reward: -1600.22\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 137400\n",
      "                    Iteration time: 0.80s\n",
      "                        Total time: 491.53s\n",
      "                               ETA: 224.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 687/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 254 steps/s (collection: 0.638s, learning 0.149s)\n",
      "               Value function loss: 10180.2135\n",
      "                    Surrogate loss: 0.1857\n",
      "             Mean action noise std: 0.77\n",
      "                 Mean total reward: -1600.52\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 137600\n",
      "                    Iteration time: 0.79s\n",
      "                        Total time: 492.32s\n",
      "                               ETA: 224.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 688/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 250 steps/s (collection: 0.660s, learning 0.138s)\n",
      "               Value function loss: 8632.2266\n",
      "                    Surrogate loss: -0.0002\n",
      "             Mean action noise std: 0.78\n",
      "                 Mean total reward: -1598.43\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 137800\n",
      "                    Iteration time: 0.80s\n",
      "                        Total time: 493.12s\n",
      "                               ETA: 223.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 689/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 236 steps/s (collection: 0.710s, learning 0.136s)\n",
      "               Value function loss: 8791.4133\n",
      "                    Surrogate loss: 0.0029\n",
      "             Mean action noise std: 0.78\n",
      "                 Mean total reward: -1598.16\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 138000\n",
      "                    Iteration time: 0.85s\n",
      "                        Total time: 493.96s\n",
      "                               ETA: 222.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 690/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 253 steps/s (collection: 0.648s, learning 0.141s)\n",
      "               Value function loss: 4255.0369\n",
      "                    Surrogate loss: 0.0071\n",
      "             Mean action noise std: 0.78\n",
      "                 Mean total reward: -1594.17\n",
      "               Mean episode length: 200.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 138200\n",
      "                    Iteration time: 0.79s\n",
      "                        Total time: 494.75s\n",
      "                               ETA: 222.0s\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 165\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Expert (PPO) Training Start ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# train() の代わりに learn() を使う\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_learning_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_iterations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_at_random_ep_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 必要に応じてランダム初期長を有効化\u001b[39;49;00m\n\u001b[0;32m    168\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Expert Training Done ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin5050\\anaconda3\\envs\\env_isaaclab\\lib\\site-packages\\rsl_rl\\runners\\on_policy_runner.py:151\u001b[0m, in \u001b[0;36mOnPolicyRunner.learn\u001b[1;34m(self, num_learning_iterations, init_at_random_ep_len)\u001b[0m\n\u001b[0;32m    149\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malg\u001b[38;5;241m.\u001b[39mact(obs, critic_obs)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Step environment\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Move to the agent device\u001b[39;00m\n\u001b[0;32m    154\u001b[0m obs, rewards, dones \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), rewards\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), dones\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[34], line 53\u001b[0m, in \u001b[0;36mGymVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     51\u001b[0m obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mstack(obs_b, \u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m---> 53\u001b[0m rew \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrew_b\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     54\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(done_b, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# ── ここから extras を必ず返す──\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# policy 用観測\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% セル１: インポート＆GymVecEnvラッパー（前出のものを流用）\n",
    "import os, numpy as np, gymnasium as gym, torch\n",
    "from types import SimpleNamespace\n",
    "from rsl_rl.env import VecEnv\n",
    "from rsl_rl.runners.on_policy_runner import OnPolicyRunner\n",
    "\n",
    "class GymVecEnv(VecEnv):\n",
    "    def __init__(self, env_name, num_envs=1, device=\"cpu\"):\n",
    "        self.envs = [gym.make(env_name) for _ in range(num_envs)]\n",
    "        self.num_envs = num_envs\n",
    "        self.device = torch.device(device)\n",
    "        single = self.envs[0]\n",
    "        self.num_actions = np.prod(single.action_space.shape)\n",
    "        self.num_obs     = np.prod(single.observation_space.shape)\n",
    "        self.max_episode_length = getattr(single, \"_max_episode_steps\", 1000)\n",
    "        self._obs = None\n",
    "        self.episode_length_buf = torch.zeros(\n",
    "            (self.num_envs,), dtype=torch.long, device=self.device\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=0):\n",
    "        for i, e in enumerate(self.envs):\n",
    "            e.reset(seed=seed+i)\n",
    "        return seed\n",
    "\n",
    "    def reset(self):\n",
    "        obs = np.stack([e.reset()[0] for e in self.envs], 0)\n",
    "        self._obs = torch.from_numpy(obs).float().to(self.device)\n",
    "        return self._obs\n",
    "\n",
    "    def step(self, actions: torch.Tensor):\n",
    "        acts = actions.cpu().numpy()\n",
    "\n",
    "        # print(actions.min(), actions.max())\n",
    "\n",
    "\n",
    "        obs_b, rew_b, done_b = [], [], []\n",
    "        for env, a in zip(self.envs, acts):\n",
    "            o, r, ter, tru, _ = env.step(a)\n",
    "            done = ter or tru\n",
    "            if done:\n",
    "                o, _ = env.reset()\n",
    "            obs_b.append(o); rew_b.append(r); done_b.append(done)\n",
    "        # obs = torch.from_numpy(np.stack(obs_b,0)).float().to(self.device)\n",
    "        # self._obs = obs\n",
    "        # rew = torch.from_numpy(np.array(rew_b)).float().to(self.device)\n",
    "        # dones = torch.from_numpy(np.array(done_b, dtype=int)).to(self.device)\n",
    "        # return obs, rew, dones, {}\n",
    "        # バッチ化\n",
    "        obs = torch.from_numpy(np.stack(obs_b, 0)).float().to(self.device)\n",
    "        self._obs = obs\n",
    "        rew = torch.from_numpy(np.array(rew_b)).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array(done_b, dtype=int)).to(self.device)\n",
    "\n",
    "        # ── ここから extras を必ず返す──\n",
    "        # policy 用観測\n",
    "        policy_obs = obs\n",
    "        critic_obs = obs\n",
    "        # # critic 用観測（必要なら zeros で代用）\n",
    "        # critic_obs = torch.zeros_like(obs)\n",
    "\n",
    "        extras = {\n",
    "            \"observations\": {\n",
    "                \"policy\": policy_obs,\n",
    "                \"critic\": critic_obs,\n",
    "            }\n",
    "        }\n",
    "        return obs, rew, dones, extras\n",
    "\n",
    "    def get_observations(self):\n",
    "        \"\"\"\n",
    "        OnPolicyRunner が期待する形式:\n",
    "          obs, extras = get_observations()\n",
    "        extras[\"observations\"] は dict で、\n",
    "          extras[\"observations\"][\"policy\"] → policy 用観測 tensor\n",
    "          extras[\"observations\"][\"critic\"] → critic 用観測（ここではなし or zeros）\n",
    "        のように使われます。\n",
    "        \"\"\"\n",
    "        # policy 用観測\n",
    "        policy_obs = self._obs\n",
    "        critic_obs = self._obs\n",
    "        # critic 用観測がない場合は空/dummy tensor でも構わない\n",
    "        # ここでは単純に policy_obs を再利用します\n",
    "        # critic_obs = torch.zeros_like(self._obs)\n",
    "\n",
    "        extras = {\n",
    "            \"observations\": {\n",
    "                \"policy\": policy_obs,\n",
    "                \"critic\": critic_obs,\n",
    "            }\n",
    "        }\n",
    "        return policy_obs, extras\n",
    "    \n",
    "    def close(self):\n",
    "        for e in self.envs:\n",
    "            e.close()\n",
    "    \n",
    "    @property\n",
    "    def episode_length_buf(self) -> torch.Tensor:\n",
    "        return self._episode_length_buf\n",
    "\n",
    "    @episode_length_buf.setter\n",
    "    def episode_length_buf(self, value: torch.Tensor):\n",
    "        self._episode_length_buf = value\n",
    "\n",
    "# %% セル２: PPO 用設定辞書をべた書き → Runner 初期化＆学習\n",
    "TASK        = \"Pendulum-v1\"\n",
    "NUM_ENVS    = 1\n",
    "TOTAL_STEPS = 200000\n",
    "STEPS_PER_ENV = 200\n",
    "\n",
    "DEVICE      = \"cuda:0\"\n",
    "\n",
    "# VecEnv 作成\n",
    "venv = GymVecEnv(TASK, NUM_ENVS, device=DEVICE)\n",
    "\n",
    "# %% セル２: PPO 用設定辞書をべた書き → Runner 初期化＆学習\n",
    "\n",
    "train_cfg = {\n",
    "    # １ env あたり何ステップ収集するか\n",
    "    \"num_steps_per_env\": STEPS_PER_ENV,\n",
    "    # 最大イテレーション数（PPOループ回数）\n",
    "    \"max_iterations\":   TOTAL_STEPS//STEPS_PER_ENV,\n",
    "    # チェックポイント保存間隔\n",
    "    \"save_interval\":    1000,\n",
    "    # 実験名（ログフォルダ用）\n",
    "    \"experiment_name\":  \"pendulum_ppo\",\n",
    "    # 経験的正規化を使うか否か\n",
    "    \"empirical_normalization\": True,\n",
    "\n",
    "    # ポリシーのネットワーク構成\n",
    "    \"policy\": {\n",
    "        \"class_name\":        \"ActorCritic\",\n",
    "        \"init_noise_std\":    0.1,\n",
    "        \"actor_hidden_dims\": [64, 64],\n",
    "        \"critic_hidden_dims\":[64, 64],\n",
    "        \"activation\":        \"tanh\",\n",
    "        \"apply_tanh\":   True, \n",
    "    },\n",
    "\n",
    "    # アルゴリズムのハイパーパラメータ\n",
    "    \"algorithm\": {\n",
    "        \"class_name\":           \"PPO\",\n",
    "        \"value_loss_coef\":      1.0,\n",
    "        \"use_clipped_value_loss\": True,\n",
    "        \"clip_param\":           0.2,\n",
    "        \"entropy_coef\":         0.01,\n",
    "        \"num_learning_epochs\":  5,\n",
    "        \"num_mini_batches\":     4,\n",
    "        \"learning_rate\":        1.0e-3,\n",
    "        \"schedule\":             \"adaptive\",\n",
    "        \"gamma\":                0.99,\n",
    "        \"lam\":                  0.95,\n",
    "        \"desired_kl\":           0.01,\n",
    "        \"max_grad_norm\":        1.0,\n",
    "    },\n",
    "}\n",
    "\n",
    "log_dir = \"./pendulum_ppo_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "runner = OnPolicyRunner(venv, train_cfg, log_dir=log_dir, device=DEVICE)\n",
    "print(\"=== Expert (PPO) Training Start ===\")\n",
    "# train() の代わりに learn() を使う\n",
    "runner.learn(\n",
    "    num_learning_iterations=train_cfg[\"max_iterations\"],\n",
    "    init_at_random_ep_len=True,    # 必要に応じてランダム初期長を有効化\n",
    ")\n",
    "print(\"=== Expert Training Done ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c865bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MP4 を保存しました: ./videos/expert_pendulum.mp4\n"
     ]
    }
   ],
   "source": [
    "import torch, os, cv2, numpy as np, gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "policy_fn = runner.get_inference_policy(device=\"cpu\")\n",
    "\n",
    "os.makedirs(\"./videos\", exist_ok=True)\n",
    "video_path = \"./videos/expert_pendulum.mp4\"\n",
    "\n",
    "env = TimeLimit(gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\"), max_episode_steps=1000)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "frame = env.render()\n",
    "h, w, _ = frame.shape\n",
    "\n",
    "writer = cv2.VideoWriter(video_path,\n",
    "                         cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "                         30, (w, h))\n",
    "\n",
    "for _ in range(1000):\n",
    "    # ① 現フレームを書き込み\n",
    "    writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # ② obs -> Tensor\n",
    "    obs_t = torch.from_numpy(obs[None, ...]).float()\n",
    "    with torch.no_grad():\n",
    "        action_t, *_ = policy_fn(obs_t)          # (1,1) Tensor\n",
    "\n",
    "    # ③ Tensor -> (1,) ndarray\n",
    "    action = action_t.squeeze().cpu().numpy().flatten()  # shape (1,)\n",
    "    if action.ndim == 0:                               # 保険\n",
    "        action = np.array([action])\n",
    "\n",
    "    # ④ 環境へ入力\n",
    "    obs, _, done, truncated, _ = env.step(action)\n",
    "\n",
    "    # ⑤ 次フレーム取得\n",
    "    frame = env.render()\n",
    "    if done or truncated:\n",
    "        writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        break\n",
    "\n",
    "writer.release()\n",
    "env.close()\n",
    "print(\"MP4 を保存しました:\", video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f160176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "5 expert episodes collected\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ee571a3e2a4424968492f6d40ae9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245bc5c9192c4ab996a5244d7875c1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca48b21877e442fb1af7f011a4ad5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6787bce312ef40fda18d03585bdac7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac35cfa9bb764704a52f3349b62d2815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2725eca20ce475bb55651e22710f78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6737589500454cb3faa5e4a9f3a33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb2cd121b474e13ae5fcde1fd516f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84e36cb2da64882a4cb8e545e186aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adeb5936b78a4a3f8744e33cb9c734f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c6a4f89106499a9a96e94089ed9204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a084416f82074c34a37cc9b972c21f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49cbb957ac04959863ce3cda1afe2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72763602c4a846dd95acdd4f0fc8c89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba8eba3df3c40f298df68f63cf4898b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 0         |\n",
      "|    ent_loss       | -0.00142  |\n",
      "|    entropy        | 1.42      |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 70.5      |\n",
      "|    loss           | 0.955     |\n",
      "|    neglogp        | 0.956     |\n",
      "|    prob_true_act  | 0.385     |\n",
      "|    samples_so_far | 32        |\n",
      "| rollout/          |           |\n",
      "|    return_max     | -860      |\n",
      "|    return_mean    | -1.23e+03 |\n",
      "|    return_min     | -1.53e+03 |\n",
      "|    return_std     | 235       |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "489batch [00:03, 220.41batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000917 |\n",
      "|    entropy        | 0.917     |\n",
      "|    epoch          | 5         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 71.4      |\n",
      "|    loss           | 0.416     |\n",
      "|    neglogp        | 0.417     |\n",
      "|    prob_true_act  | 0.659     |\n",
      "|    samples_so_far | 16032     |\n",
      "| rollout/          |           |\n",
      "|    return_max     | -1.55e+03 |\n",
      "|    return_mean    | -1.68e+03 |\n",
      "|    return_min     | -1.84e+03 |\n",
      "|    return_std     | 113       |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "930batch [00:06, 144.90batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DAgger checkpoint to: c:\\Users\\admin5050\\akamisaka\\TransformableQuadrupedWheelchairIsaacLab\\exts\\transformable_quadruped_wheelchair_isaaclab\\transformable_quadruped_wheelchair_isaaclab\\envs\\dagger_scratch\\checkpoint-001.pt\n",
      "Saved DAgger policy to: c:\\Users\\admin5050\\akamisaka\\TransformableQuadrupedWheelchairIsaacLab\\exts\\transformable_quadruped_wheelchair_isaaclab\\transformable_quadruped_wheelchair_isaaclab\\envs\\dagger_scratch\\policy-001.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "# imitation ライブラリから必要なモジュールをインポート\n",
    "from imitation.data.rollout import rollout, make_sample_until, generate_trajectories\n",
    "from imitation.algorithms.bc      import BC\n",
    "from imitation.algorithms.dagger  import SimpleDAggerTrainer, LinearBetaSchedule\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "\n",
    "vec_env = DummyVecEnv([lambda: gym.make(\"Pendulum-v1\")] * 1)\n",
    "\n",
    "import shutil, os\n",
    "scratch_dir = \"./dagger_scratch\"\n",
    "if os.path.exists(scratch_dir):\n",
    "    shutil.rmtree(scratch_dir)\n",
    "\n",
    "expert_fn = runner.get_inference_policy()\n",
    "\n",
    "class ExpertWrapper(BasePolicy):\n",
    "    def __init__(self, fn, observation_space, action_space, device=\"cpu\"):\n",
    "        super().__init__(observation_space, action_space, device, squash_output=False)\n",
    "        self.fn = fn\n",
    "    def _predict(self, obs, deterministic=True):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            return self.fn(obs_t)           # Tensor をそのまま返す\n",
    "    def forward(self, obs, deterministic=False):\n",
    "        return self._predict(obs, deterministic)\n",
    "\n",
    "expert = ExpertWrapper(\n",
    "    fn=runner.get_inference_policy(),\n",
    "    observation_space=vec_env.observation_space,\n",
    "    action_space     =vec_env.action_space,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "# class ExpertWrapper(BasePolicy):\n",
    "#     \"\"\"rsl-rl の推論関数 `fn` を SB3 Policy 風に包む.\"\"\"\n",
    "#     def __init__(self, fn, observation_space, action_space, device=\"cpu\"):\n",
    "#         super().__init__(observation_space, action_space, device, squash_output=False)\n",
    "#         self.fn = fn                                    # rsl-rl の推論関数\n",
    "    \n",
    "#     # ここを修正 ───────────────────────────\n",
    "#     def _predict(self, obs, deterministic=True):\n",
    "#         obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device)\n",
    "#         with torch.no_grad():\n",
    "#             act_t = self.fn(obs_t)      # すでに Tensor\n",
    "#         return act_t                    # ← NumPy に変換しない！\n",
    "#     # ────────────────────────────────────\n",
    "\n",
    "#     # forward も Tensor を返すよう揃える\n",
    "#     def forward(self, obs, deterministic=False):\n",
    "#         return self._predict(obs, deterministic)\n",
    "    \n",
    "    # # SB3 が内部で呼ぶメソッド\n",
    "    # def _predict(self, obs, deterministic=True):\n",
    "    #     obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device)\n",
    "    #     with torch.no_grad():\n",
    "    #         act_t = self.fn(obs_t)\n",
    "    #     return act_t.cpu().numpy()\n",
    "\n",
    "    # # PyTorch の forward（BC 学習では使わないのでダミーで可）\n",
    "    # def forward(self, obs, deterministic=False):\n",
    "    #     return torch.as_tensor(self._predict(obs, deterministic), device=self.device)\n",
    "    \n",
    "# class ExpertWrapper:\n",
    "#     def __init__(self, fn, observation_space, action_space, device=\"cpu\"):\n",
    "#         self.fn                = fn\n",
    "#         self.observation_space = observation_space\n",
    "#         self.action_space      = action_space\n",
    "#         self.device            = torch.device(device)\n",
    "\n",
    "#     def __call__(self, obs: np.ndarray, state=None, dones=None):\n",
    "#         # NumPy→Tensor\n",
    "#         obs_t = torch.from_numpy(obs).float().to(self.device)\n",
    "#         # 行動だけ返す expert_fn を呼び出し\n",
    "#         with torch.no_grad():\n",
    "#             act_t = self.fn(obs_t)            # 返り値は Tensor のみ\n",
    "#         # Tensor→NumPy\n",
    "#         act = act_t.cpu().numpy()\n",
    "#         # DAgger 用に、(actions, next_state) のタプルで返す\n",
    "#         # next_state を持たないモデルなので None を渡す\n",
    "#         return act, None\n",
    "\n",
    "# class ExpertWrapper:\n",
    "#     def __init__(self, fn, observation_space, action_space, device=\"cpu\"):\n",
    "#         self.fn                = fn\n",
    "#         self.observation_space = observation_space\n",
    "#         self.action_space      = action_space\n",
    "#         self.device            = torch.device(device)\n",
    "\n",
    "#     def _forward(self, obs):\n",
    "#         obs_t = torch.as_tensor(obs).float().to(self.device)\n",
    "#         with torch.no_grad():\n",
    "#             act_t = self.fn(obs_t)\n",
    "#         return act_t.cpu().numpy()\n",
    "\n",
    "#     # SB3 互換\n",
    "#     def predict(self, obs, deterministic=True):\n",
    "#         return self._forward(obs), None\n",
    "\n",
    "#     # # callable 用としても動く\n",
    "#     # def __call__(self, obs, state=None, dones=None):\n",
    "#     #     return self._forward(obs), None\n",
    "\n",
    "expert = ExpertWrapper(\n",
    "    runner.get_inference_policy(),\n",
    "    observation_space=vec_env.observation_space,\n",
    "    action_space=vec_env.action_space,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "# ─── 2) Student (SB3 PPO) の準備 ────────────────────────────\n",
    "env     = gym.make(\"Pendulum-v1\")\n",
    "student = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# ─── 3) Expert Trajectories の収集 ─────────────────────────\n",
    "# def collect_expert_trajs(policy, venv, min_episodes: int, seed: int):\n",
    "#     \"\"\"Generate trajectories from the expert policy.\"\"\"\n",
    "#     sample_until = make_sample_until(min_episodes=min_episodes)\n",
    "#     trajs = generate_trajectories(\n",
    "#         policy               = policy,                     # ExpertWrapper\n",
    "#         venv                 = venv,\n",
    "#         sample_until         = sample_until,\n",
    "#         deterministic_policy = False,                      # ← ここを False に\n",
    "#         rng                  = np.random.default_rng(seed),\n",
    "#     )\n",
    "#     return trajs\n",
    "\n",
    "seed = 0\n",
    "# expert_trajs = collect_expert_trajs(expert, vec_env, min_episodes=5, seed=seed)\n",
    "# print(f\"Collected {len(expert_trajs)} expert episodes.\")\n",
    "\n",
    "def collect_expert_trajs(policy):\n",
    "    sample_until = make_sample_until(min_episodes=5)\n",
    "    return generate_trajectories(\n",
    "        policy               = policy,\n",
    "        venv                 = vec_env,\n",
    "        sample_until         = sample_until,\n",
    "        deterministic_policy = False,      # ← ここだけで十分\n",
    "        rng                  = np.random.default_rng(seed),\n",
    "    )\n",
    "\n",
    "expert_trajs = collect_expert_trajs(expert)\n",
    "print(len(expert_trajs), \"expert episodes collected\")\n",
    "\n",
    "# from types import MethodType\n",
    "# def _predict(self, obs, deterministic=True):\n",
    "#     # obs が NumPy なら Tensor に変換\n",
    "#     obs_t = torch.as_tensor(obs, dtype=torch.float32).to(next(self.parameters()).device)\n",
    "#     with torch.no_grad():\n",
    "#         act_t = self(obs_t)             # forward 呼び出し\n",
    "#     return act_t.cpu().numpy(), None    # SB3 と同じ戻り値 (action, state)\n",
    "# bc_trainer.policy.predict = MethodType(_predict, bc_trainer.policy)\n",
    "# 1️⃣  ラッパークラスはそのまま\n",
    "\n",
    "class BCPredictOnly(torch.nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        self.net = net                          # 元ネットを登録\n",
    "        self.device = next(net.parameters()).device  # ← 追加: device を持たせる\n",
    "\n",
    "    # BC 学習用\n",
    "    def forward(self, obs):\n",
    "        return self.net(obs)\n",
    "\n",
    "    # ロールアウト用\n",
    "    def predict(self, obs, deterministic=True):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            act_t = self.net(obs_t)\n",
    "        return act_t.cpu().numpy(), None        # SB3 と同じ戻り値\n",
    "\n",
    "\n",
    "########################################\n",
    "# ② _policy の差し替えはそのまま\n",
    "########################################\n",
    "# bc_trainer = BC(\n",
    "#     observation_space = vec_env.observation_space,\n",
    "#     action_space      = vec_env.action_space,\n",
    "#     rng               = np.random.default_rng(seed),\n",
    "# )\n",
    "# bc_trainer._policy = BCPredictOnly(bc_trainer.policy)   # ← OK\n",
    "\n",
    "bc_trainer = BC(\n",
    "    observation_space = vec_env.observation_space,\n",
    "    action_space      = vec_env.action_space,\n",
    "    rng               = np.random.default_rng(seed),\n",
    ")\n",
    "\n",
    "from types import MethodType\n",
    "import torch\n",
    "\n",
    "def bc_predict(self, obs, state=None, episode_start=None, deterministic=True):\n",
    "    dev = next(self.parameters()).device\n",
    "    obs_t = torch.as_tensor(obs, dtype=torch.float32, device=dev)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = self(obs_t)                # ← forward の戻り値\n",
    "\n",
    "    # --- ここを追加 --------------------------------------------------\n",
    "    # BC ネットは (acts,) のタプルを返すので 1 要素目を取り出す\n",
    "    if isinstance(out, (tuple, list)):\n",
    "        out = out[0]\n",
    "\n",
    "    # Distribution 型を返す実装の場合に備えた保険\n",
    "    if isinstance(out, torch.distributions.Distribution):\n",
    "        out = out.mean if deterministic else out.sample()\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    return out.cpu().numpy(), None       # SB3 互換の戻り値\n",
    "\n",
    "bc_trainer.policy.predict = MethodType(bc_predict, bc_trainer.policy)\n",
    "\n",
    "\n",
    "# ─── 4) DAggerTrainer の初期化 ─────────────────────────────\n",
    "# dagger_trainer = SimpleDAggerTrainer(\n",
    "#     venv             = vec_env,                   # 学習＆ロールアウト用 VecEnv\n",
    "#     scratch_dir      = \"./dagger_scratch\",        # 中間結果保存フォルダ\n",
    "#     expert_policy    = expert,                    # ExpertWrapper\n",
    "#     bc_trainer       = BC(\n",
    "#         observation_space=vec_env.observation_space,\n",
    "#         action_space     =vec_env.action_space,\n",
    "#         rng              = np.random.default_rng(seed),\n",
    "#     ),\n",
    "#     expert_trajs     = expert_trajs,              # 事前に集めた expert_demo\n",
    "#     rng              = np.random.default_rng(seed),\n",
    "#     beta_schedule    = LinearBetaSchedule(5),     # 5 ラウンドで β:1 → 0\n",
    "# )\n",
    "\n",
    "# dagger_trainer = SimpleDAggerTrainer(\n",
    "#     venv          = vec_env,\n",
    "#     scratch_dir   = \"./dagger_scratch\",\n",
    "#     expert_policy = expert,\n",
    "#     bc_trainer    = bc_trainer,         # ← 追加済みの bc_trainer を渡す\n",
    "#     expert_trajs  = expert_trajs,\n",
    "#     rng           = np.random.default_rng(seed),\n",
    "#     beta_schedule = LinearBetaSchedule(5),\n",
    "# )\n",
    "\n",
    "# # ─── 5) DAgger 学習の実行 ─────────────────────────────────\n",
    "# dagger_trainer.train(\n",
    "#     total_timesteps             = 100_000,          # student に与える学習ステップ数\n",
    "#     rollout_round_min_episodes  = 5,                # 各ラウンドで最低収集エピソード数\n",
    "#     rollout_round_min_timesteps = 2000,             # 各ラウンドで最低収集ステップ数\n",
    "#     bc_train_kwargs             = {\"n_epochs\": 10}, # BC 更新時のエポック数\n",
    "#     deterministic_policy        = False\n",
    "# )\n",
    "\n",
    "dagger_trainer = SimpleDAggerTrainer(\n",
    "    venv          = vec_env,\n",
    "    scratch_dir   = \"./dagger_scratch\",\n",
    "    expert_policy = expert,\n",
    "    bc_trainer    = bc_trainer,\n",
    "    expert_trajs  = expert_trajs,\n",
    "    rng           = np.random.default_rng(seed),\n",
    "    beta_schedule = LinearBetaSchedule(5),\n",
    ")\n",
    "\n",
    "dagger_trainer.train(\n",
    "    total_timesteps            = 1000,\n",
    "    rollout_round_min_episodes = 5,\n",
    "    rollout_round_min_timesteps= 2000,\n",
    "    bc_train_kwargs            = {\"n_epochs\": 10},\n",
    ")\n",
    "\n",
    "# ─── 6) 訓練済みポリシーの保存 ─────────────────────────────\n",
    "ckpt_path, policy_path = dagger_trainer.save_trainer()\n",
    "print(f\"Saved DAgger checkpoint to: {ckpt_path}\")\n",
    "print(f\"Saved DAgger policy to: {policy_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eabc512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MP4 を保存しました: ./videos/pendulum_dagger.mp4\n"
     ]
    }
   ],
   "source": [
    "# import os, cv2, numpy as np, gymnasium as gym\n",
    "# from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "# # 0) 出力先フォルダ\n",
    "# os.makedirs(\"./videos\", exist_ok=True)\n",
    "# video_path = \"./videos/pendulum_dagger.avi\"   # .mp4 にしたい場合は拡張子と fourcc を変える\n",
    "\n",
    "# # 1) 学習済み policy\n",
    "# policy = dagger_trainer.bc_trainer.policy     # バッチ次元を付けて predict する\n",
    "\n",
    "# # 2) 環境 (rgb_array) + 1000 ステップ制限\n",
    "# env = TimeLimit(\n",
    "#     gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\"),\n",
    "#     max_episode_steps=1000\n",
    "# )\n",
    "\n",
    "# # 3) reset → 最初の obs とフレーム\n",
    "# obs, _ = env.reset()\n",
    "# frame = env.render()              # ここで初回フレームを取得\n",
    "# h, w, _ = frame.shape\n",
    "\n",
    "# # 4) VideoWriter 初期化\n",
    "# writer = cv2.VideoWriter(\n",
    "#     video_path,\n",
    "#     cv2.VideoWriter_fourcc(*\"XVID\"),  # MP4 にしたいなら \"mp4v\" と .mp4\n",
    "#     30,                               # FPS\n",
    "#     (w, h)\n",
    "# )\n",
    "\n",
    "# # 5) ループ\n",
    "# for _ in range(1000):\n",
    "#     writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))   # 直前のフレームを書き込む\n",
    "\n",
    "#     action_b, _ = policy.predict(obs[None, ...], deterministic=True)\n",
    "#     obs, _, done, truncated, _ = env.step(action_b[0])\n",
    "\n",
    "#     frame = env.render()                                   # 次のフレーム\n",
    "#     if done or truncated:\n",
    "#         writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # 最終フレームも保存\n",
    "#         break\n",
    "\n",
    "# # 6) 後処理\n",
    "# writer.release()\n",
    "# env.close()\n",
    "# print(\"OpenCV で保存:\", video_path)\n",
    "\n",
    "import os, cv2, numpy as np, gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "# ------------------------- 設定 --------------------------\n",
    "os.makedirs(\"./videos\", exist_ok=True)\n",
    "video_path = \"./videos/pendulum_dagger.mp4\"   # ← .mp4 に変更\n",
    "\n",
    "policy = dagger_trainer.bc_trainer.policy     # 学習済みポリシー\n",
    "\n",
    "env = TimeLimit(\n",
    "    gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\"),\n",
    "    max_episode_steps=1000\n",
    ")\n",
    "\n",
    "# ------------------ 初期フレームと VideoWriter ------------\n",
    "obs, _ = env.reset()\n",
    "frame = env.render()                # 最初のフレーム (H,W,3) RGB\n",
    "h, w, _ = frame.shape\n",
    "\n",
    "writer = cv2.VideoWriter(\n",
    "    video_path,\n",
    "    cv2.VideoWriter_fourcc(*\"mp4v\"),  # ← fourcc を MP4 用に\n",
    "    30,                               # FPS\n",
    "    (w, h)\n",
    ")\n",
    "\n",
    "# ---------------------- ループ ----------------------------\n",
    "for _ in range(1000):\n",
    "    writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    action_b, _ = policy.predict(obs[None, ...], deterministic=True)\n",
    "    obs, _, done, truncated, _ = env.step(action_b[0])\n",
    "\n",
    "    frame = env.render()\n",
    "    if done or truncated:\n",
    "        writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        break\n",
    "\n",
    "# -------------------- 後処理 ------------------------------\n",
    "writer.release()\n",
    "env.close()\n",
    "print(\"MP4 を保存しました:\", video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b02a0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2922d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
